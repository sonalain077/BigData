{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0107fb-d5ce-4e0f-9a47-454d987216ef",
   "metadata": {},
   "source": [
    "# 0.Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c067934-bd5a-4110-9844-f451dc2f5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project Root: /home/img/BigData/Project\n",
      " Config loaded: /home/img/BigData/Project/conf/bda_project_config.yml\n",
      " Project Name: Bitcoin Price Predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 11:48:07 WARN Utils: Your hostname, a03-341a, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 11:48:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 11:48:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/07 11:48:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/07 11:48:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/07 11:48:09 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "App name: BTC_ETL_Custom\n",
      "Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import struct\n",
    "import hashlib\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType, IntegerType, TimestampType\n",
    ")\n",
    "\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"\n",
    "    Trouve la racine du projet en cherchant le dossier conf/.\n",
    "    Fonctionne depuis n'importe quel sous-dossier du projet.\n",
    "    \"\"\"\n",
    "    current = Path.cwd().resolve()\n",
    "    \n",
    "    if current.name == \"notebooks\":\n",
    "        candidate = current.parent\n",
    "        if (candidate / \"conf\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    search = current\n",
    "    while search != search.parent:\n",
    "        if (search / \"conf\").exists() and (search / \"conf\" / \"bda_project_config.yml\").exists():\n",
    "            return search\n",
    "        search = search.parent\n",
    "    \n",
    "    if (current / \"conf\").exists():\n",
    "        return current\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find project root (looked for 'conf/' folder)\\n\"\n",
    "        f\"Started from: {Path.cwd()}\\n\"\n",
    "        f\"Tip: Run this notebook from the Project/ or Project/notebooks/ directory\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "CONFIG_PATH = PROJECT_ROOT / \"conf\" / \"bda_project_config.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "PATHS = config['paths']\n",
    "SPARK_CFG = config['spark']\n",
    "\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Config loaded: {CONFIG_PATH}\")\n",
    "print(f\" Project Name: {config['project']['name']}\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    .appName(SPARK_CFG['app_name'])\n",
    "    .master(SPARK_CFG['master'])\n",
    "    .config(\"spark.driver.memory\", SPARK_CFG['driver_memory'])\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"App name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72183d3f-b909-486a-a5d4-c22e772200b7",
   "metadata": {},
   "source": [
    "# 1.File Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2cfced-b9d4-4275-ac0c-43a70266e4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PATHS CONFIGURED (relative to PROJECT_ROOT)\n",
      "============================================================\n",
      "Blocks Dir     : /home/img/BigData/Project/data/blocks/blocks\n",
      "Output Parquet : /home/img/BigData/Project/data/output/transactions_parquet\n",
      "Metrics File   : /home/img/BigData/Project/project_metrics_log.csv\n",
      "\n",
      "Blocks dir exists: True\n"
     ]
    }
   ],
   "source": [
    "BLOCKS_DIR = PROJECT_ROOT / PATHS['raw_blocks']\n",
    "\n",
    "OUTPUT_PATH = PROJECT_ROOT / PATHS['output_transactions']\n",
    "\n",
    "METRICS_FILE = PROJECT_ROOT / PATHS['metrics_file']\n",
    "\n",
    "EVIDENCE_DIR = PROJECT_ROOT / \"evidence\"\n",
    "\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EVIDENCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATHS CONFIGURED (relative to PROJECT_ROOT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Blocks Dir     : {BLOCKS_DIR}\")\n",
    "print(f\"Output Parquet : {OUTPUT_PATH}\")\n",
    "print(f\"Metrics File   : {METRICS_FILE}\")\n",
    "print(f\"\\nBlocks dir exists: {BLOCKS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707d346-a8a3-457d-8974-e8f37f9733a1",
   "metadata": {},
   "source": [
    "# 2.Bitcoin Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0baecdcc-2495-4283-8910-68bf4d74e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser functions defined\n"
     ]
    }
   ],
   "source": [
    "BITCOIN_MAGIC = bytes.fromhex('F9BEB4D9')\n",
    "\n",
    "def double_sha256(data: bytes) -> bytes:\n",
    "    return hashlib.sha256(hashlib.sha256(data).digest()).digest()\n",
    "\n",
    "def read_varint(data: bytes, offset: int) -> tuple:\n",
    "    b = data[offset]\n",
    "    if b < 0xFD:\n",
    "        return b, offset + 1\n",
    "    elif b == 0xFD:\n",
    "        return struct.unpack('<H', data[offset+1:offset+3])[0], offset + 3\n",
    "    elif b == 0xFE:\n",
    "        return struct.unpack('<I', data[offset+1:offset+5])[0], offset + 5\n",
    "    else:\n",
    "        return struct.unpack('<Q', data[offset+1:offset+9])[0], offset + 9\n",
    "\n",
    "def parse_tx(data: bytes, offset: int) -> tuple:\n",
    "    \"\"\"Parse une transaction, retourne (dict, new_offset).\"\"\"\n",
    "    start = offset\n",
    "    offset += 4  # version\n",
    "    \n",
    "    is_segwit = data[offset] == 0x00 and data[offset+1] == 0x01\n",
    "    if is_segwit:\n",
    "        offset += 2\n",
    "    \n",
    "    n_in, offset = read_varint(data, offset)\n",
    "    for _ in range(n_in):\n",
    "        offset += 36\n",
    "        script_len, offset = read_varint(data, offset)\n",
    "        offset += script_len + 4\n",
    "    \n",
    "    n_out, offset = read_varint(data, offset)\n",
    "    total_sats = 0\n",
    "    for _ in range(n_out):\n",
    "        total_sats += struct.unpack('<Q', data[offset:offset+8])[0]\n",
    "        offset += 8\n",
    "        script_len, offset = read_varint(data, offset)\n",
    "        offset += script_len\n",
    "    \n",
    "    if is_segwit:\n",
    "        for _ in range(n_in):\n",
    "            wit_count, offset = read_varint(data, offset)\n",
    "            for _ in range(wit_count):\n",
    "                wit_len, offset = read_varint(data, offset)\n",
    "                offset += wit_len\n",
    "    \n",
    "    offset += 4  \n",
    "    txid = double_sha256(data[start:offset])[::-1].hex()\n",
    "    \n",
    "    return {'tx_id': txid, 'n_inputs': n_in, 'n_outputs': n_out, 'amount_sats': total_sats}, offset\n",
    "\n",
    "def parse_blk_file(filepath: str) -> list:\n",
    "    \"\"\"Parse un fichier blk*.dat complet. Execute en parallele sur les workers.\"\"\"\n",
    "    transactions = []\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    offset = 0\n",
    "    size = len(data)\n",
    "    \n",
    "    while offset < size - 88:\n",
    "        if data[offset:offset+4] != BITCOIN_MAGIC:\n",
    "            offset += 1\n",
    "            continue\n",
    "        \n",
    "        offset += 4\n",
    "        block_size = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "        offset += 4\n",
    "        \n",
    "        if block_size == 0 or offset + block_size > size:\n",
    "            break\n",
    "        \n",
    "        block_start = offset\n",
    "        \n",
    "        try:\n",
    "            header = data[offset:offset+80]\n",
    "            block_hash = double_sha256(header)[::-1].hex()\n",
    "            timestamp = struct.unpack('<I', header[68:72])[0]\n",
    "            offset += 80\n",
    "            \n",
    "            tx_count, offset = read_varint(data, offset)\n",
    "            \n",
    "            for _ in range(tx_count):\n",
    "                if offset >= block_start + block_size:\n",
    "                    break\n",
    "                tx, offset = parse_tx(data, offset)\n",
    "                tx['block_hash'] = block_hash\n",
    "                tx['timestamp'] = timestamp\n",
    "                tx['block_size'] = block_size\n",
    "                transactions.append(tx)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        offset = block_start + block_size\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "print(\"Parser functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97411bc8-1904-4d79-b206-bce892c4abbd",
   "metadata": {},
   "source": [
    "# 3.Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0a3236-83e9-4456-a40f-845123bdf6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block files found: 9\n",
      "Total size: 1.09 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 9) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transactions parsed: 2,236,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Lister les fichiers blk*.dat\n",
    "blk_files = sorted(BLOCKS_DIR.glob(\"blk*.dat\")) if BLOCKS_DIR.exists() else []\n",
    "print(f\"Block files found: {len(blk_files)}\")\n",
    "\n",
    "if blk_files:\n",
    "    total_input_size = sum(f.stat().st_size for f in blk_files)\n",
    "    print(f\"Total size: {total_input_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # RDD parallele: chaque worker parse un fichier\n",
    "    file_paths = [str(f) for f in blk_files]\n",
    "    files_rdd = spark.sparkContext.parallelize(file_paths, numSlices=len(file_paths))\n",
    "    \n",
    "    # flatMap car parse_blk_file retourne une liste de transactions\n",
    "    transactions_rdd = files_rdd.flatMap(parse_blk_file)\n",
    "    transactions_rdd.cache()\n",
    "    \n",
    "    tx_count = transactions_rdd.count()\n",
    "    print(f\"\\nTransactions parsed: {tx_count:,}\")\n",
    "else:\n",
    "    print(f\"No block files in: {BLOCKS_DIR}\")\n",
    "    transactions_rdd = None\n",
    "    tx_count = 0\n",
    "    total_input_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da25bb-4221-4467-a0a4-16a314445343",
   "metadata": {},
   "source": [
    "# 4.Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302f61f6-7440-4115-8409-43e284cb2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: 2,236,800 rows\n",
      "root\n",
      " |-- tx_id: string (nullable = false)\n",
      " |-- block_hash: string (nullable = false)\n",
      " |-- timestamp: long (nullable = false)\n",
      " |-- n_inputs: integer (nullable = false)\n",
      " |-- n_outputs: integer (nullable = false)\n",
      " |-- amount_sats: long (nullable = false)\n",
      " |-- block_size: integer (nullable = false)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- timestamp_hour: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+--------------+--------+-----------+\n",
      "|               tx_id|           datetime|timestamp_hour|n_inputs|amount_sats|\n",
      "+--------------------+-------------------+--------------+--------+-----------+\n",
      "|d1827b6a6c1b67830...|2013-12-07 14:22:49|    1386424800|       1| 2513325458|\n",
      "|3fb28e73f5a8cb701...|2013-12-07 14:22:49|    1386424800|       2|  110442816|\n",
      "|6711e8803b04b774f...|2013-12-07 14:22:49|    1386424800|       3| 1998950000|\n",
      "|bef8b2a4273a11650...|2013-12-07 14:22:49|    1386424800|       2| 2500000000|\n",
      "|55b18dda4e62a1d3b...|2013-12-07 14:22:49|    1386424800|       5|  101002476|\n",
      "|cc5393f91030f312e...|2013-12-07 14:22:49|    1386424800|       1| 1290678019|\n",
      "|cd0cf1c33a144753b...|2013-12-07 14:22:49|    1386424800|       1| 1164005422|\n",
      "|293eb3ce30463f527...|2013-12-07 14:22:49|    1386424800|       3| 4604970000|\n",
      "|eede4bb9c982826ea...|2013-12-07 14:22:49|    1386424800|       4| 2001031958|\n",
      "|dec0f8cb4e8529657...|2013-12-07 14:22:49|    1386424800|       1|  197799998|\n",
      "+--------------------+-------------------+--------------+--------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 9) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COUVERTURE TEMPORELLE DES BLOCS\n",
      "============================================================\n",
      "Premier bloc : 2013-12-05 19:19:51\n",
      "Dernier bloc : 2014-01-15 01:19:13\n",
      "Nombre de blocs uniques : 6,947\n",
      "Duree couverte : 40 jours, 5 heures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if transactions_rdd and tx_count > 0:\n",
    "    # Schema Spark\n",
    "    tx_schema = StructType([\n",
    "        StructField(\"tx_id\", StringType(), False),\n",
    "        StructField(\"block_hash\", StringType(), False),\n",
    "        StructField(\"timestamp\", LongType(), False),\n",
    "        StructField(\"n_inputs\", IntegerType(), False),\n",
    "        StructField(\"n_outputs\", IntegerType(), False),\n",
    "        StructField(\"amount_sats\", LongType(), False),\n",
    "        StructField(\"block_size\", IntegerType(), False)\n",
    "    ])\n",
    "    \n",
    "    # RDD[dict] -> RDD[tuple] -> DataFrame\n",
    "    rows_rdd = transactions_rdd.map(lambda tx: (\n",
    "        tx['tx_id'], tx['block_hash'], tx['timestamp'],\n",
    "        tx['n_inputs'], tx['n_outputs'], tx['amount_sats'], tx['block_size']\n",
    "    ))\n",
    "    \n",
    "    raw_df = spark.createDataFrame(rows_rdd, schema=tx_schema)\n",
    "    \n",
    "    # Colonnes temporelles (meme logique que market_etl pour jointure)\n",
    "    transformed_df = (\n",
    "        raw_df\n",
    "        .withColumn(\"datetime\", F.from_unixtime(F.col(\"timestamp\")).cast(TimestampType()))\n",
    "        .withColumn(\"timestamp_hour\", (F.floor(F.col(\"timestamp\") / 3600) * 3600).cast(LongType()))\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"datetime\")))\n",
    "        .withColumn(\"hour\", F.hour(F.col(\"datetime\")))\n",
    "    )\n",
    "    \n",
    "    transformed_df.cache()\n",
    "    row_count = transformed_df.count()\n",
    "    \n",
    "    print(f\"DataFrame: {row_count:,} rows\")\n",
    "    transformed_df.printSchema()\n",
    "    \n",
    "    transformed_df.select(\"tx_id\", \"datetime\", \"timestamp_hour\", \"n_inputs\", \"amount_sats\").show(10, truncate=20)\n",
    "\n",
    "    date_range = transformed_df.agg(\n",
    "        F.min(\"datetime\").alias(\"first_block\"),\n",
    "        F.max(\"datetime\").alias(\"last_block\"),\n",
    "        F.countDistinct(\"block_hash\").alias(\"n_blocks\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"COUVERTURE TEMPORELLE DES BLOCS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Premier bloc : {date_range['first_block']}\")\n",
    "    print(f\"Dernier bloc : {date_range['last_block']}\")\n",
    "    print(f\"Nombre de blocs uniques : {date_range['n_blocks']:,}\")\n",
    "    \n",
    "    # Calculer la duree\n",
    "    if date_range['first_block'] and date_range['last_block']:\n",
    "        duration = date_range['last_block'] - date_range['first_block']\n",
    "        print(f\"Duree couverte : {duration.days} jours, {duration.seconds // 3600} heures\")\n",
    "else:\n",
    "    transformed_df = None\n",
    "    row_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535411cb-f7fe-4a2e-97bf-9812019cdba3",
   "metadata": {},
   "source": [
    "# 5.Spark Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae16b366-b28e-4799-8a8a-cb36bee70791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPARK EXECUTION PLAN\n",
      "============================================================\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan (1)\n",
      "   +- InMemoryRelation (2)\n",
      "         +- * Project (5)\n",
      "            +- * Project (4)\n",
      "               +- * Scan ExistingRDD (3)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [11]: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6, datetime#7, timestamp_hour#8L, date#9, hour#10]\n",
      "Arguments: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6, datetime#7, timestamp_hour#8L, date#9, hour#10]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6, datetime#7, timestamp_hour#8L, date#9, hour#10], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan ExistingRDD [codegen id : 1]\n",
      "Output [7]: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6]\n",
      "Arguments: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6], MapPartitionsRDD[6] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(4) Project [codegen id : 1]\n",
      "Output [9]: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6, cast(from_unixtime(timestamp#2L, yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp) AS datetime#7, (FLOOR((cast(timestamp#2L as double) / 3600.0)) * 3600) AS timestamp_hour#8L]\n",
      "Input [7]: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6]\n",
      "\n",
      "(5) Project [codegen id : 1]\n",
      "Output [11]: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6, datetime#7, timestamp_hour#8L, cast(datetime#7 as date) AS date#9, hour(datetime#7, Some(UTC)) AS hour#10]\n",
      "Input [9]: [tx_id#0, block_hash#1, timestamp#2L, n_inputs#3, n_outputs#4, amount_sats#5L, block_size#6, datetime#7, timestamp_hour#8L]\n",
      "\n",
      "\n",
      "\n",
      "Execution plan saved to: /home/img/BigData/Project/evidence/block_ingestion_explain.txt\n"
     ]
    }
   ],
   "source": [
    "if transformed_df:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPARK EXECUTION PLAN\")\n",
    "    print(\"=\" * 60)\n",
    "    transformed_df.explain(\"formatted\")\n",
    "    \n",
    "    plan_buffer = StringIO()\n",
    "    with redirect_stdout(plan_buffer):\n",
    "        transformed_df.explain(\"formatted\")\n",
    "    \n",
    "    plan_file = EVIDENCE_DIR / \"block_ingestion_explain.txt\"\n",
    "    plan_file.write_text(f\"# Block Ingestion Execution Plan\\n# Date: {datetime.now()}\\n\\n{plan_buffer.getvalue()}\")\n",
    "    \n",
    "    print(f\"\\nExecution plan saved to: {plan_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0b362-1ec5-4424-953c-698c48ae50f1",
   "metadata": {},
   "source": [
    "# 6.Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88fe804-a226-42f9-bb28-c93959b8f946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parquet saved to: /home/img/BigData/Project/data/output/transactions_parquet\n",
      "Parquet files created: 9\n",
      "Total size: 155.18 MB\n",
      "Rows in Parquet: 2,236,800\n",
      "Integrity check: OK\n"
     ]
    }
   ],
   "source": [
    "if transformed_df:\n",
    "    final_df = transformed_df.select(\n",
    "        \"tx_id\", \"block_hash\", \"timestamp\", \"datetime\", \"timestamp_hour\",\n",
    "        \"date\", \"hour\", \"n_inputs\", \"n_outputs\", \"amount_sats\", \"block_size\"\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        final_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"compression\", \"snappy\")\n",
    "        .parquet(str(OUTPUT_PATH))\n",
    "    )\n",
    "    \n",
    "    print(f\"  Parquet saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "if transformed_df:\n",
    "    parquet_files = list(OUTPUT_PATH.glob(\"*.parquet\"))\n",
    "    total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "    \n",
    "    print(f\"Parquet files created: {len(parquet_files)}\")\n",
    "    print(f\"Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    verify_df = spark.read.parquet(str(OUTPUT_PATH))\n",
    "    verify_count = verify_df.count()\n",
    "    \n",
    "    print(f\"Rows in Parquet: {verify_count:,}\")\n",
    "    print(f\"Integrity check: {'OK' if verify_count == row_count else 'MISMATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51765c9d-469e-4f8d-a67c-55bc9e30d9de",
   "metadata": {},
   "source": [
    "# 7.Spark Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b69d789-b5b4-4c43-903e-4e04d738fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Metrics logged to: /home/img/BigData/Project/project_metrics_log.csv\n",
      "   Run ID: block_ingestion_20251207_114845\n",
      "   Input size: 1118.59 MB\n",
      "   Output size: 155.18 MB\n",
      "Cache released.\n"
     ]
    }
   ],
   "source": [
    "run_id = f\"block_ingestion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "if not METRICS_FILE.exists():\n",
    "    with open(METRICS_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['run_id', 'task', 'note', 'files_read', 'input_size_bytes', \n",
    "                        'shuffle_read_bytes', 'shuffle_write_bytes', 'timestamp'])\n",
    "\n",
    "output_size = sum(f.stat().st_size for f in OUTPUT_PATH.glob(\"*.parquet\")) if OUTPUT_PATH.exists() else 0\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "with open(METRICS_FILE, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        run_id,\n",
    "        \"block_ingestion\",\n",
    "        f\"txs={row_count}\",\n",
    "        len(blk_files),\n",
    "        total_input_size,\n",
    "        0,\n",
    "        0,\n",
    "        timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"   Metrics logged to: {METRICS_FILE}\")\n",
    "print(f\"   Run ID: {run_id}\")\n",
    "print(f\"   Input size: {total_input_size / (1024*1024):.2f} MB\")\n",
    "print(f\"   Output size: {output_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "if transactions_rdd:\n",
    "    transactions_rdd.unpersist()\n",
    "if transformed_df:\n",
    "    transformed_df.unpersist()\n",
    "print(\"Cache released.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a710f-cbbb-44f4-9cee-8a9e0ab1652e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
