{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c13b5b",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4653eeaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:05.510899Z",
     "iopub.status.busy": "2025-12-07T04:17:05.510527Z",
     "iopub.status.idle": "2025-12-07T04:17:14.835418Z",
     "shell.execute_reply": "2025-12-07T04:17:14.832798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project Root: /home/img/BigData/Project\n",
      " Config loaded: /home/img/BigData/Project/conf/bda_project_config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 05:17:08 WARN Utils: Your hostname, a03-341a, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 05:17:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 05:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 05:17:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/07 05:17:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 4.0.1 initialisÃ©\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Trouve la racine du projet\"\"\"\n",
    "    current = Path.cwd().resolve()\n",
    "    \n",
    "    if current.name == \"notebooks\":\n",
    "        candidate = current.parent\n",
    "        if (candidate / \"conf\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    search = current\n",
    "    while search != search.parent:\n",
    "        if (search / \"conf\").exists() and (search / \"conf\" / \"bda_project_config.yml\").exists():\n",
    "            return search\n",
    "        search = search.parent\n",
    "    \n",
    "    if (current / \"conf\").exists():\n",
    "        return current\n",
    "    \n",
    "    raise FileNotFoundError(\"Impossible de trouver la racine du projet\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "CONFIG_PATH = PROJECT_ROOT / \"conf\" / \"bda_project_config.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "SPARK_CFG = config['spark']\n",
    "\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Config loaded: {CONFIG_PATH}\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SanityCheck-ETL-Global\")\n",
    "    .master(SPARK_CFG['master'])\n",
    "    .config(\"spark.driver.memory\", SPARK_CFG['driver_memory'])\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8ef19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:14.841079Z",
     "iopub.status.busy": "2025-12-07T04:17:14.840347Z",
     "iopub.status.idle": "2025-12-07T04:17:14.851430Z",
     "shell.execute_reply": "2025-12-07T04:17:14.848885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHEMINS DES PARQUET\n",
      "Market Parquet     : /home/img/BigData/Project/data/output/market_parquet\n",
      "Transactions Parquet: /home/img/BigData/Project/data/output/transactions_parquet\n",
      "\n",
      "Market existe      : Oui True\n",
      "Transactions existe: Oui True\n"
     ]
    }
   ],
   "source": [
    "MARKET_PARQUET_PATH = PROJECT_ROOT / \"data\" / \"output\" / \"market_parquet\"\n",
    "TX_PARQUET_PATH = PROJECT_ROOT / \"data\" / \"output\" / \"transactions_parquet\"\n",
    "\n",
    "\n",
    "print(f\"Fichiers     : {MARKET_PARQUET_PATH}\")\n",
    "print(f\" Parquet: {TX_PARQUET_PATH}\")\n",
    "print()\n",
    "print(f\"Market existe      : {'Oui' if MARKET_PARQUET_PATH.exists() else 'Non'} {MARKET_PARQUET_PATH.exists()}\")\n",
    "print(f\"Transactions existe: {'Oui' if TX_PARQUET_PATH.exists() else 'Non'} {TX_PARQUET_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69277a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. VÃ©rification du SchÃ©ma : Prix de MarchÃ© (`market_parquet`)\n",
    "\n",
    "On vÃ©rifie que le Parquet contient bien les colonnes attendues aprÃ¨s l'ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b4cf9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:14.855233Z",
     "iopub.status.busy": "2025-12-07T04:17:14.854929Z",
     "iopub.status.idle": "2025-12-07T04:17:14.862770Z",
     "shell.execute_reply": "2025-12-07T04:17:14.860176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHÃ‰MA ATTENDU : market_parquet\n",
      "Nombre de colonnes attendues : 10\n",
      "Colonnes : ['close', 'date', 'datetime', 'high', 'hour', 'low', 'open', 'timestamp_hour', 'timestamp_unix', 'volume']\n"
     ]
    }
   ],
   "source": [
    "EXPECTED_MARKET_COLS = {\n",
    "    \"timestamp_unix\",  \n",
    "    \"datetime\",         \n",
    "    \"timestamp_hour\",   \n",
    "    \"date\",             \n",
    "    \"hour\",             \n",
    "    \"open\",             \n",
    "    \"high\",             \n",
    "    \"low\",              \n",
    "    \"close\",            \n",
    "    \"volume\"            \n",
    "}\n",
    "\n",
    "print(\"SCHÃ‰MA ATTENDU : market_parquet\")\n",
    "print(f\"Nombre de colonnes attendues : {len(EXPECTED_MARKET_COLS)}\")\n",
    "print(f\"Colonnes : {sorted(EXPECTED_MARKET_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc3c7e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:14.866816Z",
     "iopub.status.busy": "2025-12-07T04:17:14.866529Z",
     "iopub.status.idle": "2025-12-07T04:17:17.307492Z",
     "shell.execute_reply": "2025-12-07T04:17:17.305494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHÃ‰MA RÃ‰EL : market_parquet\n",
      "Nombre de colonnes : 10\n",
      "Colonnes : ['close', 'date', 'datetime', 'high', 'hour', 'low', 'open', 'timestamp_hour', 'timestamp_unix', 'volume']\n",
      "\n",
      " SchÃ©ma market_parquet CONFORME aux attentes (colonnes identiques)\n",
      "\n",
      "Schema Spark :\n",
      "root\n",
      " |-- timestamp_unix: double (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- timestamp_hour: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if MARKET_PARQUET_PATH.exists():\n",
    "    df_market = spark.read.parquet(str(MARKET_PARQUET_PATH))\n",
    "    \n",
    "    cols_market = set(df_market.columns)\n",
    "    \n",
    "    print(\"SCHÃ‰MA RÃ‰EL : market_parquet\")\n",
    "    print(f\"Nombre de colonnes : {len(cols_market)}\")\n",
    "    print(f\"Colonnes : {sorted(cols_market)}\")\n",
    "    print()\n",
    "    \n",
    "    missing_market = EXPECTED_MARKET_COLS.difference(cols_market)\n",
    "    extra_market = cols_market.difference(EXPECTED_MARKET_COLS)\n",
    "    \n",
    "    if missing_market:\n",
    "        print(f\" Colonnes MANQUANTES : {missing_market}\")\n",
    "    if extra_market:\n",
    "        print(f\"  Colonnes EN PLUS (non attendues) : {extra_market}\")\n",
    "    \n",
    "    if not missing_market and not extra_market:\n",
    "        print(\" SchÃ©ma market_parquet CONFORME aux attentes (colonnes identiques)\")\n",
    "    elif not missing_market:\n",
    "        print(\" SchÃ©ma market_parquet OK (toutes les colonnes attendues prÃ©sentes)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Schema Spark :\")\n",
    "    df_market.printSchema()\n",
    "else:\n",
    "    print(\" market_parquet introuvable !\")\n",
    "    print(\" Lancez d'abord : notebooks/etl_market_price.ipynb\")\n",
    "    df_market = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665a27e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. VÃ©rification du SchÃ©ma : Transactions (`transactions_parquet`)\n",
    "\n",
    "On vÃ©rifie que le Parquet contient bien les colonnes attendues aprÃ¨s l'ETL des blocs Bitcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87828b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:17.311063Z",
     "iopub.status.busy": "2025-12-07T04:17:17.310801Z",
     "iopub.status.idle": "2025-12-07T04:17:17.317270Z",
     "shell.execute_reply": "2025-12-07T04:17:17.315688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š SCHÃ‰MA ATTENDU : transactions_parquet\n",
      "============================================================\n",
      "Nombre de colonnes attendues : 11\n",
      "Colonnes : ['amount_sats', 'block_hash', 'block_size', 'date', 'datetime', 'hour', 'n_inputs', 'n_outputs', 'timestamp', 'timestamp_hour', 'tx_id']\n"
     ]
    }
   ],
   "source": [
    "# Colonnes attendues dans transactions_parquet\n",
    "EXPECTED_TX_COLS = {\n",
    "    \"tx_id\",            # Identifiant unique de la transaction\n",
    "    \"block_hash\",       # Hash du bloc contenant la transaction\n",
    "    \"timestamp\",        # Timestamp Unix original du bloc\n",
    "    \"datetime\",         # Conversion en timestamp lisible\n",
    "    \"timestamp_hour\",   # ClÃ© de jointure (arrondi Ã  l'heure)\n",
    "    \"date\",             # Date extraite\n",
    "    \"hour\",             # Heure extraite\n",
    "    \"n_inputs\",         # Nombre d'inputs (UTXOs consommÃ©s)\n",
    "    \"n_outputs\",        # Nombre d'outputs (destinataires)\n",
    "    \"amount_sats\",      # Montant total en satoshis\n",
    "    \"block_size\"        # Taille du bloc en octets\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCHÃ‰MA ATTENDU : transactions_parquet\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Nombre de colonnes attendues : {len(EXPECTED_TX_COLS)}\")\n",
    "print(f\"Colonnes : {sorted(EXPECTED_TX_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af2503f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:17.319984Z",
     "iopub.status.busy": "2025-12-07T04:17:17.319716Z",
     "iopub.status.idle": "2025-12-07T04:17:17.464654Z",
     "shell.execute_reply": "2025-12-07T04:17:17.462709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SCHÃ‰MA RÃ‰EL : transactions_parquet\n",
      "Nombre de colonnes : 11\n",
      "Colonnes : ['amount_sats', 'block_hash', 'block_size', 'date', 'datetime', 'hour', 'n_inputs', 'n_outputs', 'timestamp', 'timestamp_hour', 'tx_id']\n",
      "\n",
      " SchÃ©ma transactions_parquet CONFORME aux attentes (colonnes identiques)\n",
      "\n",
      "Schema Spark :\n",
      "root\n",
      " |-- tx_id: string (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- timestamp_hour: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- n_inputs: integer (nullable = true)\n",
      " |-- n_outputs: integer (nullable = true)\n",
      " |-- amount_sats: long (nullable = true)\n",
      " |-- block_size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TX_PARQUET_PATH.exists():\n",
    "    df_tx = spark.read.parquet(str(TX_PARQUET_PATH))\n",
    "    \n",
    "    cols_tx = set(df_tx.columns)\n",
    "    \n",
    "    print(\" SCHÃ‰MA RÃ‰EL : transactions_parquet\")\n",
    "    print(f\"Nombre de colonnes : {len(cols_tx)}\")\n",
    "    print(f\"Colonnes : {sorted(cols_tx)}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    missing_tx = EXPECTED_TX_COLS.difference(cols_tx)\n",
    "    extra_tx = cols_tx.difference(EXPECTED_TX_COLS)\n",
    "    \n",
    "    if missing_tx:\n",
    "        print(f\" Colonnes MANQUANTES : {missing_tx}\")\n",
    "    if extra_tx:\n",
    "        print(f\" Colonnes EN PLUS (non attendues) : {extra_tx}\")\n",
    "    \n",
    "    if not missing_tx and not extra_tx:\n",
    "        print(\" SchÃ©ma transactions_parquet CONFORME aux attentes (colonnes identiques)\")\n",
    "    elif not missing_tx:\n",
    "        print(\" SchÃ©ma transactions_parquet OK (toutes les colonnes attendues prÃ©sentes)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Schema Spark :\")\n",
    "    df_tx.printSchema()\n",
    "else:\n",
    "    print(\"transactions_parquet introuvable !\")\n",
    "    print(\"Lancez d'abord : notebooks/etl_bitcoin_blocks.ipynb\")\n",
    "    df_tx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c709deb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PÃ©riodes Temporelles Couvertes\n",
    "\n",
    "On extrait la plage temporelle de chaque dataset :\n",
    "- PremiÃ¨re valeur de `datetime`\n",
    "- DerniÃ¨re valeur de `datetime`\n",
    "- Nombre de lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3fa649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:17.468226Z",
     "iopub.status.busy": "2025-12-07T04:17:17.467871Z",
     "iopub.status.idle": "2025-12-07T04:17:20.957060Z",
     "shell.execute_reply": "2025-12-07T04:17:20.954880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… PÃ‰RIODES TEMPORELLES COUVERTES\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prix de marchÃ© (market_parquet):\n",
      "   PremiÃ¨re donnÃ©e : 01/01/2012 11:01:00\n",
      "   DerniÃ¨re donnÃ©e : 01/12/2025 00:59:00\n",
      "   DurÃ©e couverte  : 5,082 jours (~13 ans)\n",
      "   Nombre de lignes: 7,317,759\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transactions blockchain (transactions_parquet):\n",
      "   PremiÃ¨re donnÃ©e : 05/12/2013 19:19:51\n",
      "   DerniÃ¨re donnÃ©e : 15/01/2014 01:19:13\n",
      "   DurÃ©e couverte  : 40 jours\n",
      "   Nombre de lignes: 2,236,800\n"
     ]
    }
   ],
   "source": [
    "market_min, market_max, market_count = None, None, 0\n",
    "tx_min, tx_max, tx_count = None, None, 0\n",
    "\n",
    "print(\"PÃ‰RIODES TEMPORELLES COUVERTES\")\n",
    "print()\n",
    "\n",
    "if df_market is not None:\n",
    "    market_stats = df_market.agg(\n",
    "        F.min(\"datetime\").alias(\"min_date\"),\n",
    "        F.max(\"datetime\").alias(\"max_date\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    market_min = market_stats[\"min_date\"]\n",
    "    market_max = market_stats[\"max_date\"]\n",
    "    market_count = market_stats[\"count\"]\n",
    "    \n",
    "    duration_market = (market_max - market_min).days if market_max and market_min else 0\n",
    "    \n",
    "    print(\"Prix de marchÃ© (market_parquet):\")\n",
    "    print(f\"   PremiÃ¨re donnÃ©e : {market_min.strftime('%d/%m/%Y %H:%M:%S') if market_min else 'N/A'}\")\n",
    "    print(f\"   DerniÃ¨re donnÃ©e : {market_max.strftime('%d/%m/%Y %H:%M:%S') if market_max else 'N/A'}\")\n",
    "    print(f\"   DurÃ©e couverte  : {duration_market:,} jours (~{duration_market // 365} ans)\")\n",
    "    print(f\"   Nombre de lignes: {market_count:,}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Prix de marchÃ© : donnÃ©es non disponibles\")\n",
    "    print()\n",
    "\n",
    "if df_tx is not None:\n",
    "    tx_stats = df_tx.agg(\n",
    "        F.min(\"datetime\").alias(\"min_date\"),\n",
    "        F.max(\"datetime\").alias(\"max_date\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    tx_min = tx_stats[\"min_date\"]\n",
    "    tx_max = tx_stats[\"max_date\"]\n",
    "    tx_count = tx_stats[\"count\"]\n",
    "    \n",
    "    duration_tx = (tx_max - tx_min).days if tx_max and tx_min else 0\n",
    "    \n",
    "    print(\" Transactions blockchain (transactions_parquet):\")\n",
    "    print(f\"   PremiÃ¨re donnÃ©e : {tx_min.strftime('%d/%m/%Y %H:%M:%S') if tx_min else 'N/A'}\")\n",
    "    print(f\"   DerniÃ¨re donnÃ©e : {tx_max.strftime('%d/%m/%Y %H:%M:%S') if tx_max else 'N/A'}\")\n",
    "    print(f\"   DurÃ©e couverte  : {duration_tx:,} jours\")\n",
    "    print(f\"   Nombre de lignes: {tx_count:,}\")\n",
    "else:\n",
    "    print(\"Transactions blockchain : donnÃ©es non disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bb598",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Chevauchement Temporel entre Prix et Transactions\n",
    "\n",
    "On calcule l'**intersection temporelle** entre les deux sÃ©ries :\n",
    "- `dÃ©but_chevauchement = max(min_prix, min_transactions)`\n",
    "- `fin_chevauchement = min(max_prix, max_transactions)`\n",
    "\n",
    "Si le chevauchement est non vide, la **jointure temporelle** (sur `timestamp_hour`) est possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77dc4142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:20.959961Z",
     "iopub.status.busy": "2025-12-07T04:17:20.959676Z",
     "iopub.status.idle": "2025-12-07T04:17:20.972446Z",
     "shell.execute_reply": "2025-12-07T04:17:20.969658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”— CHEVAUCHEMENT TEMPOREL\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š RÃ©capitulatif des pÃ©riodes :\n",
      "   Prix         : 01/01/2012 â†’ 01/12/2025\n",
      "   Transactions : 05/12/2013 â†’ 15/01/2014\n",
      "\n",
      "âœ… CHEVAUCHEMENT TEMPOREL DÃ‰TECTÃ‰ !\n",
      "\n",
      "   ğŸ”— PÃ©riode commune : 05/12/2013 19:19 â†’ 15/01/2014 01:19\n",
      "   ğŸ“… DurÃ©e           : 42 jour(s) (965 heures)\n",
      "\n",
      "   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "   âœ… Les deux datasets ont des donnÃ©es sur une pÃ©riode commune.\n",
      "   âœ… La jointure temporelle sur 'timestamp_hour' est POSSIBLE.\n",
      "   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "   ğŸ’¡ Prochaine Ã©tape : notebooks/feature_eng.ipynb\n",
      "      â†’ Jointure des prix et transactions par timestamp_hour\n",
      "      â†’ CrÃ©ation des features pour le modÃ¨le de prÃ©diction\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\" CHEVAUCHEMENT TEMPOREL\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "if market_min and tx_min:\n",
    "    # Calcul de l'intersection\n",
    "    overlap_start = max(market_min, tx_min)\n",
    "    overlap_end = min(market_max, tx_max)\n",
    "    \n",
    "    print(\" RÃ©capitulatif des pÃ©riodes :\")\n",
    "    print(f\"   Prix         : {market_min.strftime('%d/%m/%Y')} â†’ {market_max.strftime('%d/%m/%Y')}\")\n",
    "    print(f\"   Transactions : {tx_min.strftime('%d/%m/%Y')} â†’ {tx_max.strftime('%d/%m/%Y')}\")\n",
    "    print()\n",
    "    \n",
    "    if overlap_start <= overlap_end:\n",
    "        # Chevauchement existe\n",
    "        # +1 pour inclure le jour de dÃ©but ET de fin\n",
    "        overlap_days = (overlap_end.date() - overlap_start.date()).days + 1\n",
    "        overlap_hours = int((overlap_end - overlap_start).total_seconds() / 3600)\n",
    "        \n",
    "        print(\" CHEVAUCHEMENT TEMPOREL DÃ‰TECTÃ‰ !\")\n",
    "        print()\n",
    "        print(f\"    PÃ©riode commune : {overlap_start.strftime('%d/%m/%Y %H:%M')} â†’ {overlap_end.strftime('%d/%m/%Y %H:%M')}\")\n",
    "        print(f\"    DurÃ©e           : {overlap_days} jour(s) ({overlap_hours:,} heures)\")\n",
    "        print()\n",
    "        print(\"   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "        print(\"    Les deux datasets ont des donnÃ©es sur une pÃ©riode commune.\")\n",
    "        print(\"    La jointure temporelle sur 'timestamp_hour' est POSSIBLE.\")\n",
    "        print(\"   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "        print()\n",
    "        print(\"    Prochaine Ã©tape : notebooks/feature_eng.ipynb\")\n",
    "        print(\"      â†’ Jointure des prix et transactions par timestamp_hour\")\n",
    "        print(\"      â†’ CrÃ©ation des features pour le modÃ¨le de prÃ©diction\")\n",
    "    else:\n",
    "        # Pas de chevauchement\n",
    "        gap_days = (overlap_start - overlap_end).days\n",
    "        \n",
    "        print(\" AUCUN CHEVAUCHEMENT TEMPOREL !\")\n",
    "        print()\n",
    "        print(f\"    Les deux datasets ne se chevauchent pas.\")\n",
    "        print(f\"    Ã‰cart entre les deux : {gap_days} jour(s)\")\n",
    "        print()\n",
    "        print(\"   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "        print(\"    La jointure temporelle ne produira AUCUN rÃ©sultat.\")\n",
    "        print(\"   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "        print()\n",
    "        print(\"    Solutions possibles :\")\n",
    "        print(\"      1. TÃ©lÃ©charger des blocs Bitcoin couvrant la pÃ©riode des prix\")\n",
    "        print(\"      2. TÃ©lÃ©charger des prix couvrant la pÃ©riode des blocs\")\n",
    "        print(\"      3. Utiliser un dataset dÃ©rivÃ© (ex: BigQuery public dataset)\")\n",
    "\n",
    "elif market_min:\n",
    "    print(\"  Impossible de calculer le chevauchement\")\n",
    "    print(\"   â†’ transactions_parquet manquant ou vide\")\n",
    "    print(\"   â†’ Lancez d'abord : notebooks/etl_bitcoin_blocks.ipynb\")\n",
    "\n",
    "else:\n",
    "    print(\" Impossible de calculer le chevauchement\")\n",
    "    print(\"   â†’ Les deux Parquet sont manquants ou vides\")\n",
    "    print(\"   â†’ Lancez d'abord : scripts/run_etl.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99c43c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. RÃ©sumÃ© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5c235b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:20.976955Z",
     "iopub.status.busy": "2025-12-07T04:17:20.976459Z",
     "iopub.status.idle": "2025-12-07T04:17:20.991343Z",
     "shell.execute_reply": "2025-12-07T04:17:20.988370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“‹ RÃ‰SUMÃ‰ DU SANITY CHECK ETL GLOBAL\n",
      "============================================================\n",
      "\n",
      "âœ… market_parquet      : 10 colonnes, 7,317,759 lignes\n",
      "âœ… transactions_parquet : 11 colonnes, 2,236,800 lignes\n",
      "âœ… Chevauchement temporel : 42 jour(s) de donnÃ©es communes\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ TOUS LES CHECKS PASSENT !\n",
      "   â†’ Vous pouvez continuer avec feature_eng.ipynb\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\" RÃ‰SUMÃ‰ DU SANITY CHECK ETL GLOBAL\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Statut market\n",
    "market_ok = df_market is not None and not EXPECTED_MARKET_COLS.difference(set(df_market.columns))\n",
    "print(f\"{'ValidÃ©' if market_ok else 'Invalide'} market_parquet      : \", end=\"\")\n",
    "if df_market is not None:\n",
    "    print(f\"{len(df_market.columns)} colonnes, {market_count:,} lignes\")\n",
    "else:\n",
    "    print(\"NON DISPONIBLE\")\n",
    "\n",
    "# Statut transactions\n",
    "tx_ok = df_tx is not None and not EXPECTED_TX_COLS.difference(set(df_tx.columns))\n",
    "print(f\"{'ValidÃ©' if tx_ok else 'Invalide'} transactions_parquet : \", end=\"\")\n",
    "if df_tx is not None:\n",
    "    print(f\"{len(df_tx.columns)} colonnes, {tx_count:,} lignes\")\n",
    "else:\n",
    "    print(\"NON DISPONIBLE\")\n",
    "\n",
    "# Statut chevauchement\n",
    "overlap_ok = False\n",
    "if market_min and tx_min:\n",
    "    overlap_start = max(market_min, tx_min)\n",
    "    overlap_end = min(market_max, tx_max)\n",
    "    overlap_ok = overlap_start <= overlap_end\n",
    "\n",
    "print(f\"{'ValidÃ©' if overlap_ok else 'Invalide'} Chevauchement temporel : \", end=\"\")\n",
    "if overlap_ok:\n",
    "    overlap_days = (overlap_end.date() - overlap_start.date()).days + 1\n",
    "    print(f\"{overlap_days} jour(s) de donnÃ©es communes\")\n",
    "else:\n",
    "    print(\"AUCUN\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if market_ok and tx_ok and overlap_ok:\n",
    "    print(\" TOUS LES CHECKS PASSENT !\")\n",
    "    print(\"   â†’ Vous pouvez continuer avec feature_eng.ipynb\")\n",
    "elif market_ok and overlap_ok:\n",
    "    print(\"  CHECKS PARTIELS\")\n",
    "    print(\"   â†’ transactions_parquet manquant ou incomplet\")\n",
    "    print(\"   â†’ Le feature engineering sera limitÃ© aux donnÃ©es prix\")\n",
    "else:\n",
    "    print(\" CHECKS Ã‰CHOUÃ‰S\")\n",
    "    print(\"   â†’ VÃ©rifiez les ETL avant de continuer\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6187cd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T04:17:20.997040Z",
     "iopub.status.busy": "2025-12-07T04:17:20.996281Z",
     "iopub.status.idle": "2025-12-07T04:17:21.092464Z",
     "shell.execute_reply": "2025-12-07T04:17:21.090154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session fermÃ©e\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage\n",
    "spark.stop()\n",
    "print(\"Spark session fermÃ©e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
