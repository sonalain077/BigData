{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d0737a-05de-4225-84bc-bcf161362707",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e79e75f-125f-404f-a62e-88683cd2c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project Root: /home/img/BigData/Project\n",
      " Config loaded: /home/img/BigData/Project/conf/bda_project_config.yml\n",
      " Project Name: Bitcoin Price Predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 11:37:56 WARN Utils: Your hostname, a03-341a, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 11:37:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 11:37:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/07 11:37:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/07 11:37:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "App name: BTC_ETL_Custom\n",
      "Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType, TimestampType\n",
    "\n",
    "\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd().resolve()\n",
    "    if current.name == \"notebooks\":\n",
    "        candidate = current.parent\n",
    "        if (candidate / \"conf\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    search = current\n",
    "    while search != search.parent:  # Jusqu'Ã  la racine du filesystem\n",
    "        if (search / \"conf\").exists() and (search / \"conf\" / \"bda_project_config.yml\").exists():\n",
    "            return search\n",
    "        search = search.parent\n",
    "    \n",
    "    if (current / \"conf\").exists():\n",
    "        return current\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find project root (looked for 'conf/' folder)\\n\"\n",
    "        f\"Started from: {Path.cwd()}\\n\"\n",
    "        f\"Tip: Run this notebook from the Project/ or Project/notebooks/ directory\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "CONFIG_PATH = PROJECT_ROOT / \"conf\" / \"bda_project_config.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "PATHS = config['paths']\n",
    "SPARK_CFG = config['spark']\n",
    "\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Config loaded: {CONFIG_PATH}\")\n",
    "print(f\" Project Name: {config['project']['name']}\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(SPARK_CFG['app_name'])\n",
    "    .master(SPARK_CFG['master'])\n",
    "    .config(\"spark.driver.memory\", SPARK_CFG['driver_memory'])\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"App name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b13a2-d658-4f5f-ba64-cc08cb8bf742",
   "metadata": {},
   "source": [
    "# 1. File Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0d9e2a-73cc-4203-95ba-6a5c70989c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PATHS CONFIGURED (relative to PROJECT_ROOT)\n",
      "============================================================\n",
      "Input CSV      : /home/img/BigData/Project/data/prices/btcusd_1-min_data.csv\n",
      "Output Parquet : /home/img/BigData/Project/data/output/market_parquet\n",
      "Metrics File   : /home/img/BigData/Project/project_metrics_log.csv\n",
      "\n",
      "CSV exists: True\n"
     ]
    }
   ],
   "source": [
    "MARKET_DATA_PATH = PROJECT_ROOT / PATHS['market_data']\n",
    "\n",
    "OUTPUT_MARKET_PATH = PROJECT_ROOT / PATHS['output_market']\n",
    "\n",
    "METRICS_FILE = PROJECT_ROOT / PATHS['metrics_file']\n",
    "\n",
    "EVIDENCE_DIR = PROJECT_ROOT / \"evidence\"\n",
    "\n",
    "OUTPUT_MARKET_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EVIDENCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATHS CONFIGURED (relative to PROJECT_ROOT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input CSV      : {MARKET_DATA_PATH}\")\n",
    "print(f\"Output Parquet : {OUTPUT_MARKET_PATH}\")\n",
    "print(f\"Metrics File   : {METRICS_FILE}\")\n",
    "print(f\"\\nCSV exists: {MARKET_DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8f681-f908-4e90-8c27-b39775fcfb21",
   "metadata": {},
   "source": [
    "# 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad55daed-4f82-41b4-812b-c60aa0af1722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows loaded: 7,317,759\n",
      "Columns: ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "root\n",
      " |-- Timestamp: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n",
      "+------------+----+----+----+-----+------+\n",
      "|Timestamp   |Open|High|Low |Close|Volume|\n",
      "+------------+----+----+----+-----+------+\n",
      "|1.32541206E9|4.58|4.58|4.58|4.58 |0.0   |\n",
      "|1.32541212E9|4.58|4.58|4.58|4.58 |0.0   |\n",
      "|1.32541218E9|4.58|4.58|4.58|4.58 |0.0   |\n",
      "|1.32541224E9|4.58|4.58|4.58|4.58 |0.0   |\n",
      "|1.3254123E9 |4.58|4.58|4.58|4.58 |0.0   |\n",
      "+------------+----+----+----+-----+------+\n",
      "only showing top 5 rows\n",
      "Date range: 2012-01-01 10:01:00 -> 2025-11-30 23:59:00\n",
      "Duration: 5082 days\n"
     ]
    }
   ],
   "source": [
    "price_schema = StructType([\n",
    "    StructField(\"Timestamp\", DoubleType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Volume\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "raw_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(price_schema)\n",
    "    .csv(str(MARKET_DATA_PATH))\n",
    ")\n",
    "\n",
    "raw_df.cache()\n",
    "raw_count = raw_df.count()\n",
    "\n",
    "print(f\"Rows loaded: {raw_count:,}\")\n",
    "print(f\"Columns: {raw_df.columns}\")\n",
    "\n",
    "raw_df.printSchema()\n",
    "raw_df.show(5, truncate=False)\n",
    "\n",
    "ts_stats = raw_df.agg(\n",
    "    F.min(\"Timestamp\").alias(\"min_ts\"),\n",
    "    F.max(\"Timestamp\").alias(\"max_ts\")\n",
    ").collect()[0]\n",
    "\n",
    "min_date = datetime.utcfromtimestamp(ts_stats[\"min_ts\"])\n",
    "max_date = datetime.utcfromtimestamp(ts_stats[\"max_ts\"])\n",
    "\n",
    "print(f\"Date range: {min_date} -> {max_date}\")\n",
    "print(f\"Duration: {(max_date - min_date).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77fba1-e512-4dc6-b1fb-42f6b391936b",
   "metadata": {},
   "source": [
    "# 3. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b7544a-5dc9-4cf2-b225-430db102f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after transformation:\n",
      "root\n",
      " |-- Timestamp: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- timestamp_hour: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n",
      "+------------+-------------------+--------------+----------+----+-----+\n",
      "|Timestamp   |datetime           |timestamp_hour|date      |hour|Close|\n",
      "+------------+-------------------+--------------+----------+----+-----+\n",
      "|1.32541206E9|2012-01-01 10:01:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541212E9|2012-01-01 10:02:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541218E9|2012-01-01 10:03:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541224E9|2012-01-01 10:04:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.3254123E9 |2012-01-01 10:05:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541236E9|2012-01-01 10:06:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541242E9|2012-01-01 10:07:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541248E9|2012-01-01 10:08:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.32541254E9|2012-01-01 10:09:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "|1.3254126E9 |2012-01-01 10:10:00|1325412000    |2012-01-01|10  |4.58 |\n",
      "+------------+-------------------+--------------+----------+----+-----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "transformed_df = (\n",
    "    raw_df\n",
    "    .withColumn(\"datetime\", F.from_unixtime(F.col(\"Timestamp\")).cast(TimestampType()))\n",
    "    .withColumn(\"timestamp_hour\", (F.floor(F.col(\"Timestamp\") / 3600) * 3600).cast(LongType()))\n",
    "    .withColumn(\"date\", F.to_date(F.col(\"datetime\")))\n",
    "    .withColumn(\"hour\", F.hour(F.col(\"datetime\")))\n",
    ")\n",
    "\n",
    "print(\"Schema after transformation:\")\n",
    "transformed_df.printSchema()\n",
    "\n",
    "\n",
    "transformed_df.select(\n",
    "    \"Timestamp\", \"datetime\", \"timestamp_hour\", \"date\", \"hour\", \"Close\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e16a84-cce3-46db-a328-b2f01199694f",
   "metadata": {},
   "source": [
    "# 4. Spark Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b45d38-b56a-44d0-8cc0-edc6353cb543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPARK EXECUTION PLAN\n",
      "============================================================\n",
      "== Physical Plan ==\n",
      "* Project (5)\n",
      "+- * Project (4)\n",
      "   +- InMemoryTableScan (1) (columnarIn=false, columnarOut=true)\n",
      "         +- InMemoryRelation (2)\n",
      "               +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [6]: [Close#4, High#2, Low#3, Open#1, Timestamp#0, Volume#5]\n",
      "Arguments: [Close#4, High#2, Low#3, Open#1, Timestamp#0, Volume#5]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [Timestamp#0, Open#1, High#2, Low#3, Close#4, Volume#5], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [6]: [Timestamp#0, Open#1, High#2, Low#3, Close#4, Volume#5]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/img/BigData/Project/data/prices/btcusd_1-min_data.csv]\n",
      "ReadSchema: struct<Timestamp:double,Open:double,High:double,Low:double,Close:double,Volume:double>\n",
      "\n",
      "(4) Project [codegen id : 1]\n",
      "Output [8]: [Timestamp#0, Open#1, High#2, Low#3, Close#4, Volume#5, cast(from_unixtime(cast(Timestamp#0 as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp) AS datetime#508, (FLOOR((Timestamp#0 / 3600.0)) * 3600) AS timestamp_hour#509L]\n",
      "Input [6]: [Close#4, High#2, Low#3, Open#1, Timestamp#0, Volume#5]\n",
      "\n",
      "(5) Project [codegen id : 1]\n",
      "Output [10]: [Timestamp#0, Open#1, High#2, Low#3, Close#4, Volume#5, datetime#508, timestamp_hour#509L, cast(datetime#508 as date) AS date#510, hour(datetime#508, Some(UTC)) AS hour#511]\n",
      "Input [8]: [Timestamp#0, Open#1, High#2, Low#3, Close#4, Volume#5, datetime#508, timestamp_hour#509L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SPARK EXECUTION PLAN\")\n",
    "print(\"=\" * 60)\n",
    "transformed_df.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b94dfdb-5158-4e38-a074-d5225914021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan saved to: /home/img/BigData/Project/evidence/market_etl_explain.txt\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "plan_buffer = StringIO()\n",
    "with redirect_stdout(plan_buffer):\n",
    "    transformed_df.explain(\"formatted\")\n",
    "\n",
    "plan_file = EVIDENCE_DIR / \"market_etl_explain.txt\"\n",
    "plan_file.write_text(f\"# Market ETL Execution Plan\\n# Date: {datetime.now()}\\n\\n{plan_buffer.getvalue()}\")\n",
    "\n",
    "print(f\"Execution plan saved to: {plan_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d2669-c44d-4748-96b4-685147501a50",
   "metadata": {},
   "source": [
    "# 6. Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46883d5f-31bd-4dbb-8fbe-e3aac79503ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Schema:\n",
      "root\n",
      " |-- timestamp_unix: double (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- timestamp_hour: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = transformed_df.select(\n",
    "    F.col(\"Timestamp\").alias(\"timestamp_unix\"),\n",
    "    \"datetime\",\n",
    "    \"timestamp_hour\",\n",
    "    \"date\",\n",
    "    \"hour\",\n",
    "    F.col(\"Open\").alias(\"open\"),\n",
    "    F.col(\"High\").alias(\"high\"),\n",
    "    F.col(\"Low\").alias(\"low\"),\n",
    "    F.col(\"Close\").alias(\"close\"),\n",
    "    F.col(\"Volume\").alias(\"volume\")\n",
    ")\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c641f470-a61e-40f1-aae0-587b54cdcf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================>                   (8 + 4) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parquet saved to: /home/img/BigData/Project/data/output/market_parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "(\n",
    "    final_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .parquet(str(OUTPUT_MARKET_PATH))\n",
    ")\n",
    "\n",
    "print(f\"  Parquet saved to: {OUTPUT_MARKET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd61193-e1f1-4322-b4af-9c3b276ef14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files created: 12\n",
      "Total size: 198.50 MB\n",
      "Rows in Parquet: 7,317,759\n",
      "Integrity check: OK\n"
     ]
    }
   ],
   "source": [
    "parquet_files = list(OUTPUT_MARKET_PATH.glob(\"*.parquet\"))\n",
    "total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "\n",
    "print(f\"Parquet files created: {len(parquet_files)}\")\n",
    "print(f\"Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "verify_df = spark.read.parquet(str(OUTPUT_MARKET_PATH))\n",
    "verify_count = verify_df.count()\n",
    "\n",
    "print(f\"Rows in Parquet: {verify_count:,}\")\n",
    "print(f\"Integrity check: {'OK' if verify_count == raw_count else 'MISMATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88675b5-6eed-4ac2-b4d3-e8cd10359618",
   "metadata": {},
   "source": [
    "# 7. Spark Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8118b13-ddc2-4f5d-a7b1-0c1268167e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Metrics logged to: /home/img/BigData/Project/project_metrics_log.csv\n",
      "   Run ID: market_etl_20251207_113821\n",
      "   Input size: 365.96 MB\n",
      "   Output size: 198.50 MB\n",
      "Cache released.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "run_id = f\"market_etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "if not METRICS_FILE.exists():\n",
    "    with open(METRICS_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['run_id', 'task', 'note', 'files_read', 'input_size_bytes', \n",
    "                        'shuffle_read_bytes', 'shuffle_write_bytes', 'timestamp'])\n",
    "\n",
    "\n",
    "input_size = MARKET_DATA_PATH.stat().st_size if MARKET_DATA_PATH.exists() else 0\n",
    "output_size = sum(f.stat().st_size for f in OUTPUT_MARKET_PATH.glob(\"*.parquet\"))\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "with open(METRICS_FILE, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        run_id,                    # run_id\n",
    "        \"market_etl\",              # task\n",
    "        f\"rows={raw_count}\",       # note\n",
    "        1,                         # files_read (1 CSV)\n",
    "        input_size,                # input_size_bytes\n",
    "        0,                         # shuffle_read_bytes (pas de shuffle)\n",
    "        0,                         # shuffle_write_bytes\n",
    "        timestamp                  # timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"   Metrics logged to: {METRICS_FILE}\")\n",
    "print(f\"   Run ID: {run_id}\")\n",
    "print(f\"   Input size: {input_size / (1024*1024):.2f} MB\")\n",
    "print(f\"   Output size: {output_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "\n",
    "raw_df.unpersist()\n",
    "print(\"Cache released.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77188e17-789c-4b03-be1e-598aad398aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
