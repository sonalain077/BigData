{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047908b3",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11ec5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/img/BigData/Project\n",
      "Config loaded: /home/img/BigData/Project/conf/bda_project_config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 12:21:27 WARN Utils: Your hostname, a03-341a, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 12:21:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 12:21:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/07 12:21:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/07 12:21:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/07 12:21:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/12/07 12:21:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd().resolve()\n",
    "    if current.name == \"notebooks\":\n",
    "        candidate = current.parent\n",
    "        if (candidate / \"conf\").exists():\n",
    "            return candidate\n",
    "    search = current\n",
    "    while search != search.parent:\n",
    "        if (search / \"conf\").exists() and (search / \"conf\" / \"bda_project_config.yml\").exists():\n",
    "            return search\n",
    "        search = search.parent\n",
    "    if (current / \"conf\").exists():\n",
    "        return current\n",
    "    raise FileNotFoundError(f\"Cannot find project root from {Path.cwd()}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "CONFIG_PATH = PROJECT_ROOT / \"conf\" / \"bda_project_config.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "PATHS = config['paths']\n",
    "SPARK_CFG = config['spark']\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Config loaded: {CONFIG_PATH}\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(f\"{SPARK_CFG['app_name']}_ModelTraining\")\n",
    "    .master(SPARK_CFG['master'])\n",
    "    .config(\"spark.driver.memory\", SPARK_CFG['driver_memory'])\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2ace3",
   "metadata": {},
   "source": [
    "# 1. Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ab84e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Path: /home/img/BigData/Project/data/output/features_parquet\n",
      "Features exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 933\n",
      "Columns: 20\n",
      "root\n",
      " |-- timestamp_hour: long (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- return_1h: double (nullable = true)\n",
      " |-- return_24h: double (nullable = true)\n",
      " |-- volatility_24h: double (nullable = true)\n",
      " |-- volume_exchange: double (nullable = true)\n",
      " |-- tx_count: long (nullable = true)\n",
      " |-- volume_btc: double (nullable = true)\n",
      " |-- avg_block_size: double (nullable = true)\n",
      " |-- whale_tx_count: long (nullable = true)\n",
      " |-- whale_volume_btc: double (nullable = true)\n",
      " |-- miner_issuance_btc: double (nullable = true)\n",
      " |-- nvt_like: double (nullable = true)\n",
      " |-- onchain_vs_exchange: double (nullable = true)\n",
      " |-- tx_count_zscore: double (nullable = true)\n",
      " |-- whale_zscore: double (nullable = true)\n",
      " |-- issuance_zscore: double (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "FEATURES_PATH = PROJECT_ROOT / PATHS.get('output_features', 'data/output_features')\n",
    "METRICS_FILE = PROJECT_ROOT / PATHS['metrics_file']\n",
    "EVIDENCE_DIR = PROJECT_ROOT / \"evidence\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVIDENCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Features Path: {FEATURES_PATH}\")\n",
    "print(f\"Features exists: {FEATURES_PATH.exists()}\")\n",
    "\n",
    "df = spark.read.parquet(str(FEATURES_PATH))\n",
    "df.cache()\n",
    "\n",
    "total_count = df.count()\n",
    "print(f\"\\nTotal samples: {total_count:,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef34db",
   "metadata": {},
   "source": [
    "# 2. Temporal Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95c6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range: 2013-12-06 23:00:00 -> 2014-01-14 23:00:00\n",
      "Split point (80%): 2014-01-07 03:48:00\n",
      "\n",
      "Train set: 748 samples (80.2%)\n",
      "Test set:  185 samples (19.8%)\n",
      "\n",
      "Label distribution:\n",
      "Train:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  380|\n",
      "|    0|  368|\n",
      "+-----+-----+\n",
      "\n",
      "Test:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|   97|\n",
      "|    0|   88|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_range = df.agg(\n",
    "    F.min(\"timestamp_hour\").alias(\"min_ts\"),\n",
    "    F.max(\"timestamp_hour\").alias(\"max_ts\")\n",
    ").collect()[0]\n",
    "\n",
    "min_ts = time_range['min_ts']\n",
    "max_ts = time_range['max_ts']\n",
    "split_ts = min_ts + int(0.8 * (max_ts - min_ts))\n",
    "\n",
    "print(f\"Zone temporelle couverte: {datetime.utcfromtimestamp(min_ts)} -> {datetime.utcfromtimestamp(max_ts)}\")\n",
    "print(f\"Split à partir de : {datetime.utcfromtimestamp(split_ts)}\")\n",
    "\n",
    "train_df = df.filter(F.col(\"timestamp_hour\") < split_ts)\n",
    "test_df = df.filter(F.col(\"timestamp_hour\") >= split_ts)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\nTrain set: {train_count:,} lignes ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"Test set:  {test_count:,} lignes ({test_count/total_count*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n Vérification de la distribution des classes du label:\")\n",
    "print(\"Train:\")\n",
    "train_df.groupBy(\"label\").count().show()\n",
    "print(\"Test:\")\n",
    "test_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510ef4e",
   "metadata": {},
   "source": [
    "# 3. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37981976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SETS ===\n",
      "\n",
      "ALL features (18):\n",
      "   1. close  [RAW]\n",
      "   2. return_1h\n",
      "   3. return_24h\n",
      "   4. volatility_24h\n",
      "   5. volume_exchange  [RAW]\n",
      "   6. tx_count  [RAW]\n",
      "   7. volume_btc  [RAW]\n",
      "   8. avg_block_size  [RAW]\n",
      "   9. whale_tx_count  [RAW]\n",
      "  10. whale_volume_btc  [RAW]\n",
      "  11. miner_issuance_btc\n",
      "  12. nvt_like\n",
      "  13. onchain_vs_exchange\n",
      "  14. tx_count_zscore\n",
      "  15. whale_zscore\n",
      "  16. issuance_zscore\n",
      "  17. hour_of_day\n",
      "  18. day_of_week\n",
      "\n",
      "GENERALISABLE features (11) - sans valeurs brutes:\n",
      "   1. return_1h\n",
      "   2. return_24h\n",
      "   3. volatility_24h\n",
      "   4. miner_issuance_btc\n",
      "   5. nvt_like\n",
      "   6. onchain_vs_exchange\n",
      "   7. tx_count_zscore\n",
      "   8. whale_zscore\n",
      "   9. issuance_zscore\n",
      "  10. hour_of_day\n",
      "  11. day_of_week\n",
      "Data cleaned (NaN -> 0)\n",
      " VectorAssembler and StandardScaler ready\n"
     ]
    }
   ],
   "source": [
    "all_feature_columns = [col for col in df.columns if col not in ['timestamp_hour', 'label']]\n",
    "\n",
    "RAW_VALUE_FEATURES = [\n",
    "    'close',           # Prix brut dépend de l'époque\n",
    "    'close_lag1', 'close_lag2', 'close_lag3', 'close_lag6', 'close_lag12', 'close_lag24',  # Lags du prix brut\n",
    "    'volume_btc',      # Volume brut - depend de l'adoption\n",
    "    'volume_exchange', # Volume exchange brut\n",
    "    'tx_count',        # Nombre de tx brut - depend de l'adoption\n",
    "    'whale_tx_count', 'whale_volume_btc',  # Valeurs brutes whales\n",
    "    'avg_block_size',  # Taille brute\n",
    "]\n",
    "\n",
    "\n",
    "generalisable_features = [col for col in all_feature_columns if col not in RAW_VALUE_FEATURES]\n",
    "\n",
    "print(f\"\\nListe de toutes les features ({len(all_feature_columns)}):\")\n",
    "for i, col in enumerate(all_feature_columns, 1):\n",
    "    marker = \"  [Brutes]\" if col in RAW_VALUE_FEATURES else \"\"\n",
    "    print(f\"  {i:2d}. {col}{marker}\")\n",
    "\n",
    "print(f\"\\n Features générales ({len(generalisable_features)}):\")\n",
    "for i, col in enumerate(generalisable_features, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "\n",
    "feature_columns = all_feature_columns  \n",
    "\n",
    "\n",
    "train_clean = train_df.na.fill(0)\n",
    "test_clean = test_df.na.fill(0)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd79904",
   "metadata": {},
   "source": [
    "# 4. Baseline Model - Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5157d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe majoritaire (train): 1\n",
      "\n",
      "=== BASELINE ===\n",
      "Strategy: Always predict class 1\n",
      "Baseline Accuracy: 0.5243 (52.43%)\n",
      "\n",
      "Notre modele doit battre: 52.43%\n"
     ]
    }
   ],
   "source": [
    "majority_class = train_clean.groupBy(\"label\").count().orderBy(F.desc(\"count\")).first()[\"label\"]\n",
    "print(f\"Classe majoritaire (train): {majority_class}\")\n",
    "\n",
    "test_majority_count = test_clean.filter(F.col(\"label\") == majority_class).count()\n",
    "baseline_accuracy = test_majority_count / test_count\n",
    "\n",
    "print(f\"\\n=== BASELINE ===\")\n",
    "print(f\"Strategie : Toujours prédire la classe {majority_class}\")\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nNotre modèle doit battre: {baseline_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b91c72",
   "metadata": {},
   "source": [
    "# 5. Model Training - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3972117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:21:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/07 12:21:46 WARN DAGScheduler: Broadcasting large task binary with size 1187.6 KiB\n",
      "25/12/07 12:21:47 WARN DAGScheduler: Broadcasting large task binary with size 1446.4 KiB\n",
      "25/12/07 12:21:47 WARN DAGScheduler: Broadcasting large task binary with size 1698.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.84s\n",
      "+--------------+-----+----------+--------------------+\n",
      "|timestamp_hour|label|prediction|         probability|\n",
      "+--------------+-----+----------+--------------------+\n",
      "|    1389067200|    1|       0.0|[0.57960122099219...|\n",
      "|    1389070800|    1|       1.0|[0.49917989895980...|\n",
      "|    1389074400|    0|       0.0|[0.62012156436929...|\n",
      "|    1389078000|    0|       0.0|[0.60592155538844...|\n",
      "|    1389081600|    0|       0.0|[0.60661454415436...|\n",
      "|    1389085200|    1|       1.0|[0.46074951594725...|\n",
      "|    1389088800|    0|       0.0|[0.69595878784893...|\n",
      "|    1389092400|    1|       1.0|[0.43563204007016...|\n",
      "|    1389096000|    0|       1.0|[0.48906253750196...|\n",
      "|    1389099600|    1|       1.0|[0.40581418526870...|\n",
      "+--------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:21:49 WARN DAGScheduler: Broadcasting large task binary with size 1047.8 KiB\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = rf_pipeline.fit(train_clean)\n",
    "\n",
    "rf_train_time = time.time() - start_time\n",
    "print(f\"Temps d'entrainement: {rf_train_time:.2f}s\")\n",
    "\n",
    "rf_predictions = rf_model.transform(test_clean)\n",
    "rf_predictions.select(\"timestamp_hour\", \"label\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ad50f",
   "metadata": {},
   "source": [
    "# 6. Model Training - Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f756ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosted Trees...\n",
      "Training time: 16.53s\n",
      "+--------------+-----+----------+\n",
      "|timestamp_hour|label|prediction|\n",
      "+--------------+-----+----------+\n",
      "|    1389067200|    1|       0.0|\n",
      "|    1389070800|    1|       0.0|\n",
      "|    1389074400|    0|       0.0|\n",
      "|    1389078000|    0|       0.0|\n",
      "|    1389081600|    0|       0.0|\n",
      "|    1389085200|    1|       1.0|\n",
      "|    1389088800|    0|       0.0|\n",
      "|    1389092400|    1|       0.0|\n",
      "|    1389096000|    0|       1.0|\n",
      "|    1389099600|    1|       1.0|\n",
      "+--------------+-----+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:22:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "print(\"Training Gradient Boosted Trees...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gbt_model = gbt_pipeline.fit(train_clean)\n",
    "\n",
    "gbt_train_time = time.time() - start_time\n",
    "print(f\"Temps d'entrainement: {gbt_train_time:.2f}s\")\n",
    "\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test_clean)\n",
    "gbt_predictions.select(\"timestamp_hour\", \"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a490ed4",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c0a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:22:06 WARN DAGScheduler: Broadcasting large task binary with size 1069.1 KiB\n",
      "25/12/07 12:22:06 WARN DAGScheduler: Broadcasting large task binary with size 1069.1 KiB\n",
      "25/12/07 12:22:07 WARN DAGScheduler: Broadcasting large task binary with size 1056.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "Model                    Accuracy     F1-Score      AUC-ROC\n",
      "------------------------------------------------------------\n",
      "Baseline (majority)        0.5243          N/A       0.5000\n",
      "Random Forest              0.5730       0.5730       0.6105\n",
      "GBT                        0.5459       0.5458       0.5975\n",
      "============================================================\n",
      "\n",
      "Best model: Random Forest (AUC: 0.6105)\n",
      "Improvement over baseline: +9.28%\n"
     ]
    }
   ],
   "source": [
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "multi_evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "multi_evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "rf_accuracy = multi_evaluator_acc.evaluate(rf_predictions)\n",
    "rf_f1 = multi_evaluator_f1.evaluate(rf_predictions)\n",
    "rf_auc = binary_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "gbt_accuracy = multi_evaluator_acc.evaluate(gbt_predictions)\n",
    "gbt_f1 = multi_evaluator_f1.evaluate(gbt_predictions)\n",
    "gbt_auc = binary_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Evaluation des résultats des modèles\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'Accuracy':>12} {'F1-Score':>12} {'AUC-ROC':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Baseline':<20} {baseline_accuracy:>12.4f} {'N/A':>12} {'0.5000':>12}\")\n",
    "print(f\"{'Random Forest':<20} {rf_accuracy:>12.4f} {rf_f1:>12.4f} {rf_auc:>12.4f}\")\n",
    "print(f\"{'GBT':<20} {gbt_accuracy:>12.4f} {gbt_f1:>12.4f} {gbt_auc:>12.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_model_name = \"Random Forest\" if rf_auc > gbt_auc else \"GBT\"\n",
    "best_auc = max(rf_auc, gbt_auc)\n",
    "print(f\"\\nBest model: {best_model_name} (AUC: {best_auc:.4f})\")\n",
    "\n",
    "best_accuracy = rf_accuracy if rf_auc > gbt_auc else gbt_accuracy\n",
    "improvement = (best_accuracy - baseline_accuracy) / baseline_accuracy * 100\n",
    "print(f\"Improvement over baseline: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b4795",
   "metadata": {},
   "source": [
    "# 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c678b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE IMPORTANCE (Random Forest)\n",
      "============================================================\n",
      "Rank   Feature                     Importance\n",
      "------------------------------------------------------------\n",
      "1      return_1h                       0.0962 ████\n",
      "2      tx_count_zscore                 0.0875 ████\n",
      "3      volume_btc                      0.0869 ████\n",
      "4      volume_exchange                 0.0852 ████\n",
      "5      hour_of_day                     0.0840 ████\n",
      "6      tx_count                        0.0796 ███\n",
      "7      return_24h                      0.0774 ███\n",
      "8      nvt_like                        0.0763 ███\n",
      "9      avg_block_size                  0.0757 ███\n",
      "10     onchain_vs_exchange             0.0723 ███\n",
      "11     volatility_24h                  0.0698 ███\n",
      "12     close                           0.0649 ███\n",
      "13     day_of_week                     0.0443 ██\n",
      "14     whale_tx_count                  0.0000 \n",
      "15     whale_volume_btc                0.0000 \n",
      "------------------------------------------------------------\n",
      "\n",
      "Top 5 features:\n",
      "  - return_1h: 0.0962\n",
      "  - tx_count_zscore: 0.0875\n",
      "  - volume_btc: 0.0869\n",
      "  - volume_exchange: 0.0852\n",
      "  - hour_of_day: 0.0840\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = rf_model.stages[-1]\n",
    "importances = rf_classifier.featureImportances.toArray()\n",
    "\n",
    "feature_importance = list(zip(feature_columns, importances))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Features les plus importantes pour le Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<6} {'Feature':<25} {'Importance':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (feat, imp) in enumerate(feature_importance[:15], 1):\n",
    "    bar = \"█\" * int(imp * 50)\n",
    "    print(f\"{i:<6} {feat:<25} {imp:>12.4f} {bar}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nTop 5 features:\")\n",
    "for feat, imp in feature_importance[:5]:\n",
    "    print(f\"  - {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7446ea9",
   "metadata": {},
   "source": [
    "# 9. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff8b25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RF with GENERALISABLE features only (11 features)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:22:09 WARN DAGScheduler: Broadcasting large task binary with size 1132.1 KiB\n",
      "25/12/07 12:22:09 WARN DAGScheduler: Broadcasting large task binary with size 1400.7 KiB\n",
      "25/12/07 12:22:10 WARN DAGScheduler: Broadcasting large task binary with size 1660.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:22:11 WARN DAGScheduler: Broadcasting large task binary with size 1030.6 KiB\n",
      "25/12/07 12:22:11 WARN DAGScheduler: Broadcasting large task binary with size 1030.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTATS\n",
      "======================================================================\n",
      "Feature Set                      N_Features     Accuracy      AUC-ROC\n",
      "----------------------------------------------------------------------\n",
      "Baseline (majority)                       -       0.5243       0.5000\n",
      "ALL features                             18       0.5730       0.6105\n",
      "GENERALISABLE only                       11       0.5405       0.5907\n",
      "======================================================================\n",
      "\n",
      " Analysis:\n",
      "   Accuracy drop without raw values: +5.66%\n",
      "   AUC drop without raw values: +3.24%\n",
      "\n",
      " WARNING: Le modele depend beaucoup des valeurs brutes.\n",
      "   -> Risque d'overfitting temporel, moins generalisable.\n",
      "\n",
      " Top 5 features (GENERALISABLE model):\n",
      "   1. nvt_like: 0.1538\n",
      "   2. return_1h: 0.1388\n",
      "   3. onchain_vs_exchange: 0.1377\n",
      "   4. return_24h: 0.1336\n",
      "   5. tx_count_zscore: 0.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:22:12 WARN DAGScheduler: Broadcasting large task binary with size 1018.3 KiB\n"
     ]
    }
   ],
   "source": [
    "assembler_gen = VectorAssembler(\n",
    "    inputCols=generalisable_features,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler_gen = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "rf_gen = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline_gen = Pipeline(stages=[assembler_gen, scaler_gen, rf_gen])\n",
    "\n",
    "print(f\"\\n Entrainement du Random Forest qu'avec des features générales({len(generalisable_features)} features)...\")\n",
    "start_time = time.time()\n",
    "model_gen = pipeline_gen.fit(train_clean)\n",
    "gen_train_time = time.time() - start_time\n",
    "print(f\"Temps d'entrainement: {gen_train_time:.2f}s\")\n",
    "\n",
    "predictions_gen = model_gen.transform(test_clean)\n",
    "\n",
    "gen_accuracy = multi_evaluator_acc.evaluate(predictions_gen)\n",
    "gen_f1 = multi_evaluator_f1.evaluate(predictions_gen)\n",
    "gen_auc = binary_evaluator.evaluate(predictions_gen)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTATS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature Set':<30} {'N_Features':>12} {'Accuracy':>12} {'AUC-ROC':>12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Baseline (majority)':<30} {'-':>12} {baseline_accuracy:>12.4f} {'0.5000':>12}\")\n",
    "print(f\"{'ALL features':<30} {len(all_feature_columns):>12} {rf_accuracy:>12.4f} {rf_auc:>12.4f}\")\n",
    "print(f\"{'GENERALISABLE only':<30} {len(generalisable_features):>12} {gen_accuracy:>12.4f} {gen_auc:>12.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "acc_drop = (rf_accuracy - gen_accuracy) / rf_accuracy * 100\n",
    "auc_drop = (rf_auc - gen_auc) / rf_auc * 100\n",
    "\n",
    "print(f\"\\n Analysis:\")\n",
    "print(f\"   Accuracy drop without raw values: {acc_drop:+.2f}%\")\n",
    "print(f\"   AUC drop without raw values: {auc_drop:+.2f}%\")\n",
    "\n",
    "if abs(acc_drop) < 5:\n",
    "    print(f\"\\n GOOD: Le modele reste performant sans valeurs brutes!\")\n",
    "    print(f\"   -> Le modele est probablement generalisable a d'autres periodes.\")\n",
    "else:\n",
    "    print(f\"\\n WARNING: Le modele depend beaucoup des valeurs brutes.\")\n",
    "    print(f\"   -> Risque d'overfitting temporel, moins generalisable.\")\n",
    "\n",
    "rf_gen_classifier = model_gen.stages[-1]\n",
    "importances_gen = rf_gen_classifier.featureImportances.toArray()\n",
    "feature_importance_gen = list(zip(generalisable_features, importances_gen))\n",
    "feature_importance_gen.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n Top 5 features (GENERALISABLE model):\")\n",
    "for i, (feat, imp) in enumerate(feature_importance_gen[:5], 1):\n",
    "    print(f\"   {i}. {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b9cd513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 12:22:12 WARN DAGScheduler: Broadcasting large task binary with size 1065.5 KiB\n",
      "25/12/07 12:22:12 WARN DAGScheduler: Broadcasting large task binary with size 1009.3 KiB\n",
      "25/12/07 12:22:12 WARN DAGScheduler: Broadcasting large task binary with size 1009.1 KiB\n",
      "25/12/07 12:22:12 WARN DAGScheduler: Broadcasting large task binary with size 1009.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "CONFUSION MATRIX (Random Forest)\n",
      "========================================\n",
      "                  Predicted\n",
      "                  0       1\n",
      "Actual  0        53      35\n",
      "        1        44      53\n",
      "========================================\n",
      "\n",
      "Precision (class 1): 0.6023\n",
      "Recall (class 1):    0.5464\n",
      "Specificity:         0.6023\n"
     ]
    }
   ],
   "source": [
    "best_predictions = rf_predictions if rf_auc > gbt_auc else gbt_predictions\n",
    "\n",
    "confusion = best_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "confusion_data = confusion.collect()\n",
    "\n",
    "tn = fp = fn = tp = 0\n",
    "for row in confusion_data:\n",
    "    if row['label'] == 0 and row['prediction'] == 0:\n",
    "        tn = row['count']\n",
    "    elif row['label'] == 0 and row['prediction'] == 1:\n",
    "        fp = row['count']\n",
    "    elif row['label'] == 1 and row['prediction'] == 0:\n",
    "        fn = row['count']\n",
    "    elif row['label'] == 1 and row['prediction'] == 1:\n",
    "        tp = row['count']\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"CONFUSION MATRIX ({best_model_name})\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"                  Predicted\")\n",
    "print(f\"                  0       1\")\n",
    "print(f\"Actual  0      {tn:4d}    {fp:4d}\")\n",
    "print(f\"        1      {fn:4d}    {tp:4d}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"\\nPrecision : {precision:.4f}\")\n",
    "print(f\"Recall :    {recall:.4f}\")\n",
    "print(f\"Specificity:         {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05363d53",
   "metadata": {},
   "source": [
    "# 10. Save Results & Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5881162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged to: /home/img/BigData/Project/project_metrics_log.csv\n",
      "Summary saved to: /home/img/BigData/Project/evidence/model_training_summary.txt\n",
      "Best model saved to: /home/img/BigData/Project/models/best_model_random_forest\n"
     ]
    }
   ],
   "source": [
    "run_id = f\"model_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "if not METRICS_FILE.exists():\n",
    "    with open(METRICS_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['run_id', 'task', 'note', 'files_read', 'input_size_bytes',\n",
    "                        'shuffle_read_bytes', 'shuffle_write_bytes', 'timestamp'])\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "with open(METRICS_FILE, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Log RF results\n",
    "    writer.writerow([\n",
    "        run_id, \"model_rf\",\n",
    "        f\"acc={rf_accuracy:.4f},f1={rf_f1:.4f},auc={rf_auc:.4f}\",\n",
    "        1, 0, 0, 0, timestamp\n",
    "    ])\n",
    "    # Log GBT results\n",
    "    writer.writerow([\n",
    "        run_id, \"model_gbt\",\n",
    "        f\"acc={gbt_accuracy:.4f},f1={gbt_f1:.4f},auc={gbt_auc:.4f}\",\n",
    "        1, 0, 0, 0, timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"Metrics logged to: {METRICS_FILE}\")\n",
    "\n",
    "summary_file = EVIDENCE_DIR / \"model_training_summary.txt\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# Model Training Summary\\n\")\n",
    "    f.write(f\"# Date: {datetime.now()}\\n\")\n",
    "    f.write(f\"# Run ID: {run_id}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Dataset\\n\")\n",
    "    f.write(f\"Total samples: {total_count}\\n\")\n",
    "    f.write(f\"Train samples: {train_count} ({train_count/total_count*100:.1f}%)\\n\")\n",
    "    f.write(f\"Test samples: {test_count} ({test_count/total_count*100:.1f}%)\\n\")\n",
    "    f.write(f\"Features: {len(feature_columns)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Results\\n\")\n",
    "    f.write(f\"Baseline Accuracy: {baseline_accuracy:.4f}\\n\\n\")\n",
    "    f.write(f\"Random Forest:\\n\")\n",
    "    f.write(f\"  Accuracy: {rf_accuracy:.4f}\\n\")\n",
    "    f.write(f\"  F1-Score: {rf_f1:.4f}\\n\")\n",
    "    f.write(f\"  AUC-ROC: {rf_auc:.4f}\\n\\n\")\n",
    "    f.write(f\"GBT:\\n\")\n",
    "    f.write(f\"  Accuracy: {gbt_accuracy:.4f}\\n\")\n",
    "    f.write(f\"  F1-Score: {gbt_f1:.4f}\\n\")\n",
    "    f.write(f\"  AUC-ROC: {gbt_auc:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Feature Importance (Top 10)\\n\")\n",
    "    for i, (feat, imp) in enumerate(feature_importance[:10], 1):\n",
    "        f.write(f\"{i}. {feat}: {imp:.4f}\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {summary_file}\")\n",
    "\n",
    "best_model = rf_model if rf_auc > gbt_auc else gbt_model\n",
    "model_path = MODELS_DIR / f\"best_model_{best_model_name.lower().replace(' ', '_')}\"\n",
    "best_model.write().overwrite().save(str(model_path))\n",
    "print(f\"Best model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03edc45",
   "metadata": {},
   "source": [
    "# 11. Spark Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6397dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPARK EXECUTION PLAN (Best Model Predictions)\n",
      "============================================================\n",
      "== Physical Plan ==\n",
      "* Project (10)\n",
      "+- * Project (9)\n",
      "   +- * Project (8)\n",
      "      +- * Project (7)\n",
      "         +- * Project (6)\n",
      "            +- * Filter (5)\n",
      "               +- InMemoryTableScan (1) (columnarIn=false, columnarOut=true)\n",
      "                     +- InMemoryRelation (2)\n",
      "                           +- * ColumnarToRow (4)\n",
      "                              +- Scan parquet  (3)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [20]: [avg_block_size#8, close#1, day_of_week#18, hour_of_day#17, issuance_zscore#16, label#19, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, return_1h#2, return_24h#3, timestamp_hour#0L, tx_count#6L, tx_count_zscore#14, volatility_24h#4, volume_btc#7, volume_exchange#5, whale_tx_count#9L, whale_volume_btc#10, whale_zscore#15]\n",
      "Arguments: [avg_block_size#8, close#1, day_of_week#18, hour_of_day#17, issuance_zscore#16, label#19, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, return_1h#2, return_24h#3, timestamp_hour#0L, tx_count#6L, tx_count_zscore#14, volatility_24h#4, volume_btc#7, volume_exchange#5, whale_tx_count#9L, whale_volume_btc#10, whale_zscore#15], [isnotnull(timestamp_hour#0L), (timestamp_hour#0L >= 1389066480), atleastnnonnulls(18, coalesce(nanvl(close#1, null), 0.0), coalesce(nanvl(return_1h#2, null), 0.0), coalesce(nanvl(return_24h#3, null), 0.0), coalesce(nanvl(volatility_24h#4, null), 0.0), coalesce(nanvl(volume_exchange#5, null), 0.0), coalesce(tx_count#6L, 0), coalesce(nanvl(volume_btc#7, null), 0.0), coalesce(nanvl(avg_block_size#8, null), 0.0), coalesce(whale_tx_count#9L, 0), coalesce(nanvl(whale_volume_btc#10, null), 0.0), coalesce(nanvl(miner_issuance_btc#11, null), 0.0), coalesce(nanvl(nvt_like#12, null), 0.0), coalesce(nanvl(onchain_vs_exchange#13, null), 0.0), coalesce(nanvl(tx_count_zscore#14, null), 0.0), coalesce(nanvl(whale_zscore#15, null), 0.0), coalesce(nanvl(issuance_zscore#16, null), 0.0), coalesce(hour_of_day#17, 0), coalesce(day_of_week#18, 0))]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [timestamp_hour#0L, close#1, return_1h#2, return_24h#3, volatility_24h#4, volume_exchange#5, tx_count#6L, volume_btc#7, avg_block_size#8, whale_tx_count#9L, whale_volume_btc#10, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, tx_count_zscore#14, whale_zscore#15, issuance_zscore#16, hour_of_day#17, day_of_week#18, label#19], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan parquet \n",
      "Output [20]: [timestamp_hour#0L, close#1, return_1h#2, return_24h#3, volatility_24h#4, volume_exchange#5, tx_count#6L, volume_btc#7, avg_block_size#8, whale_tx_count#9L, whale_volume_btc#10, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, tx_count_zscore#14, whale_zscore#15, issuance_zscore#16, hour_of_day#17, day_of_week#18, label#19]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/home/img/BigData/Project/data/output/features_parquet]\n",
      "ReadSchema: struct<timestamp_hour:bigint,close:double,return_1h:double,return_24h:double,volatility_24h:double,volume_exchange:double,tx_count:bigint,volume_btc:double,avg_block_size:double,whale_tx_count:bigint,whale_volume_btc:double,miner_issuance_btc:double,nvt_like:double,onchain_vs_exchange:double,tx_count_zscore:double,whale_zscore:double,issuance_zscore:double,hour_of_day:int,day_of_week:int,label:int>\n",
      "\n",
      "(4) ColumnarToRow [codegen id : 1]\n",
      "Input [20]: [timestamp_hour#0L, close#1, return_1h#2, return_24h#3, volatility_24h#4, volume_exchange#5, tx_count#6L, volume_btc#7, avg_block_size#8, whale_tx_count#9L, whale_volume_btc#10, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, tx_count_zscore#14, whale_zscore#15, issuance_zscore#16, hour_of_day#17, day_of_week#18, label#19]\n",
      "\n",
      "(5) Filter [codegen id : 1]\n",
      "Input [20]: [avg_block_size#8, close#1, day_of_week#18, hour_of_day#17, issuance_zscore#16, label#19, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, return_1h#2, return_24h#3, timestamp_hour#0L, tx_count#6L, tx_count_zscore#14, volatility_24h#4, volume_btc#7, volume_exchange#5, whale_tx_count#9L, whale_volume_btc#10, whale_zscore#15]\n",
      "Condition : ((isnotnull(timestamp_hour#0L) AND (timestamp_hour#0L >= 1389066480)) AND atleastnnonnulls(18, coalesce(nanvl(close#1, null), 0.0), coalesce(nanvl(return_1h#2, null), 0.0), coalesce(nanvl(return_24h#3, null), 0.0), coalesce(nanvl(volatility_24h#4, null), 0.0), coalesce(nanvl(volume_exchange#5, null), 0.0), coalesce(tx_count#6L, 0), coalesce(nanvl(volume_btc#7, null), 0.0), coalesce(nanvl(avg_block_size#8, null), 0.0), coalesce(whale_tx_count#9L, 0), coalesce(nanvl(whale_volume_btc#10, null), 0.0), coalesce(nanvl(miner_issuance_btc#11, null), 0.0), coalesce(nanvl(nvt_like#12, null), 0.0), coalesce(nanvl(onchain_vs_exchange#13, null), 0.0), coalesce(nanvl(tx_count_zscore#14, null), 0.0), coalesce(nanvl(whale_zscore#15, null), 0.0), coalesce(nanvl(issuance_zscore#16, null), 0.0), coalesce(hour_of_day#17, 0), coalesce(day_of_week#18, 0)))\n",
      "\n",
      "(6) Project [codegen id : 1]\n",
      "Output [20]: [coalesce(timestamp_hour#0L, 0) AS timestamp_hour#3305L, coalesce(nanvl(close#1, null), 0.0) AS close#3306, coalesce(nanvl(return_1h#2, null), 0.0) AS return_1h#3307, coalesce(nanvl(return_24h#3, null), 0.0) AS return_24h#3308, coalesce(nanvl(volatility_24h#4, null), 0.0) AS volatility_24h#3309, coalesce(nanvl(volume_exchange#5, null), 0.0) AS volume_exchange#3310, coalesce(tx_count#6L, 0) AS tx_count#3311L, coalesce(nanvl(volume_btc#7, null), 0.0) AS volume_btc#3312, coalesce(nanvl(avg_block_size#8, null), 0.0) AS avg_block_size#3313, coalesce(whale_tx_count#9L, 0) AS whale_tx_count#3314L, coalesce(nanvl(whale_volume_btc#10, null), 0.0) AS whale_volume_btc#3315, coalesce(nanvl(miner_issuance_btc#11, null), 0.0) AS miner_issuance_btc#3316, coalesce(nanvl(nvt_like#12, null), 0.0) AS nvt_like#3317, coalesce(nanvl(onchain_vs_exchange#13, null), 0.0) AS onchain_vs_exchange#3318, coalesce(nanvl(tx_count_zscore#14, null), 0.0) AS tx_count_zscore#3319, coalesce(nanvl(whale_zscore#15, null), 0.0) AS whale_zscore#3320, coalesce(nanvl(issuance_zscore#16, null), 0.0) AS issuance_zscore#3321, coalesce(hour_of_day#17, 0) AS hour_of_day#3322, coalesce(day_of_week#18, 0) AS day_of_week#3323, coalesce(label#19, 0) AS label#3324]\n",
      "Input [20]: [avg_block_size#8, close#1, day_of_week#18, hour_of_day#17, issuance_zscore#16, label#19, miner_issuance_btc#11, nvt_like#12, onchain_vs_exchange#13, return_1h#2, return_24h#3, timestamp_hour#0L, tx_count#6L, tx_count_zscore#14, volatility_24h#4, volume_btc#7, volume_exchange#5, whale_tx_count#9L, whale_volume_btc#10, whale_zscore#15]\n",
      "\n",
      "(7) Project [codegen id : 1]\n",
      "Output [21]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, UDF(struct(close, close#3306, return_1h, return_1h#3307, return_24h, return_24h#3308, volatility_24h, volatility_24h#3309, volume_exchange, volume_exchange#3310, tx_count_double_VectorAssembler_51712dab8179, cast(tx_count#3311L as double), volume_btc, volume_btc#3312, avg_block_size, avg_block_size#3313, whale_tx_count_double_VectorAssembler_51712dab8179, cast(whale_tx_count#3314L as double), whale_volume_btc, whale_volume_btc#3315, miner_issuance_btc, miner_issuance_btc#3316, nvt_like, nvt_like#3317, onchain_vs_exchange, ... 11 more fields)) AS features_raw#7147]\n",
      "Input [20]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324]\n",
      "\n",
      "(8) Project [codegen id : 1]\n",
      "Output [22]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, features_raw#7147, UDF(features_raw#7147) AS features#7151]\n",
      "Input [21]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, features_raw#7147]\n",
      "\n",
      "(9) Project [codegen id : 1]\n",
      "Output [23]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, features_raw#7147, features#7151, UDF(features#7151) AS rawPrediction#7157]\n",
      "Input [22]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, features_raw#7147, features#7151]\n",
      "\n",
      "(10) Project [codegen id : 1]\n",
      "Output [25]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, features_raw#7147, features#7151, rawPrediction#7157, UDF(rawPrediction#7157) AS probability#7161, UDF(rawPrediction#7157) AS prediction#7168]\n",
      "Input [23]: [timestamp_hour#3305L, close#3306, return_1h#3307, return_24h#3308, volatility_24h#3309, volume_exchange#3310, tx_count#3311L, volume_btc#3312, avg_block_size#3313, whale_tx_count#3314L, whale_volume_btc#3315, miner_issuance_btc#3316, nvt_like#3317, onchain_vs_exchange#3318, tx_count_zscore#3319, whale_zscore#3320, issuance_zscore#3321, hour_of_day#3322, day_of_week#3323, label#3324, features_raw#7147, features#7151, rawPrediction#7157]\n",
      "\n",
      "\n",
      "\n",
      "Execution plan saved to: /home/img/BigData/Project/evidence/model_training_explain.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SPARK EXECUTION PLAN (Best Model Predictions)\")\n",
    "print(\"=\" * 60)\n",
    "best_predictions.explain(\"formatted\")\n",
    "\n",
    "# Save plan\n",
    "plan_buffer = StringIO()\n",
    "with redirect_stdout(plan_buffer):\n",
    "    best_predictions.explain(\"formatted\")\n",
    "\n",
    "plan_file = EVIDENCE_DIR / \"model_training_explain.txt\"\n",
    "plan_file.write_text(f\"# Model Training Execution Plan\\n# Date: {datetime.now()}\\n\\n{plan_buffer.getvalue()}\")\n",
    "print(f\"\\nExecution plan saved to: {plan_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4d663",
   "metadata": {},
   "source": [
    "# 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06b3502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "            BITCOIN PRICE DIRECTION PREDICTION\n",
      "                     FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      " Dataset:\n",
      "   - 933 samples\n",
      "   - Temporal split: 748 train / 185 test\n",
      "   - Period: 2013-12-06 to 2014-01-14\n",
      "\n",
      " Performance (ALL features = 18):\n",
      "   - Baseline (majority): 52.43%\n",
      "   - Best Model (Random Forest):\n",
      "       Accuracy: 57.30%\n",
      "       AUC-ROC:  0.6105\n",
      "       Improvement: +9.28% over baseline\n",
      "\n",
      " Ablation Study (GENERALISABLE features = 11):\n",
      "   - Accuracy: 54.05%\n",
      "   - AUC-ROC:  0.5907\n",
      "   - Drop vs ALL: +5.66% accuracy, +3.24% AUC\n",
      " Dépendance aux valeurs brutes (risque d'overfitting temporel)\n",
      "\n",
      " Top 5 Features (ALL):\n",
      "   1. return_1h (0.0962)\n",
      "   2. tx_count_zscore (0.0875)\n",
      "   3. volume_btc (0.0869)\n",
      "   4. volume_exchange (0.0852)\n",
      "   5. hour_of_day (0.0840)\n",
      "\n",
      " Top 5 Features (GENERALISABLE):\n",
      "   1. nvt_like (0.1538)\n",
      "   2. return_1h (0.1388)\n",
      "   3. onchain_vs_exchange (0.1377)\n",
      "   4. return_24h (0.1336)\n",
      "   5. tx_count_zscore (0.1290)\n",
      "\n",
      " Artifacts saved:\n",
      "   - Metrics: /home/img/BigData/Project/project_metrics_log.csv\n",
      "   - Summary: /home/img/BigData/Project/evidence/model_training_summary.txt\n",
      "   - Model: /home/img/BigData/Project/models/best_model_random_forest\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Cache released.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            BITCOIN PRICE DIRECTION PREDICTION\")\n",
    "print(\"                     FINAL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Dataset:\")\n",
    "print(f\"   - {total_count:,} samples\")\n",
    "print(f\"   - Temporal split: {train_count} train / {test_count} test\")\n",
    "print(f\"   - Period: {datetime.utcfromtimestamp(min_ts).date()} to {datetime.utcfromtimestamp(max_ts).date()}\")\n",
    "\n",
    "print(f\"\\n Performance (ALL features = {len(all_feature_columns)}):\")\n",
    "print(f\"   - Baseline (majority): {baseline_accuracy*100:.2f}%\")\n",
    "print(f\"   - Best Model ({best_model_name}):\")\n",
    "print(f\"       Accuracy: {best_accuracy*100:.2f}%\")\n",
    "print(f\"       AUC-ROC:  {best_auc:.4f}\")\n",
    "print(f\"       Improvement: {improvement:+.2f}% over baseline\")\n",
    "\n",
    "print(f\"\\n Ablation Study (GENERALISABLE features = {len(generalisable_features)}):\")\n",
    "print(f\"   - Accuracy: {gen_accuracy*100:.2f}%\")\n",
    "print(f\"   - AUC-ROC:  {gen_auc:.4f}\")\n",
    "print(f\"   - Drop vs ALL: {acc_drop:+.2f}% accuracy, {auc_drop:+.2f}% AUC\")\n",
    "if abs(acc_drop) < 5:\n",
    "    print(f\" Modèle generalisable (faible dependance aux valeurs brutes)\")\n",
    "else:\n",
    "    print(f\" Dépendance aux valeurs brutes (risque d'overfitting temporel)\")\n",
    "\n",
    "print(f\"\\n Top 5 Features (ALL):\")\n",
    "for i, (feat, imp) in enumerate(feature_importance[:5], 1):\n",
    "    print(f\"   {i}. {feat} ({imp:.4f})\")\n",
    "\n",
    "print(f\"\\n Top 5 Features (GENERALISABLE):\")\n",
    "for i, (feat, imp) in enumerate(feature_importance_gen[:5], 1):\n",
    "    print(f\"   {i}. {feat} ({imp:.4f})\")\n",
    "    \n",
    "print(f\"\\n Artifacts saved:\")\n",
    "print(f\"   - Metrics: {METRICS_FILE}\")\n",
    "print(f\"   - Summary: {summary_file}\")\n",
    "print(f\"   - Model: {model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "df.unpersist()\n",
    "print(\"\\nCache released.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfad0a3-3df1-41ff-97fd-865e38a78c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
