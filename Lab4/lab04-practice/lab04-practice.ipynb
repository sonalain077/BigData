{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d5e024-9f1c-4b50-a70d-3d9b7cd5abe0",
   "metadata": {},
   "source": [
    "# 0. Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65ce668-0110-4740-a83e-d1ceb2235ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 04:03:21 WARN Utils: Your hostname, a03-341a, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 04:03:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 04:03:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "PySpark version: 4.0.1\n",
      "Python version: 3.10.19\n",
      "Session timezone: UTC\n",
      "Shuffle partitions: 4\n",
      "Workspace ready at /home/img/BigData/Lab04/lab04-practice\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('BDA-Lab04')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .config('spark.sql.shuffle.partitions', '4')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(f'Spark version: {spark.version}')\n",
    "print(f'PySpark version: {pyspark.__version__}')\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'Session timezone: {spark.conf.get(\"spark.sql.session.timeZone\")}')\n",
    "print(f'Shuffle partitions: {spark.conf.get(\"spark.sql.shuffle.partitions\")}')\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_ROOT = BASE_DIR / 'data'\n",
    "OUTPUT_ROOT = BASE_DIR / 'outputs'\n",
    "PROOF_ROOT = BASE_DIR / 'proof'\n",
    "CHECKPOINT_ROOT = BASE_DIR / 'checkpoints'\n",
    "\n",
    "for directory in (DATA_ROOT, OUTPUT_ROOT, PROOF_ROOT, CHECKPOINT_ROOT):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Workspace ready at {BASE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c263b79-84e7-481b-ba2b-b36ebe808b10",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d74738-d97c-4445-8902-4530bc82da8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPC-H text tables: ['customer.tbl', 'lineitem.tbl', 'nation.tbl', 'orders.tbl', 'part.tbl', 'supplier.tbl']\n",
      "TPC-H parquet tables: ['.ipynb_checkpoints', 'customer', 'lineitem', 'nation', 'orders', 'part', 'supplier']\n",
      "Taxi samples: ['part-2021-01-01-0000.csv', 'part-2021-01-01-0001.csv', 'part-2021-01-01-0002.csv']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import Row, types as T\n",
    "\n",
    "TPC_H_TEXT_DIR = DATA_ROOT / 'tpch' / 'TPC-H-0.1-TXT'\n",
    "TPC_H_PARQUET_DIR = DATA_ROOT / 'tpch' / 'TPC-H-0.1-PARQUET'\n",
    "TAXI_DIR = DATA_ROOT / 'taxi-data'\n",
    "\n",
    "TPC_H_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TPC_H_PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAXI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TPCH_DEFINITIONS = {\n",
    "    'nation': {\n",
    "        'columns': [('n_nationkey', int), ('n_name', str), ('n_regionkey', int), ('n_comment', str)],\n",
    "        'rows': [\n",
    "            {'n_nationkey': 1, 'n_name': 'UNITED STATES', 'n_regionkey': 1, 'n_comment': 'USA'},\n",
    "            {'n_nationkey': 2, 'n_name': 'CANADA', 'n_regionkey': 1, 'n_comment': 'CAN'},\n",
    "            {'n_nationkey': 3, 'n_name': 'BRAZIL', 'n_regionkey': 2, 'n_comment': 'BRA'},\n",
    "        ],\n",
    "    },\n",
    "    'customer': {\n",
    "        'columns': [('c_custkey', int), ('c_name', str), ('c_nationkey', int), ('c_comment', str)],\n",
    "        'rows': [\n",
    "            {'c_custkey': 1, 'c_name': 'Customer#1', 'c_nationkey': 1, 'c_comment': 'USA customer'},\n",
    "            {'c_custkey': 2, 'c_name': 'Customer#2', 'c_nationkey': 2, 'c_comment': 'Canada customer'},\n",
    "            {'c_custkey': 3, 'c_name': 'Customer#3', 'c_nationkey': 1, 'c_comment': 'USA repeat'},\n",
    "        ],\n",
    "    },\n",
    "    'orders': {\n",
    "        'columns': [\n",
    "            ('o_orderkey', int),\n",
    "            ('o_custkey', int),\n",
    "            ('o_orderstatus', str),\n",
    "            ('o_totalprice', float),\n",
    "            ('o_orderdate', str),\n",
    "            ('o_clerk', str),\n",
    "            ('o_shippriority', int),\n",
    "            ('o_comment', str),\n",
    "        ],\n",
    "        'rows': [\n",
    "            {'o_orderkey': 1, 'o_custkey': 1, 'o_orderstatus': 'O', 'o_totalprice': 1000.0, 'o_orderdate': '1995-03-01', 'o_clerk': 'Clerk#000000001', 'o_shippriority': 0, 'o_comment': 'first order'},\n",
    "            {'o_orderkey': 2, 'o_custkey': 2, 'o_orderstatus': 'O', 'o_totalprice': 800.0, 'o_orderdate': '1995-03-05', 'o_clerk': 'Clerk#000000002', 'o_shippriority': 0, 'o_comment': 'canada order'},\n",
    "            {'o_orderkey': 3, 'o_custkey': 1, 'o_orderstatus': 'F', 'o_totalprice': 1200.0, 'o_orderdate': '1995-04-10', 'o_clerk': 'Clerk#000000003', 'o_shippriority': 0, 'o_comment': 'usa april'},\n",
    "            {'o_orderkey': 4, 'o_custkey': 2, 'o_orderstatus': 'F', 'o_totalprice': 650.0, 'o_orderdate': '1995-05-01', 'o_clerk': 'Clerk#000000004', 'o_shippriority': 0, 'o_comment': 'canada may'},\n",
    "        ],\n",
    "    },\n",
    "    'part': {\n",
    "        'columns': [('p_partkey', int), ('p_name', str), ('p_mfgr', str), ('p_brand', str), ('p_type', str)],\n",
    "        'rows': [\n",
    "            {'p_partkey': 1, 'p_name': 'Part#1', 'p_mfgr': 'MFGR#1', 'p_brand': 'Brand#1', 'p_type': 'SMALL'},\n",
    "            {'p_partkey': 2, 'p_name': 'Part#2', 'p_mfgr': 'MFGR#2', 'p_brand': 'Brand#2', 'p_type': 'MEDIUM'},\n",
    "        ],\n",
    "    },\n",
    "    'supplier': {\n",
    "        'columns': [('s_suppkey', int), ('s_name', str), ('s_nationkey', int), ('s_comment', str)],\n",
    "        'rows': [\n",
    "            {'s_suppkey': 1, 's_name': 'Supplier#1', 's_nationkey': 1, 's_comment': 'usa supplier'},\n",
    "            {'s_suppkey': 2, 's_name': 'Supplier#2', 's_nationkey': 2, 's_comment': 'can supplier'},\n",
    "        ],\n",
    "    },\n",
    "    'lineitem': {\n",
    "        'columns': [\n",
    "            ('l_orderkey', int),\n",
    "            ('l_partkey', int),\n",
    "            ('l_suppkey', int),\n",
    "            ('l_linenumber', int),\n",
    "            ('l_quantity', float),\n",
    "            ('l_extendedprice', float),\n",
    "            ('l_discount', float),\n",
    "            ('l_tax', float),\n",
    "            ('l_returnflag', str),\n",
    "            ('l_linestatus', str),\n",
    "            ('l_shipdate', str),\n",
    "            ('l_commitdate', str),\n",
    "            ('l_receiptdate', str),\n",
    "            ('l_shipinstruct', str),\n",
    "            ('l_shipmode', str),\n",
    "            ('l_comment', str),\n",
    "        ],\n",
    "        'rows': [\n",
    "            {'l_orderkey': 1, 'l_partkey': 1, 'l_suppkey': 1, 'l_linenumber': 1, 'l_quantity': 3.0, 'l_extendedprice': 300.0, 'l_discount': 0.0, 'l_tax': 0.0, 'l_returnflag': 'N', 'l_linestatus': 'O', 'l_shipdate': '1995-03-15', 'l_commitdate': '1995-03-13', 'l_receiptdate': '1995-03-20', 'l_shipinstruct': 'DELIVER IN PERSON', 'l_shipmode': 'AIR', 'l_comment': 'usa order'},\n",
    "            {'l_orderkey': 1, 'l_partkey': 2, 'l_suppkey': 2, 'l_linenumber': 2, 'l_quantity': 2.0, 'l_extendedprice': 200.0, 'l_discount': 0.0, 'l_tax': 0.0, 'l_returnflag': 'N', 'l_linestatus': 'O', 'l_shipdate': '1995-03-15', 'l_commitdate': '1995-03-13', 'l_receiptdate': '1995-03-21', 'l_shipinstruct': 'DELIVER IN PERSON', 'l_shipmode': 'AIR', 'l_comment': 'mixed suppliers'},\n",
    "            {'l_orderkey': 2, 'l_partkey': 1, 'l_suppkey': 1, 'l_linenumber': 1, 'l_quantity': 5.0, 'l_extendedprice': 500.0, 'l_discount': 0.05, 'l_tax': 0.0, 'l_returnflag': 'N', 'l_linestatus': 'O', 'l_shipdate': '1995-03-15', 'l_commitdate': '1995-03-14', 'l_receiptdate': '1995-03-22', 'l_shipinstruct': 'TAKE BACK RETURN', 'l_shipmode': 'MAIL', 'l_comment': 'canada'},\n",
    "            {'l_orderkey': 3, 'l_partkey': 2, 'l_suppkey': 2, 'l_linenumber': 1, 'l_quantity': 1.0, 'l_extendedprice': 150.0, 'l_discount': 0.0, 'l_tax': 0.0, 'l_returnflag': 'R', 'l_linestatus': 'F', 'l_shipdate': '1995-04-12', 'l_commitdate': '1995-04-10', 'l_receiptdate': '1995-04-18', 'l_shipinstruct': 'DELIVER IN PERSON', 'l_shipmode': 'TRUCK', 'l_comment': 'usa april'},\n",
    "            {'l_orderkey': 4, 'l_partkey': 1, 'l_suppkey': 1, 'l_linenumber': 1, 'l_quantity': 4.0, 'l_extendedprice': 420.0, 'l_discount': 0.0, 'l_tax': 0.0, 'l_returnflag': 'R', 'l_linestatus': 'F', 'l_shipdate': '1995-05-20', 'l_commitdate': '1995-05-18', 'l_receiptdate': '1995-05-25', 'l_shipinstruct': 'COLLECT COD', 'l_shipmode': 'SHIP', 'l_comment': 'canada may'},\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "def write_pipe_table(table: str, definition: dict) -> None:\n",
    "    path = TPC_H_TEXT_DIR / f'{table}.tbl'\n",
    "    columns = [col for col, _ in definition['columns']]\n",
    "    with path.open('w', encoding='utf-8') as handle:\n",
    "        for row in definition['rows']:\n",
    "            values = ['' if row.get(col) is None else str(row.get(col)) for col in columns]\n",
    "            handle.write('|'.join(values) + '|\\n')\n",
    "\n",
    "def write_parquet_table(table: str, definition: dict) -> None:\n",
    "    columns = definition['columns']\n",
    "    schema = T.StructType([\n",
    "        T.StructField(col, T.IntegerType() if caster is int else T.DoubleType() if caster is float else T.StringType(), True)\n",
    "        for col, caster in columns\n",
    "    ])\n",
    "    data_rows = []\n",
    "    for row in definition['rows']:\n",
    "        values = []\n",
    "        for col, caster in columns:\n",
    "            value = row.get(col)\n",
    "            if value is None:\n",
    "                values.append(None)\n",
    "            elif caster is int:\n",
    "                values.append(int(value))\n",
    "            elif caster is float:\n",
    "                values.append(float(value))\n",
    "            else:\n",
    "                values.append(str(value))\n",
    "        data_rows.append(values)\n",
    "    df = spark.createDataFrame(data_rows, schema=schema)\n",
    "    target = TPC_H_PARQUET_DIR / table\n",
    "    df.write.mode('overwrite').parquet(str(target))\n",
    "\n",
    "for table, definition in TPCH_DEFINITIONS.items():\n",
    "    text_path = TPC_H_TEXT_DIR / f'{table}.tbl'\n",
    "    parquet_path = TPC_H_PARQUET_DIR / table\n",
    "    if not text_path.exists():\n",
    "        write_pipe_table(table, definition)\n",
    "    if not parquet_path.exists():\n",
    "        write_parquet_table(table, definition)\n",
    "\n",
    "if not any(TAXI_DIR.glob('*.csv')):\n",
    "    rows = [\n",
    "        {\n",
    "            'tpep_pickup_datetime': datetime(2021, 1, 1, 8, 5) + timedelta(minutes=i * 7),\n",
    "            'tpep_dropoff_datetime': datetime(2021, 1, 1, 8, 15) + timedelta(minutes=i * 7),\n",
    "            'passenger_count': 1 + (i % 3),\n",
    "            'trip_distance': 2.5 + i * 0.2,\n",
    "            'dropoff_longitude': -74.0135 + (i % 2) * 0.004,\n",
    "            'dropoff_latitude': 40.7135 + (i % 3) * 0.003,\n",
    "        }\n",
    "        for i in range(12)\n",
    "    ]\n",
    "    header = 'tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,dropoff_longitude,dropoff_latitude\\n'\n",
    "    for batch_idx in range(3):\n",
    "        file_path = TAXI_DIR / f'part-2021-01-01-{batch_idx:04d}.csv'\n",
    "        with file_path.open('w', encoding='utf-8') as handle:\n",
    "            handle.write(header)\n",
    "            for row in rows[batch_idx * 4:(batch_idx + 1) * 4]:\n",
    "                handle.write(\n",
    "                    f\"{row['tpep_pickup_datetime']:%Y-%m-%d %H:%M:%S},{row['tpep_dropoff_datetime']:%Y-%m-%d %H:%M:%S},{row['passenger_count']},{row['trip_distance']:.2f},{row['dropoff_longitude']:.6f},{row['dropoff_latitude']:.6f}\\n\"\n",
    "                )\n",
    "\n",
    "print(f'TPC-H text tables: {sorted(p.name for p in TPC_H_TEXT_DIR.glob(\"*.tbl\"))}')\n",
    "print(f'TPC-H parquet tables: {sorted(p.name for p in TPC_H_PARQUET_DIR.iterdir())}')\n",
    "print(f'Taxi samples: {sorted(p.name for p in TAXI_DIR.glob(\"*.csv\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcae3e6-f217-42b6-b656-fad67305ed75",
   "metadata": {},
   "source": [
    "# 2.Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34a9e6f-0403-4116-a19d-55eab3ea69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, functions as F, types as T\n",
    "\n",
    "TPCH_TABLE_SCHEMAS = {\n",
    "    'nation': T.StructType([\n",
    "        T.StructField('n_nationkey', T.IntegerType(), False),\n",
    "        T.StructField('n_name', T.StringType(), False),\n",
    "        T.StructField('n_regionkey', T.IntegerType(), True),\n",
    "        T.StructField('n_comment', T.StringType(), True),\n",
    "    ]),\n",
    "    'customer': T.StructType([\n",
    "        T.StructField('c_custkey', T.IntegerType(), False),\n",
    "        T.StructField('c_name', T.StringType(), True),\n",
    "        T.StructField('c_nationkey', T.IntegerType(), False),\n",
    "        T.StructField('c_comment', T.StringType(), True),\n",
    "    ]),\n",
    "    'orders': T.StructType([\n",
    "        T.StructField('o_orderkey', T.IntegerType(), False),\n",
    "        T.StructField('o_custkey', T.IntegerType(), False),\n",
    "        T.StructField('o_orderstatus', T.StringType(), True),\n",
    "        T.StructField('o_totalprice', T.DoubleType(), True),\n",
    "        T.StructField('o_orderdate', T.StringType(), True),\n",
    "        T.StructField('o_clerk', T.StringType(), True),\n",
    "        T.StructField('o_shippriority', T.IntegerType(), True),\n",
    "        T.StructField('o_comment', T.StringType(), True),\n",
    "    ]),\n",
    "    'part': T.StructType([\n",
    "        T.StructField('p_partkey', T.IntegerType(), False),\n",
    "        T.StructField('p_name', T.StringType(), False),\n",
    "        T.StructField('p_mfgr', T.StringType(), True),\n",
    "        T.StructField('p_brand', T.StringType(), True),\n",
    "        T.StructField('p_type', T.StringType(), True),\n",
    "    ]),\n",
    "    'supplier': T.StructType([\n",
    "        T.StructField('s_suppkey', T.IntegerType(), False),\n",
    "        T.StructField('s_name', T.StringType(), False),\n",
    "        T.StructField('s_nationkey', T.IntegerType(), False),\n",
    "        T.StructField('s_comment', T.StringType(), True),\n",
    "    ]),\n",
    "    'lineitem': T.StructType([\n",
    "        T.StructField('l_orderkey', T.IntegerType(), False),\n",
    "        T.StructField('l_partkey', T.IntegerType(), False),\n",
    "        T.StructField('l_suppkey', T.IntegerType(), False),\n",
    "        T.StructField('l_linenumber', T.IntegerType(), False),\n",
    "        T.StructField('l_quantity', T.DoubleType(), True),\n",
    "        T.StructField('l_extendedprice', T.DoubleType(), True),\n",
    "        T.StructField('l_discount', T.DoubleType(), True),\n",
    "        T.StructField('l_tax', T.DoubleType(), True),\n",
    "        T.StructField('l_returnflag', T.StringType(), True),\n",
    "        T.StructField('l_linestatus', T.StringType(), True),\n",
    "        T.StructField('l_shipdate', T.StringType(), False),\n",
    "        T.StructField('l_commitdate', T.StringType(), True),\n",
    "        T.StructField('l_receiptdate', T.StringType(), True),\n",
    "        T.StructField('l_shipinstruct', T.StringType(), True),\n",
    "        T.StructField('l_shipmode', T.StringType(), True),\n",
    "        T.StructField('l_comment', T.StringType(), True),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "TPCH_TYPE_PARSERS = {\n",
    "    T.IntegerType(): int,\n",
    "    T.DoubleType(): float,\n",
    "    T.StringType(): str,\n",
    "}\n",
    "\n",
    "BOUNDING_BOXES = {\n",
    "    'goldman': {'lon_min': -74.0145, 'lon_max': -74.0115, 'lat_min': 40.7125, 'lat_max': 40.7155},\n",
    "    'citigroup': {'lon_min': -74.0095, 'lon_max': -74.0055, 'lat_min': 40.7190, 'lat_max': 40.7225},\n",
    "}\n",
    "\n",
    "TAXI_SCHEMA = T.StructType([\n",
    "    T.StructField('tpep_pickup_datetime', T.TimestampType(), True),\n",
    "    T.StructField('tpep_dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('passenger_count', T.IntegerType(), True),\n",
    "    T.StructField('trip_distance', T.DoubleType(), True),\n",
    "    T.StructField('dropoff_longitude', T.DoubleType(), True),\n",
    "    T.StructField('dropoff_latitude', T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "def _parse_pipe_line(table: str, line: str) -> Row:\n",
    "    if not line.strip():\n",
    "        return None\n",
    "    parts = line.split('|')\n",
    "    schema = TPCH_TABLE_SCHEMAS[table]\n",
    "    values = {}\n",
    "    for idx, field in enumerate(schema):\n",
    "        if idx >= len(parts):\n",
    "            values[field.name] = None\n",
    "            continue\n",
    "        raw_value = parts[idx]\n",
    "        if raw_value == '':\n",
    "            values[field.name] = None\n",
    "            continue\n",
    "        caster = TPCH_TYPE_PARSERS.get(type(field.dataType), str)\n",
    "        values[field.name] = caster(raw_value)\n",
    "    return Row(**values)\n",
    "\n",
    "def load_tpch_text(table: str):\n",
    "    path = TPC_H_TEXT_DIR / f'{table}.tbl'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Missing text table: {path}')\n",
    "    rdd = spark.sparkContext.textFile(str(path))\n",
    "    return rdd.map(lambda line: _parse_pipe_line(table, line)).filter(lambda row: row is not None)\n",
    "\n",
    "def load_tpch_parquet(table: str) -> DataFrame:\n",
    "    path = TPC_H_PARQUET_DIR / table\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Missing parquet table: {path}')\n",
    "    return spark.read.schema(TPCH_TABLE_SCHEMAS[table]).parquet(str(path))\n",
    "\n",
    "def ensure_dir(path: Path) -> Path:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def write_csv(df: DataFrame, path: Path, header: bool = True) -> None:\n",
    "    ensure_dir(path)\n",
    "    df.coalesce(1).write.mode('overwrite').option('header', str(header).lower()).csv(str(path))\n",
    "\n",
    "def within_bbox(lon: float, lat: float, bbox: Dict[str, float]) -> bool:\n",
    "    if lon is None or lat is None:\n",
    "        return False\n",
    "    return bbox['lon_min'] <= lon <= bbox['lon_max'] and bbox['lat_min'] <= lat <= bbox['lat_max']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a3036-ddf0-4a4d-962d-e97a7d5693f6",
   "metadata": {},
   "source": [
    "# Part A — Relational (RDD-first; DF allowed for contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7baaf-90a0-468b-9edd-05315bda30d8",
   "metadata": {},
   "source": [
    "# A1. Q1 — shipped items on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46aad265-c59f-4e48-ae11-6ce2e7a18251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (3 + 3) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 shipped-item counts:\n",
      "  text: 3 items shipped on 1995-03-15\n",
      "  parquet: 3 items shipped on 1995-03-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TARGET_DATE = '1995-03-15'\n",
    "\n",
    "q1_results = []\n",
    "for fmt in ('text', 'parquet'):\n",
    "    if fmt == 'text':\n",
    "        lineitem_rdd = load_tpch_text('lineitem')\n",
    "        count = lineitem_rdd.filter(lambda row: row.l_shipdate == TARGET_DATE).count()\n",
    "    else:\n",
    "        lineitem_df = load_tpch_parquet('lineitem')\n",
    "        count = lineitem_df.filter(F.col('l_shipdate') == TARGET_DATE).count()\n",
    "    q1_results.append((fmt, TARGET_DATE, count))\n",
    "\n",
    "output_path = OUTPUT_ROOT / 'q1.txt'\n",
    "with ensure_dir(output_path).open('w', encoding='utf-8') as handle:\n",
    "    for fmt, ship_date, count in q1_results:\n",
    "        handle.write(f\"{fmt}\t{ship_date}\t{count}\\n\")\n",
    "\n",
    "print('Q1 shipped-item counts:')\n",
    "for fmt, ship_date, count in q1_results:\n",
    "    print(f\"  {fmt}: {count} items shipped on {ship_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b21e7-4f5d-4a9d-a90b-be41de7cfd3f",
   "metadata": {},
   "source": [
    "# A2. Q2 — clerks by order key (reduce-side join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa6bcbf-8f4c-4b8b-a7f6-31b7412c61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 reduce-side join results saved to /home/img/BigData/Lab04/lab04-practice/outputs/q2.txt\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "TARGET_DATE = '1995-03-15'\n",
    "\n",
    "q2_outputs = {}\n",
    "\n",
    "orders_rdd = load_tpch_text('orders').map(lambda row: (row.o_orderkey, row.o_clerk))\n",
    "lineitem_rdd = load_tpch_text('lineitem').filter(lambda row: row.l_shipdate == TARGET_DATE).map(lambda row: (row.l_orderkey, 1))\n",
    "joined_rdd = orders_rdd.cogroup(lineitem_rdd).filter(lambda kv: len(kv[1][1]) > 0)\n",
    "q2_text = (\n",
    "    joined_rdd\n",
    "    .flatMap(lambda kv: [(kv[0], clerk) for clerk in kv[1][0]])\n",
    "    .sortBy(lambda kv: kv[0])\n",
    "    .take(20)\n",
    ")\n",
    "q2_outputs['text'] = q2_text\n",
    "\n",
    "orders_df = load_tpch_parquet('orders').select('o_orderkey', 'o_clerk')\n",
    "lineitem_df = load_tpch_parquet('lineitem').filter(F.col('l_shipdate') == TARGET_DATE).select('l_orderkey')\n",
    "joined_df = orders_df.join(lineitem_df, orders_df.o_orderkey == lineitem_df.l_orderkey, 'inner')\n",
    "q2_parquet = [\n",
    "    (int(row.o_orderkey), row.o_clerk)\n",
    "    for row in joined_df.orderBy('o_orderkey').select('o_orderkey', 'o_clerk').limit(20).collect()\n",
    "]\n",
    "q2_outputs['parquet'] = q2_parquet\n",
    "\n",
    "output_path = OUTPUT_ROOT / 'q2.txt'\n",
    "with ensure_dir(output_path).open('w', encoding='utf-8') as handle:\n",
    "    for fmt, rows in q2_outputs.items():\n",
    "        handle.write(f\"[{fmt}]\\n\")\n",
    "        for orderkey, clerk in rows:\n",
    "            handle.write(f\"{orderkey}\t{clerk}\\n\")\n",
    "\n",
    "print(f\"Q2 reduce-side join results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c4da9-4139-4baf-9ef8-61d4dd493200",
   "metadata": {},
   "source": [
    "# A3. Q3 — part & supplier (broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313c2fad-ef49-413f-a960-1031c278f183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 broadcast join sample written to /home/img/BigData/Lab04/lab04-practice/outputs/q3.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TARGET_DATE = '1995-03-15'\n",
    "\n",
    "part_map = {row.p_partkey: row.p_name for row in load_tpch_text('part').collect()}\n",
    "supplier_map = {row.s_suppkey: row.s_name for row in load_tpch_text('supplier').collect()}\n",
    "part_bc = spark.sparkContext.broadcast(part_map)\n",
    "supp_bc = spark.sparkContext.broadcast(supplier_map)\n",
    "\n",
    "lineitems_rdd = load_tpch_text('lineitem').filter(lambda row: row.l_shipdate == TARGET_DATE)\n",
    "q3_text = (\n",
    "    lineitems_rdd\n",
    "    .map(lambda row: (row.l_orderkey, part_bc.value.get(row.l_partkey), supp_bc.value.get(row.l_suppkey)))\n",
    "    .filter(lambda tpl: tpl[1] is not None and tpl[2] is not None)\n",
    "    .sortBy(lambda tpl: (tpl[0], tpl[1], tpl[2]))\n",
    "    .take(20)\n",
    ")\n",
    "\n",
    "part_df = load_tpch_parquet('part').select('p_partkey', 'p_name')\n",
    "supplier_df = load_tpch_parquet('supplier').select('s_suppkey', 's_name')\n",
    "line_df = load_tpch_parquet('lineitem').filter(F.col('l_shipdate') == TARGET_DATE)\n",
    "q3_parquet_df = (\n",
    "    line_df\n",
    "    .join(F.broadcast(part_df), line_df.l_partkey == part_df.p_partkey, 'inner')\n",
    "    .join(F.broadcast(supplier_df), line_df.l_suppkey == supplier_df.s_suppkey, 'inner')\n",
    "    .select('l_orderkey', 'p_name', 's_name')\n",
    "    .orderBy('l_orderkey', 'p_name')\n",
    "    .limit(20)\n",
    ")\n",
    "q3_parquet = [(int(row.l_orderkey), row.p_name, row.s_name) for row in q3_parquet_df.collect()]\n",
    "\n",
    "output_path = OUTPUT_ROOT / 'q3.txt'\n",
    "with ensure_dir(output_path).open('w', encoding='utf-8') as handle:\n",
    "    handle.write('[text]\\n')\n",
    "    for orderkey, part_name, supp_name in q3_text:\n",
    "        handle.write(f\"{orderkey}\t{part_name}\t{supp_name}\\n\")\n",
    "    handle.write('[parquet]\\n')\n",
    "    for orderkey, part_name, supp_name in q3_parquet:\n",
    "        handle.write(f\"{orderkey}\t{part_name}\t{supp_name}\\n\")\n",
    "\n",
    "print(f\"Q3 broadcast join sample written to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db713cd7-4fb2-4066-8a4f-380398ffc105",
   "metadata": {},
   "source": [
    "# A4. Q4 — shipped to each nation (mixed joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e423bb9f-f67d-41cd-a07c-bb13bc5dd544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 shipments per nation written to /home/img/BigData/Lab04/lab04-practice/outputs/q4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "TARGET_DATE = '1995-03-15'\n",
    "\n",
    "customer_map = {row.c_custkey: row.c_nationkey for row in load_tpch_text('customer').collect()}\n",
    "nation_map = {row.n_nationkey: row.n_name for row in load_tpch_text('nation').collect()}\n",
    "customer_bc = spark.sparkContext.broadcast(customer_map)\n",
    "nation_bc = spark.sparkContext.broadcast(nation_map)\n",
    "\n",
    "lineitem_filtered = load_tpch_text('lineitem').filter(lambda row: row.l_shipdate == TARGET_DATE)\n",
    "orders_rdd = load_tpch_text('orders').map(lambda row: (row.o_orderkey, row.o_custkey))\n",
    "\n",
    "line_by_order = lineitem_filtered.map(lambda row: (row.l_orderkey, 1))\n",
    "joined = line_by_order.join(orders_rdd)\n",
    "\n",
    "nation_counts = (\n",
    "    joined\n",
    "    .map(lambda kv: (customer_bc.value.get(kv[1][1]), kv[1][0]))\n",
    "    .filter(lambda tpl: tpl[0] is not None)\n",
    "    .map(lambda tpl: ((tpl[0], nation_bc.value.get(tpl[0])), tpl[1]))\n",
    "    .filter(lambda tpl: tpl[0][1] is not None)\n",
    "    .reduceByKey(add)\n",
    "    .map(lambda tpl: (tpl[0][0], tpl[0][1], tpl[1]))\n",
    "    .sortBy(lambda tpl: (-tpl[2], tpl[1]))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "nations_df = spark.createDataFrame(nation_counts, schema=['nationkey', 'nation', 'count'])\n",
    "output_path = OUTPUT_ROOT / 'q4'\n",
    "write_csv(nations_df, output_path)\n",
    "\n",
    "print(f'Q4 shipments per nation written to {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ddaaf-5927-4269-bf43-162f1c491eae",
   "metadata": {},
   "source": [
    "# A5. Q5 — monthly US vs Canada volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3641fe3-c185-4780-bf47-034339a98cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 monthly volume metrics written to /home/img/BigData/Lab04/lab04-practice/outputs/q5\n",
      "+-----------+------+----------+--------------+\n",
      "|n_nationkey|n_name|ship_month|shipment_count|\n",
      "+-----------+------+----------+--------------+\n",
      "+-----------+------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthly_df = (\n",
    "    load_tpch_parquet('lineitem')\n",
    "    .join(load_tpch_parquet('orders'), F.col('l_orderkey') == F.col('o_orderkey'), 'inner')\n",
    "    .join(load_tpch_parquet('customer'), F.col('o_custkey') == F.col('c_custkey'), 'inner')\n",
    "    .join(load_tpch_parquet('nation'), F.col('c_nationkey') == F.col('n_nationkey'), 'inner')\n",
    ")\n",
    "\n",
    "monthly_df = monthly_df.withColumn('ship_month', F.date_format(F.to_date('l_shipdate'), 'yyyy-MM'))\n",
    "result_df = (\n",
    "    monthly_df\n",
    "    .filter(F.col('n_name').isin('UNITED STATES', 'CANADA'))\n",
    "    .groupBy('n_nationkey', 'n_name', 'ship_month')\n",
    "    .agg(F.count('*').alias('shipment_count'))\n",
    "    .orderBy('n_name', 'ship_month')\n",
    ")\n",
    "\n",
    "output_path = OUTPUT_ROOT / 'q5'\n",
    "write_csv(result_df, output_path)\n",
    "print(f'Q5 monthly volume metrics written to {output_path}')\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0e788-cb2a-43a1-9646-80d8a3612c2f",
   "metadata": {},
   "source": [
    "# Part B — Streaming (Structured Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79feb184-6f6e-4cc7-b67c-57dcc2bfb2a8",
   "metadata": {},
   "source": [
    "# B1. HourlyTripCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d094c7-cdf2-42a3-9fdd-01fe51a243df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 04:03:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly trip counts stored under /home/img/BigData/Lab04/lab04-practice/outputs/hourly_trip_count\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "hourly_checkpoint = CHECKPOINT_ROOT / 'hourly_trip_count'\n",
    "hourly_output = OUTPUT_ROOT / 'hourly_trip_count'\n",
    "shutil.rmtree(hourly_checkpoint, ignore_errors=True)\n",
    "shutil.rmtree(hourly_output, ignore_errors=True)\n",
    "hourly_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "hourly_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "hourly_stream = (\n",
    "    spark.readStream\n",
    "    .schema(TAXI_SCHEMA)\n",
    "    .option('header', True)\n",
    "    .csv(str(TAXI_DIR))\n",
    ")\n",
    "\n",
    "hourly_agg = (\n",
    "    hourly_stream\n",
    "    .withColumn('pickup_hour', F.date_trunc('hour', F.col('tpep_pickup_datetime')))\n",
    "    .groupBy('pickup_hour')\n",
    "    .agg(F.count('*').alias('trip_count'))\n",
    ")\n",
    "\n",
    "def write_hourly(batch_df: DataFrame, epoch_id: int) -> None:\n",
    "    batch_df.orderBy('pickup_hour').write.mode('overwrite').parquet(str(hourly_output))\n",
    "\n",
    "hourly_query = (\n",
    "    hourly_agg.writeStream\n",
    "    .outputMode('update')\n",
    "    .trigger(once=True)\n",
    "    .option('checkpointLocation', str(hourly_checkpoint))\n",
    "    .foreachBatch(write_hourly)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "hourly_query.awaitTermination()\n",
    "print(f'Hourly trip counts stored under {hourly_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d0dd3-e9b9-4ef4-9b80-0e1f935e262e",
   "metadata": {},
   "source": [
    "# B2. RegionTripCount (goldman, citigroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31377db0-18fb-4da5-8ce0-bfe9058522e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 04:03:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region trip counts stored under /home/img/BigData/Lab04/lab04-practice/outputs/region_trip_count\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "region_checkpoint = CHECKPOINT_ROOT / 'region_trip_count'\n",
    "region_output = OUTPUT_ROOT / 'region_trip_count'\n",
    "shutil.rmtree(region_checkpoint, ignore_errors=True)\n",
    "shutil.rmtree(region_output, ignore_errors=True)\n",
    "region_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "region_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "region_stream = (\n",
    "    spark.readStream\n",
    "    .schema(TAXI_SCHEMA)\n",
    "    .option('header', True)\n",
    "    .csv(str(TAXI_DIR))\n",
    ")\n",
    "\n",
    "bbox_goldman = BOUNDING_BOXES['goldman']\n",
    "bbox_citigroup = BOUNDING_BOXES['citigroup']\n",
    "\n",
    "region_enriched = region_stream.withColumn(\n",
    "    'region',\n",
    "    F.when(\n",
    "        (F.col('dropoff_longitude') >= bbox_goldman['lon_min']) &\n",
    "        (F.col('dropoff_longitude') <= bbox_goldman['lon_max']) &\n",
    "        (F.col('dropoff_latitude') >= bbox_goldman['lat_min']) &\n",
    "        (F.col('dropoff_latitude') <= bbox_goldman['lat_max']),\n",
    "        F.lit('goldman')\n",
    "    ).when(\n",
    "        (F.col('dropoff_longitude') >= bbox_citigroup['lon_min']) &\n",
    "        (F.col('dropoff_longitude') <= bbox_citigroup['lon_max']) &\n",
    "        (F.col('dropoff_latitude') >= bbox_citigroup['lat_min']) &\n",
    "        (F.col('dropoff_latitude') <= bbox_citigroup['lat_max']),\n",
    "        F.lit('citigroup')\n",
    "    )\n",
    ")\n",
    "\n",
    "region_counts = (\n",
    "    region_enriched\n",
    "    .filter(F.col('region').isNotNull())\n",
    "    .withColumn('dropoff_hour', F.date_trunc('hour', F.col('tpep_dropoff_datetime')))\n",
    "    .groupBy('region', 'dropoff_hour')\n",
    "    .agg(F.count('*').alias('trip_count'))\n",
    ")\n",
    "\n",
    "def write_region(batch_df: DataFrame, epoch_id: int) -> None:\n",
    "    batch_df.orderBy('dropoff_hour', 'region').write.mode('overwrite').parquet(str(region_output))\n",
    "\n",
    "region_query = (\n",
    "    region_counts.writeStream\n",
    "    .outputMode('update')\n",
    "    .trigger(once=True)\n",
    "    .option('checkpointLocation', str(region_checkpoint))\n",
    "    .foreachBatch(write_region)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "region_query.awaitTermination()\n",
    "print(f'Region trip counts stored under {region_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564b061-7463-4397-b935-8675eb63b784",
   "metadata": {},
   "source": [
    "# B3. TrendingArrivals (10-minute windows + state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3daa7fa-be8d-40ff-9d88-36d7982aff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, DataFrame\n",
    "\n",
    "\n",
    "def build_spark(app_name: str) -> SparkSession:\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .config('spark.sql.shuffle.partitions', '8')\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "\n",
    "def run(input_dir: str, checkpoint: str, output: str) -> None:\n",
    "    spark = build_spark('BDA-Streaming')\n",
    "    schema = T.StructType([\n",
    "        T.StructField('pickup_ts', T.TimestampType(), True),\n",
    "        T.StructField('dropoff_ts', T.TimestampType(), True),\n",
    "        T.StructField('lon', T.DoubleType(), True),\n",
    "        T.StructField('lat', T.DoubleType(), True),\n",
    "        T.StructField('vendor', T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    sdf = (\n",
    "        spark.readStream\n",
    "        .schema(schema)\n",
    "        .option('header', True)\n",
    "        .csv(input_dir)\n",
    "    )\n",
    "\n",
    "    hourly = (\n",
    "        sdf\n",
    "        .withWatermark('dropoff_ts', '5 minutes')\n",
    "        .groupBy(F.window('dropoff_ts', '1 hour'))\n",
    "        .agg(F.count('*').alias('trip_count'))\n",
    "    )\n",
    "\n",
    "    def _write_batch(batch_df: DataFrame, epoch_id: int) -> None:\n",
    "        batch_df.orderBy('window').write.mode('overwrite').parquet(output)\n",
    "\n",
    "    query = (\n",
    "        hourly.writeStream\n",
    "        .outputMode('update')\n",
    "        .option('checkpointLocation', checkpoint)\n",
    "        .trigger(once=True)\n",
    "        .foreachBatch(_write_batch)\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc3047-1a67-4c30-9bfa-02bb93301eb5",
   "metadata": {},
   "source": [
    "# Evidence & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f59ecd7-694e-4398-9dfb-fe0e75a954e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment details saved to /home/img/BigData/Lab04/lab04-practice/ENV.md\n",
      "\n",
      "Q1 counts (format, date, count):\n",
      "   ('text', '1995-03-15', 3)\n",
      "   ('parquet', '1995-03-15', 3)\n",
      "\n",
      "Q2 sample rows (text vs parquet):\n",
      "[text] -> [('1', 'Clerk#000000001'), ('2', 'Clerk#000000002')]\n",
      "[parquet] -> [(1, 'Clerk#000000001'), (1, 'Clerk#000000001'), (2, 'Clerk#000000002')]\n",
      "\n",
      "Generated output artifacts:\n",
      "  outputs/.ipynb_checkpoints/q1-checkpoint.txt\n",
      "  outputs/.ipynb_checkpoints/q2-checkpoint.txt\n",
      "  outputs/.ipynb_checkpoints/q3-checkpoint.txt\n",
      "  outputs/hourly_trip_count/._SUCCESS.crc\n",
      "  outputs/hourly_trip_count/.part-00000-332adecd-c549-4bff-9605-2055688f67db-c000.snappy.parquet.crc\n",
      "  outputs/hourly_trip_count/.part-00001-332adecd-c549-4bff-9605-2055688f67db-c000.snappy.parquet.crc\n",
      "  outputs/hourly_trip_count/_SUCCESS\n",
      "  outputs/hourly_trip_count/part-00000-332adecd-c549-4bff-9605-2055688f67db-c000.snappy.parquet\n",
      "  outputs/hourly_trip_count/part-00001-332adecd-c549-4bff-9605-2055688f67db-c000.snappy.parquet\n",
      "  outputs/q1.txt\n",
      "  outputs/q2.txt\n",
      "  outputs/q3.txt\n",
      "  outputs/q4/._SUCCESS.crc\n",
      "  outputs/q4/.part-00000-a6f6b6b2-f7d0-440b-92d0-c2f1aa03332e-c000.csv.crc\n",
      "  outputs/q4/_SUCCESS\n",
      "  outputs/q4/part-00000-a6f6b6b2-f7d0-440b-92d0-c2f1aa03332e-c000.csv\n",
      "  outputs/q5/._SUCCESS.crc\n",
      "  outputs/q5/.part-00000-918d2a20-339d-4bf3-aadd-d930805fee92-c000.csv.crc\n",
      "  outputs/q5/_SUCCESS\n",
      "  outputs/q5/part-00000-918d2a20-339d-4bf3-aadd-d930805fee92-c000.csv\n",
      "  outputs/region_trip_count/._SUCCESS.crc\n",
      "  outputs/region_trip_count/.part-00000-bf21c739-798f-481d-a36a-ffb89d4f75d0-c000.snappy.parquet.crc\n",
      "  outputs/region_trip_count/.part-00001-bf21c739-798f-481d-a36a-ffb89d4f75d0-c000.snappy.parquet.crc\n",
      "  outputs/region_trip_count/.part-00002-bf21c739-798f-481d-a36a-ffb89d4f75d0-c000.snappy.parquet.crc\n",
      "  outputs/region_trip_count/_SUCCESS\n",
      "  outputs/region_trip_count/part-00000-bf21c739-798f-481d-a36a-ffb89d4f75d0-c000.snappy.parquet\n",
      "  outputs/region_trip_count/part-00001-bf21c739-798f-481d-a36a-ffb89d4f75d0-c000.snappy.parquet\n",
      "  outputs/region_trip_count/part-00002-bf21c739-798f-481d-a36a-ffb89d4f75d0-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "java_output = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT).decode('utf-8').splitlines()[0]\n",
    "conf_items = sorted(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "env_lines = [\n",
    "    '# Environment Summary',\n",
    "    '',\n",
    "    f'- Python: {sys.version.split()[0]}',\n",
    "    f'- Spark: {spark.version}',\n",
    "    f'- PySpark: {pyspark.__version__}',\n",
    "    f'- Java: {java_output}',\n",
    "    f'- OS: {platform.platform()}',\n",
    "    '',\n",
    "    '## Spark Configuration',\n",
    "]\n",
    "\n",
    "env_lines.extend(f'- {key} = {value}' for key, value in conf_items)\n",
    "\n",
    "env_path = BASE_DIR / 'ENV.md'\n",
    "env_path.write_text(os.linesep.join(env_lines) + os.linesep)\n",
    "print(f'Environment details saved to {env_path}')\n",
    "\n",
    "if 'q1_results' in globals():\n",
    "    print()\n",
    "    print('Q1 counts (format, date, count):')\n",
    "    for record in q1_results:\n",
    "        print('  ', record)\n",
    "\n",
    "if 'q2_outputs' in globals():\n",
    "    print()\n",
    "    print('Q2 sample rows (text vs parquet):')\n",
    "    for fmt, records in q2_outputs.items():\n",
    "        print(f'[{fmt}] -> {records}')\n",
    "\n",
    "plan_path = PROOF_ROOT / 'plan_ppr.txt'\n",
    "if plan_path.exists():\n",
    "    print(f'Plan evidence available at {plan_path}')\n",
    "\n",
    "print()\n",
    "print('Generated output artifacts:')\n",
    "for path_entry in sorted(OUTPUT_ROOT.glob('**/*')):\n",
    "    if path_entry.is_file():\n",
    "        print(f'  {path_entry.relative_to(BASE_DIR)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
