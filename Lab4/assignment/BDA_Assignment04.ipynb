{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30acf193",
   "metadata": {},
   "source": [
    "# BDA Assignment ‚Äî Relational (TPC‚ÄëH, RDD‚Äëonly) + Streaming\n",
    "\n",
    "> Author : Badr TAJINI - Big Data Analytics - ESIEE 2025-2026\n",
    "\n",
    "**Chapter 7 :** Analyzing Relational Data (TPC‚ÄëH subset)  \n",
    "**Chapter 8 :** Real‚ÄëTime Analytics (NYC Taxi)\n",
    "\n",
    "**Tools :** Spark or PySpark.   \n",
    "**Advice:** Keep evidence and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6438452",
   "metadata": {},
   "source": [
    "## 0. Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d503455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Lab 4 Assignment - Relational & Streaming Analytics\n",
      "Big Data Analytics - ESIEE 2025-2026\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "1. System & Environment Information\n",
      "============================================================\n",
      "\n",
      "üìä System Information:\n",
      "  Platform    : Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "  Python      : 3.10.19\n",
      "  Python Path : /home/phams/miniconda3/envs/bda-env/bin/python\n",
      "\n",
      "============================================================\n",
      "2. Creating Spark Session\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 08:53:25 WARN Utils: Your hostname, LAPTOP-ED8D06VN, resolves to a loopback address: 127.0.1.1; using 172.19.238.66 instead (on interface eth0)\n",
      "25/12/07 08:53:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 08:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Spark Session Created Successfully!\n",
      "\n",
      "============================================================\n",
      "3. Spark & Library Versions\n",
      "============================================================\n",
      "\n",
      "üì¶ Versions:\n",
      "  Spark Version   : 4.0.1\n",
      "  PySpark Version : 4.0.1\n",
      "  Python Version  : 3.10.19\n",
      "  Scala Version   : version 2.13.16\n",
      "  Java Version    : 21.0.6\n",
      "\n",
      "============================================================\n",
      "4. Key Spark Configurations\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è Active Configurations:\n",
      "  spark.sql.session.timeZone                         : UTC\n",
      "  spark.sql.shuffle.partitions                       : 8\n",
      "  spark.driver.memory                                : 4g\n",
      "  spark.executor.memory                              : 4g\n",
      "  spark.default.parallelism                          : 8\n",
      "  spark.sql.adaptive.enabled                         : true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled      : true\n",
      "\n",
      "============================================================\n",
      "5. Session Information\n",
      "============================================================\n",
      "\n",
      "üåê Session Details:\n",
      "  Application Name : BDA-Assignment-Relational-Streaming\n",
      "  Master           : local[*]\n",
      "  Spark UI         : http://localhost:4040\n",
      "  Default Parallelism : 8\n",
      "\n",
      "============================================================\n",
      "6. Execution Timestamp\n",
      "============================================================\n",
      "\n",
      "‚è∞ Session Started (UTC):\n",
      "  2025-12-07 07:53:35 UTC\n",
      "  ISO Format: 2025-12-07T07:53:35.411565+00:00\n",
      "\n",
      "============================================================\n",
      "7. Spark Verification Test\n",
      "============================================================\n",
      "\n",
      "‚úÖ Test DataFrame Created:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "|id |name   |value|\n",
      "+---+-------+-----+\n",
      "|1  |Alice  |100  |\n",
      "|2  |Bob    |200  |\n",
      "|3  |Charlie|150  |\n",
      "+---+-------+-----+\n",
      "\n",
      "\n",
      "üìä DataFrame Schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ Row Count: 3\n",
      "\n",
      "============================================================\n",
      "‚úÖ BOOTSTRAP COMPLETE - READY FOR LAB 4\n",
      "============================================================\n",
      "\n",
      "üìã Next Steps:\n",
      "  1. Load datasets into data/\n",
      "  2. Follow Lab 4 assignment instructions\n",
      "  3. Part A: Relational Analytics (TPC-H style)\n",
      "  4. Part B: Streaming Analytics (micro-batch)\n",
      "  5. Capture Spark UI screenshots for each part\n",
      "  6. Save execution plans in proof/\n",
      "\n",
      "üéØ Spark UI Access: http://localhost:4040\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - create SparkSession('BDA-Assignment-Relational-Streaming') with UTC timezone\n",
    "# - print Spark/PySpark/Python versions\n",
    "# - set spark.sql.shuffle.partitions for local runs\n",
    "# ============================================================\n",
    "# Lab 4 Assignment - Bootstrap\n",
    "# Big Data Analytics - ESIEE 2025-2026\n",
    "# Author: Badr TAJINI\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Lab 4 Assignment - Relational & Streaming Analytics\")\n",
    "print(\"Big Data Analytics - ESIEE 2025-2026\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========== IMPORTS ==========\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import pyspark\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. System & Environment Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# System info\n",
    "print(f\"\\nüìä System Information:\")\n",
    "print(f\"  Platform    : {platform.platform()}\")\n",
    "print(f\"  Python      : {sys.version.split()[0]}\")\n",
    "print(f\"  Python Path : {sys.executable}\")\n",
    "\n",
    "# ========== CREATE SPARK SESSION ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Creating Spark Session\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Spark configuration optimized for local runs\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.session.timeZone\", \"UTC\")  # ‚úÖ UTC timezone\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"8\")  # ‚úÖ Optimized for local (vs default 200)\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.default.parallelism\", \"8\")\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # Adaptive Query Execution\n",
    "conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Create session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDA-Assignment-Relational-Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")  # Reduce noise in logs\n",
    "\n",
    "print(\"\\n‚úÖ Spark Session Created Successfully!\")\n",
    "\n",
    "# ========== PRINT VERSIONS ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Spark & Library Versions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Versions:\")\n",
    "print(f\"  Spark Version   : {spark.version}\")\n",
    "print(f\"  PySpark Version : {pyspark.__version__}\")\n",
    "print(f\"  Python Version  : {platform.python_version()}\")\n",
    "print(f\"  Scala Version   : {spark.sparkContext._jvm.scala.util.Properties.versionString()}\")\n",
    "print(f\"  Java Version    : {spark.sparkContext._jvm.System.getProperty('java.version')}\")\n",
    "\n",
    "# ========== PRINT KEY CONFIGS ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Key Spark Configurations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "key_configs = [\n",
    "    \"spark.sql.session.timeZone\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\"\n",
    "]\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Active Configurations:\")\n",
    "for config in key_configs:\n",
    "    value = spark.conf.get(config, \"Not Set\")\n",
    "    print(f\"  {config:<50} : {value}\")\n",
    "\n",
    "# ========== PRINT SESSION INFO ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Session Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüåê Session Details:\")\n",
    "print(f\"  Application Name : {spark.sparkContext.appName}\")\n",
    "print(f\"  Master           : {spark.sparkContext.master}\")\n",
    "print(f\"  Spark UI         : http://localhost:4040\")\n",
    "print(f\"  Default Parallelism : {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# ========== TIMESTAMP ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Execution Timestamp\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "utc_now = datetime.now(timezone.utc)\n",
    "print(f\"\\n‚è∞ Session Started (UTC):\")\n",
    "print(f\"  {utc_now.strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
    "print(f\"  ISO Format: {utc_now.isoformat()}\")\n",
    "\n",
    "# ========== VERIFICATION TEST ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. Spark Verification Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick test to verify Spark is working\n",
    "test_data = [(1, \"Alice\", 100), (2, \"Bob\", 200), (3, \"Charlie\", 150)]\n",
    "test_df = spark.createDataFrame(test_data, [\"id\", \"name\", \"value\"])\n",
    "\n",
    "print(\"\\n‚úÖ Test DataFrame Created:\")\n",
    "test_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nüìä DataFrame Schema:\")\n",
    "test_df.printSchema()\n",
    "\n",
    "print(\"\\nüî¢ Row Count:\", test_df.count())\n",
    "\n",
    "# Clean up test\n",
    "del test_df\n",
    "\n",
    "# ========== READY ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BOOTSTRAP COMPLETE - READY FOR LAB 4\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"  1. Load datasets into data/\")\n",
    "print(\"  2. Follow Lab 4 assignment instructions\")\n",
    "print(\"  3. Part A: Relational Analytics (TPC-H style)\")\n",
    "print(\"  4. Part B: Streaming Analytics (micro-batch)\")\n",
    "print(\"  5. Capture Spark UI screenshots for each part\")\n",
    "print(\"  6. Save execution plans in proof/\")\n",
    "\n",
    "print(\"\\nüéØ Spark UI Access: http://localhost:4040\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483b961",
   "metadata": {},
   "source": [
    "## 1. Data Layout & Quick Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a159a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 1 ‚Äî Dataset Extraction & Verification\n",
      "============================================================\n",
      "\n",
      "üì¶ Checking for compressed archives...\n",
      "\n",
      "üîç Extraction Status:\n",
      "\n",
      "  ‚úÖ TPC-H TXT            : Already extracted at data/tpch/TPC-H-0.1-TXT\n",
      "  ‚úÖ TPC-H Parquet        : Already extracted at data/tpch/TPC-H-0.1-PARQUET\n",
      "  ‚úÖ NYC Taxi             : Already extracted at data/taxi-data\n",
      "\n",
      "============================================================\n",
      "Dataset Path Verification\n",
      "============================================================\n",
      "\n",
      "üìÇ Verifying dataset paths:\n",
      "\n",
      "  ‚úÖ TPC-H TXT            : data/tpch/TPC-H-0.1-TXT (16 files)\n",
      "  ‚úÖ TPC-H Parquet        : data/tpch/TPC-H-0.1-PARQUET (0 files)\n",
      "  ‚úÖ NYC Taxi             : data/taxi-data (0 files)\n",
      "\n",
      "‚úÖ All dataset paths verified successfully!\n",
      "\n",
      "============================================================\n",
      "Part A ‚Äî TPC-H TXT Dataset Sanity Checks\n",
      "============================================================\n",
      "\n",
      "üìä TPC-H TXT Files:\n",
      "  Location: data/tpch/TPC-H-0.1-TXT\n",
      "\n",
      "  ‚úÖ lineitem.tbl         :  600,572 lines |  70.81 MB\n",
      "  ‚úÖ orders.tbl           :  150,000 lines |  16.11 MB\n",
      "  ‚úÖ part.tbl             :   20,000 lines |   2.28 MB\n",
      "  ‚úÖ supplier.tbl         :    1,000 lines |   0.13 MB\n",
      "  ‚úÖ partsupp.tbl         :   80,000 lines |  11.18 MB\n",
      "  ‚úÖ customer.tbl         :   15,000 lines |   2.31 MB\n",
      "  ‚úÖ nation.tbl           :       25 lines |   0.00 MB\n",
      "  ‚úÖ region.tbl           :        5 lines |   0.00 MB\n",
      "\n",
      "üìã Sample Records from lineitem.tbl (first 3 lines):\n",
      "\n",
      "  Line 1:\n",
      "    l_orderkey   : 1\n",
      "    l_partkey    : 15519\n",
      "    l_suppkey    : 785\n",
      "    l_quantity   : 17\n",
      "    l_shipdate   : 1996-03-13\n",
      "    (Total fields: 16)\n",
      "\n",
      "  Line 2:\n",
      "    l_orderkey   : 1\n",
      "    l_partkey    : 6731\n",
      "    l_suppkey    : 732\n",
      "    l_quantity   : 36\n",
      "    l_shipdate   : 1996-04-12\n",
      "    (Total fields: 16)\n",
      "\n",
      "  Line 3:\n",
      "    l_orderkey   : 1\n",
      "    l_partkey    : 6370\n",
      "    l_suppkey    : 371\n",
      "    l_quantity   : 8\n",
      "    l_shipdate   : 1996-01-29\n",
      "    (Total fields: 16)\n",
      "\n",
      "============================================================\n",
      "Part A ‚Äî TPC-H Parquet Dataset Sanity Checks\n",
      "============================================================\n",
      "\n",
      "üìä TPC-H Parquet Tables:\n",
      "  Location: data/tpch/TPC-H-0.1-PARQUET\n",
      "\n",
      "  ‚úÖ lineitem             :   4 files |  18.19 MB\n",
      "  ‚úÖ orders               :   4 files |   5.17 MB\n",
      "  ‚úÖ part                 :   1 files |   0.61 MB\n",
      "  ‚úÖ supplier             :   1 files |   0.08 MB\n",
      "  ‚ùå partsupp             : MISSING or not a directory\n",
      "  ‚úÖ customer             :   1 files |   1.19 MB\n",
      "  ‚úÖ nation               :   1 files |   0.00 MB\n",
      "  ‚ùå region               : MISSING or not a directory\n",
      "\n",
      "üîç Quick Parquet Read Test (lineitem):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  üìä Schema:\n",
      "root\n",
      " |-- l_orderkey: integer (nullable = true)\n",
      " |-- l_partkey: integer (nullable = true)\n",
      " |-- l_suppkey: integer (nullable = true)\n",
      " |-- l_linenumber: integer (nullable = true)\n",
      " |-- l_quantity: double (nullable = true)\n",
      " |-- l_extendedprice: double (nullable = true)\n",
      " |-- l_discount: double (nullable = true)\n",
      " |-- l_tax: double (nullable = true)\n",
      " |-- l_returnflag: string (nullable = true)\n",
      " |-- l_linestatus: string (nullable = true)\n",
      " |-- l_shipdate: string (nullable = true)\n",
      " |-- l_commitdate: string (nullable = true)\n",
      " |-- l_receiptdate: string (nullable = true)\n",
      " |-- l_shipinstruct: string (nullable = true)\n",
      " |-- l_shipmode: string (nullable = true)\n",
      " |-- l_comment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  üìè Row Count: 600,572\n",
      "\n",
      "  üìã Sample Records (first 3):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+----------+----------+\n",
      "|l_orderkey|l_partkey|l_suppkey|l_quantity|l_shipdate|\n",
      "+----------+---------+---------+----------+----------+\n",
      "|1         |15519    |785      |17.0      |1996-03-13|\n",
      "|1         |6731     |732      |36.0      |1996-04-12|\n",
      "|1         |6370     |371      |8.0       |1996-01-29|\n",
      "+----------+---------+---------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "  ‚úÖ Successfully converted to RDD: 7 partitions\n",
      "\n",
      "============================================================\n",
      "Part B ‚Äî NYC Taxi Dataset Sanity Checks\n",
      "============================================================\n",
      "\n",
      "üìÇ Taxi Data Location: data/taxi-data\n",
      "\n",
      "  üìä Total CSV files: 0\n",
      "  ‚ùå No CSV files found!\n",
      "\n",
      "============================================================\n",
      "Section 1 Summary\n",
      "============================================================\n",
      "\n",
      "‚úÖ Dataset verification completed at 2025-12-06 15:54:54 UTC\n",
      "\n",
      "üìä Summary:\n",
      "  Part A - TPC-H TXT     : 8 tables\n",
      "  Part A - TPC-H Parquet : 8 tables\n",
      "  Part B - Taxi CSV      : 0 files\n",
      "\n",
      "üéØ Ready to proceed:\n",
      "  ‚úÖ Part A (A1-A7): Relational analytics with TPC-H\n",
      "  ‚úÖ Part B (B1-B3): Streaming analytics with NYC Taxi\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - assert paths for:\n",
    "#   data/tpch/TPC-H-0.1-TXT/  and  data/tpch/TPC-H-0.1-PARQUET/\n",
    "#   data/taxi-data/\n",
    "# - small sanity reads: count lines/files; print sample records\n",
    "# ============================================================\n",
    "# Section 1 - Dataset Extraction & Verification\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 1 ‚Äî Dataset Extraction & Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ========== EXTRACT ARCHIVES IF NEEDED ==========\n",
    "\n",
    "print(\"\\nüì¶ Checking for compressed archives...\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = \"data\"\n",
    "TPCH_DIR = os.path.join(DATA_DIR, \"tpch\")\n",
    "TAXI_DIR = os.path.join(DATA_DIR, \"taxi-data\")\n",
    "\n",
    "# Create directories if needed\n",
    "os.makedirs(TPCH_DIR, exist_ok=True)\n",
    "os.makedirs(TAXI_DIR, exist_ok=True)\n",
    "\n",
    "# Archives to extract\n",
    "archives = {\n",
    "    \"TPC-H TXT\": {\n",
    "        \"archive\": os.path.join(DATA_DIR, \"TPC-H-0.1-TXT.tar.gz\"),\n",
    "        \"target\": os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\"),\n",
    "        \"extract_to\": TPCH_DIR\n",
    "    },\n",
    "    \"TPC-H Parquet\": {\n",
    "        \"archive\": os.path.join(DATA_DIR, \"TPC-H-0.1-PARQUET.tar.gz\"),\n",
    "        \"target\": os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\"),\n",
    "        \"extract_to\": TPCH_DIR\n",
    "    },\n",
    "    \"NYC Taxi\": {\n",
    "        \"archive\": os.path.join(DATA_DIR, \"taxi-data.tar.gz\"),\n",
    "        \"target\": TAXI_DIR,\n",
    "        \"extract_to\": DATA_DIR\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüîç Extraction Status:\\n\")\n",
    "\n",
    "for name, paths in archives.items():\n",
    "    archive_path = paths[\"archive\"]\n",
    "    target_path = paths[\"target\"]\n",
    "    extract_to = paths[\"extract_to\"]\n",
    "    \n",
    "    # Check if already extracted\n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"  ‚úÖ {name:<20} : Already extracted at {target_path}\")\n",
    "    elif os.path.exists(archive_path):\n",
    "        print(f\"  üì¶ {name:<20} : Extracting {archive_path}...\")\n",
    "        \n",
    "        try:\n",
    "            with tarfile.open(archive_path, 'r:gz') as tar:\n",
    "                tar.extractall(path=extract_to)\n",
    "            print(f\"     ‚úÖ Extracted to {target_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Extraction failed: {e}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {name:<20} : Archive not found at {archive_path}\")\n",
    "\n",
    "# ========== PATH VERIFICATION ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Dataset Path Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Expected paths after extraction\n",
    "TPCH_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "TPCH_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "TAXI_DATA_PATH = TAXI_DIR\n",
    "\n",
    "paths_to_check = {\n",
    "    \"TPC-H TXT\": TPCH_TXT_PATH,\n",
    "    \"TPC-H Parquet\": TPCH_PARQUET_PATH,\n",
    "    \"NYC Taxi\": TAXI_DATA_PATH\n",
    "}\n",
    "\n",
    "all_paths_valid = True\n",
    "\n",
    "print(\"\\nüìÇ Verifying dataset paths:\\n\")\n",
    "\n",
    "for name, path in paths_to_check.items():\n",
    "    if os.path.exists(path):\n",
    "        # Count files\n",
    "        if os.path.isdir(path):\n",
    "            num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "            print(f\"  ‚úÖ {name:<20} : {path} ({num_files} files)\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ {name:<20} : {path}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {name:<20} : {path} (MISSING)\")\n",
    "        all_paths_valid = False\n",
    "\n",
    "if not all_paths_valid:\n",
    "    raise FileNotFoundError(\n",
    "        \"‚ùå Some datasets are still missing! Check if the .tar.gz archives \"\n",
    "        \"are in the data/ directory and try re-running this cell.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ All dataset paths verified successfully!\")\n",
    "\n",
    "# ========== SANITY CHECKS - PART A (TPC-H TXT) ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part A ‚Äî TPC-H TXT Dataset Sanity Checks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "TPCH_TABLES = [\n",
    "    \"lineitem.tbl\",\n",
    "    \"orders.tbl\",\n",
    "    \"part.tbl\",\n",
    "    \"supplier.tbl\",\n",
    "    \"partsupp.tbl\",\n",
    "    \"customer.tbl\",\n",
    "    \"nation.tbl\",\n",
    "    \"region.tbl\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä TPC-H TXT Files:\")\n",
    "print(f\"  Location: {TPCH_TXT_PATH}\\n\")\n",
    "\n",
    "for table in TPCH_TABLES:\n",
    "    txt_file = os.path.join(TPCH_TXT_PATH, table)\n",
    "    \n",
    "    if os.path.exists(txt_file):\n",
    "        # Count lines\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            num_lines = sum(1 for _ in f)\n",
    "        \n",
    "        # File size\n",
    "        file_size_mb = os.path.getsize(txt_file) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"  ‚úÖ {table:<20} : {num_lines:>8,} lines | {file_size_mb:>6.2f} MB\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {table:<20} : MISSING\")\n",
    "\n",
    "# Sample from lineitem.tbl\n",
    "print(\"\\nüìã Sample Records from lineitem.tbl (first 3 lines):\")\n",
    "lineitem_path = os.path.join(TPCH_TXT_PATH, \"lineitem.tbl\")\n",
    "\n",
    "if os.path.exists(lineitem_path):\n",
    "    with open(lineitem_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            # Parse pipe-delimited\n",
    "            clean_line = line.rstrip('|\\n')\n",
    "            fields = clean_line.split('|')\n",
    "            \n",
    "            print(f\"\\n  Line {i+1}:\")\n",
    "            print(f\"    l_orderkey   : {fields[0]}\")\n",
    "            print(f\"    l_partkey    : {fields[1]}\")\n",
    "            print(f\"    l_suppkey    : {fields[2]}\")\n",
    "            print(f\"    l_quantity   : {fields[4]}\")\n",
    "            print(f\"    l_shipdate   : {fields[10]}\")\n",
    "            print(f\"    (Total fields: {len(fields)})\")\n",
    "\n",
    "# ========== SANITY CHECKS - PART A (TPC-H PARQUET) ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part A ‚Äî TPC-H Parquet Dataset Sanity Checks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "PARQUET_TABLES = [\n",
    "    \"lineitem\",\n",
    "    \"orders\",\n",
    "    \"part\",\n",
    "    \"supplier\",\n",
    "    \"partsupp\",\n",
    "    \"customer\",\n",
    "    \"nation\",\n",
    "    \"region\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä TPC-H Parquet Tables:\")\n",
    "print(f\"  Location: {TPCH_PARQUET_PATH}\\n\")\n",
    "\n",
    "for table in PARQUET_TABLES:\n",
    "    parquet_dir = os.path.join(TPCH_PARQUET_PATH, table)\n",
    "    \n",
    "    if os.path.exists(parquet_dir) and os.path.isdir(parquet_dir):\n",
    "        # Count Parquet files\n",
    "        parquet_files = [f for f in os.listdir(parquet_dir) if f.endswith('.parquet')]\n",
    "        num_files = len(parquet_files)\n",
    "        \n",
    "        # Total size\n",
    "        total_size = sum(\n",
    "            os.path.getsize(os.path.join(parquet_dir, f)) \n",
    "            for f in parquet_files\n",
    "        )\n",
    "        total_size_mb = total_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"  ‚úÖ {table:<20} : {num_files:>3} files | {total_size_mb:>6.2f} MB\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {table:<20} : MISSING or not a directory\")\n",
    "\n",
    "# Quick Parquet read test\n",
    "print(\"\\nüîç Quick Parquet Read Test (lineitem):\")\n",
    "\n",
    "lineitem_parquet = os.path.join(TPCH_PARQUET_PATH, \"lineitem\")\n",
    "\n",
    "if os.path.exists(lineitem_parquet):\n",
    "    # Load with Spark\n",
    "    lineitem_df = spark.read.parquet(lineitem_parquet)\n",
    "    \n",
    "    print(f\"\\n  üìä Schema:\")\n",
    "    lineitem_df.printSchema()\n",
    "    \n",
    "    print(f\"\\n  üìè Row Count: {lineitem_df.count():,}\")\n",
    "    \n",
    "    print(f\"\\n  üìã Sample Records (first 3):\")\n",
    "    lineitem_df.select(\n",
    "        \"l_orderkey\", \n",
    "        \"l_partkey\", \n",
    "        \"l_suppkey\", \n",
    "        \"l_quantity\", \n",
    "        \"l_shipdate\"\n",
    "    ).show(3, truncate=False)\n",
    "    \n",
    "    # Convert to RDD (required for Part A)\n",
    "    lineitem_rdd = lineitem_df.rdd\n",
    "    print(f\"\\n  ‚úÖ Successfully converted to RDD: {lineitem_rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# ========== SANITY CHECKS - PART B (TAXI DATA) ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part B ‚Äî NYC Taxi Dataset Sanity Checks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÇ Taxi Data Location: {TAXI_DATA_PATH}\\n\")\n",
    "\n",
    "# List CSV files\n",
    "if os.path.exists(TAXI_DATA_PATH):\n",
    "    taxi_files = sorted([\n",
    "        f for f in os.listdir(TAXI_DATA_PATH) \n",
    "        if f.endswith('.csv')\n",
    "    ])\n",
    "    \n",
    "    num_taxi_files = len(taxi_files)\n",
    "    \n",
    "    print(f\"  üìä Total CSV files: {num_taxi_files}\")\n",
    "    \n",
    "    if num_taxi_files > 0:\n",
    "        # Show first 5 and last 5\n",
    "        print(f\"\\n  üìã First 5 files:\")\n",
    "        for f in taxi_files[:5]:\n",
    "            file_path = os.path.join(TAXI_DATA_PATH, f)\n",
    "            size_kb = os.path.getsize(file_path) / 1024\n",
    "            print(f\"    - {f:<40} ({size_kb:>8.2f} KB)\")\n",
    "        \n",
    "        if num_taxi_files > 10:\n",
    "            print(f\"\\n  ... ({num_taxi_files - 10} files omitted) ...\")\n",
    "        \n",
    "        if num_taxi_files > 5:\n",
    "            print(f\"\\n  üìã Last 5 files:\")\n",
    "            for f in taxi_files[-5:]:\n",
    "                file_path = os.path.join(TAXI_DATA_PATH, f)\n",
    "                size_kb = os.path.getsize(file_path) / 1024\n",
    "                print(f\"    - {f:<40} ({size_kb:>8.2f} KB)\")\n",
    "        \n",
    "        # Sample first file\n",
    "        first_file = os.path.join(TAXI_DATA_PATH, taxi_files[0])\n",
    "        \n",
    "        print(f\"\\n  üìã Sample from {taxi_files[0]}:\")\n",
    "        with open(first_file, 'r', encoding='utf-8') as f:\n",
    "            # Header\n",
    "            header = f.readline().strip()\n",
    "            print(f\"\\n    Header: {header}\")\n",
    "            \n",
    "            # First 3 records\n",
    "            print(f\"\\n    First 3 records:\")\n",
    "            for i in range(3):\n",
    "                line = f.readline().strip()\n",
    "                if line:\n",
    "                    print(f\"      {i+1}. {line}\")\n",
    "        \n",
    "        # Total size\n",
    "        total_size = sum(\n",
    "            os.path.getsize(os.path.join(TAXI_DATA_PATH, f)) \n",
    "            for f in taxi_files\n",
    "        )\n",
    "        total_size_mb = total_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\n  üíæ Total dataset size: {total_size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"  ‚ùå No CSV files found!\")\n",
    "else:\n",
    "    print(\"  ‚ùå Taxi data directory does not exist!\")\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 1 Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset verification completed at {timestamp}\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Part A - TPC-H TXT     : {len(TPCH_TABLES)} tables\")\n",
    "print(f\"  Part A - TPC-H Parquet : {len(PARQUET_TABLES)} tables\")\n",
    "print(f\"  Part B - Taxi CSV      : {num_taxi_files if os.path.exists(TAXI_DATA_PATH) else 0} files\")\n",
    "\n",
    "print(f\"\\nüéØ Ready to proceed:\")\n",
    "print(f\"  ‚úÖ Part A (A1-A7): Relational analytics with TPC-H\")\n",
    "print(f\"  ‚úÖ Part B (B1-B3): Streaming analytics with NYC Taxi\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd917a4",
   "metadata": {},
   "source": [
    "## 2. Parsers and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce48ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 2 ‚Äî TPC-H Helpers & Parsers\n",
      "============================================================\n",
      "\n",
      "üìã Defining TPC-H Text Parsers...\n",
      "  ‚úÖ Text parsers defined for 8 TPC-H tables\n",
      "\n",
      "üì¶ Defining Parquet Loaders...\n",
      "  ‚úÖ Parquet loader function defined\n",
      "\n",
      "üì° Defining Broadcast Helper...\n",
      "  ‚úÖ Broadcast helper defined\n",
      "\n",
      "üîß Defining Utility Functions...\n",
      "  ‚úÖ Utility functions defined:\n",
      "     - save_tuples(path, iter, header)\n",
      "     - month_trunc(date_str) -> YYYY-MM\n",
      "     - format_answer(count) -> ANSWER=<count>\n",
      "\n",
      "============================================================\n",
      "Section 2 ‚Äî Parser Verification Tests\n",
      "============================================================\n",
      "\n",
      "üß™ Testing Text Parsers:\n",
      "\n",
      "  Lineitem Parser:\n",
      "    Input  : 1|15519|785|1|17.00|24710.35|0.04|0.02|N|O|1996-03-13|1996-02-12|1996-03-22|DELI...\n",
      "    Output : l_orderkey=1, l_partkey=15519, l_shipdate=1996-03-13\n",
      "    Fields : 16 (expected 16)\n",
      "\n",
      "  Orders Parser:\n",
      "    Input  : 1|370|O|172799.49|1996-01-02|5-LOW|Clerk#000000951|0|nstructions sleep furiously...\n",
      "    Output : o_orderkey=1, o_clerk=Clerk#000000951, o_orderdate=1996-01-02\n",
      "    Fields : 9 (expected 9)\n",
      "\n",
      "  Nation Parser:\n",
      "    Input  : 0|ALGERIA|0| haggle. carefully final deposits detect slyly agai|\n",
      "    Output : n_nationkey=0, n_name=ALGERIA\n",
      "    Fields : 4 (expected 4)\n",
      "\n",
      "üß™ Testing Utility Functions:\n",
      "\n",
      "  month_trunc():\n",
      "    '1996-01-15' ‚Üí '1996-01'\n",
      "    '1995-12-31' ‚Üí '1995-12'\n",
      "\n",
      "  format_answer():\n",
      "    1234 ‚Üí 'ANSWER=1234'\n",
      "    0    ‚Üí 'ANSWER=0'\n",
      "\n",
      "============================================================\n",
      "Section 2 Summary\n",
      "============================================================\n",
      "\n",
      "‚úÖ TPC-H Helpers & Parsers Ready:\n",
      "\n",
      "  üìã Text Parsers (8 tables):\n",
      "     - parse_lineitem(line) ‚Üí 16-field tuple\n",
      "     - parse_orders(line) ‚Üí 9-field tuple\n",
      "     - parse_part(line) ‚Üí 9-field tuple\n",
      "     - parse_supplier(line) ‚Üí 7-field tuple\n",
      "     - parse_customer(line) ‚Üí 8-field tuple\n",
      "     - parse_nation(line) ‚Üí 4-field tuple\n",
      "     - parse_region(line) ‚Üí 3-field tuple\n",
      "     - parse_partsupp(line) ‚Üí 5-field tuple\n",
      "\n",
      "  üì¶ Parquet Loaders:\n",
      "     - load_parquet_as_rdd(spark, table, path)\n",
      "\n",
      "  üì° Broadcast Helper:\n",
      "     - broadcast_dimension(spark, rdd, key_extractor)\n",
      "\n",
      "  üîß Utilities:\n",
      "     - save_tuples(path, iterator, header)\n",
      "     - month_trunc(date_str) ‚Üí YYYY-MM\n",
      "     - format_answer(count) ‚Üí ANSWER=<count>\n",
      "\n",
      "üéØ Next Steps:\n",
      "  - Load TPC-H tables (TXT or Parquet)\n",
      "  - Implement A1-A7 queries using RDD operations only\n",
      "  - Compare TXT vs Parquet performance\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - text parsers per TPC-H table (split('|') -> typed tuples)\n",
    "# - parquet loaders using spark.read.parquet(...).rdd\n",
    "# - broadcast helper for small dims (part, supplier, customer, nation)\n",
    "# - utilities: save_tuples(path, iterator); month_trunc('YYYY-MM-DD')\n",
    "# ============================================================\n",
    "# Section 2 - TPC-H Helpers & Parsers (Part A)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 2 ‚Äî TPC-H Helpers & Parsers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List, Iterator\n",
    "import os\n",
    "\n",
    "# ========== TEXT PARSERS (PIPE-DELIMITED) ==========\n",
    "\n",
    "print(\"\\nüìã Defining TPC-H Text Parsers...\")\n",
    "\n",
    "# Schema references:\n",
    "# - lineitem: 16 fields (l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, ...)\n",
    "# - orders: 9 fields (o_orderkey, o_custkey, o_orderstatus, o_totalprice, ...)\n",
    "# - part: 9 fields (p_partkey, p_name, p_mfgr, p_brand, p_type, ...)\n",
    "# - supplier: 7 fields (s_suppkey, s_name, s_address, s_nationkey, ...)\n",
    "# - customer: 8 fields (c_custkey, c_name, c_address, c_nationkey, ...)\n",
    "# - nation: 4 fields (n_nationkey, n_name, n_regionkey, n_comment)\n",
    "# - region: 3 fields (r_regionkey, r_name, r_comment)\n",
    "# - partsupp: 5 fields (ps_partkey, ps_suppkey, ps_availqty, ps_supplycost, ps_comment)\n",
    "\n",
    "def parse_lineitem(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse lineitem.tbl (pipe-delimited)\n",
    "    \n",
    "    Schema (16 fields):\n",
    "    0:  l_orderkey (int)\n",
    "    1:  l_partkey (int)\n",
    "    2:  l_suppkey (int)\n",
    "    3:  l_linenumber (int)\n",
    "    4:  l_quantity (float)\n",
    "    5:  l_extendedprice (float)\n",
    "    6:  l_discount (float)\n",
    "    7:  l_tax (float)\n",
    "    8:  l_returnflag (str)\n",
    "    9:  l_linestatus (str)\n",
    "    10: l_shipdate (str YYYY-MM-DD)\n",
    "    11: l_commitdate (str)\n",
    "    12: l_receiptdate (str)\n",
    "    13: l_shipinstruct (str)\n",
    "    14: l_shipmode (str)\n",
    "    15: l_comment (str)\n",
    "    \n",
    "    Returns: (l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, \n",
    "              l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus,\n",
    "              l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, \n",
    "              l_shipmode, l_comment)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # l_orderkey\n",
    "        int(fields[1]),      # l_partkey\n",
    "        int(fields[2]),      # l_suppkey\n",
    "        int(fields[3]),      # l_linenumber\n",
    "        float(fields[4]),    # l_quantity\n",
    "        float(fields[5]),    # l_extendedprice\n",
    "        float(fields[6]),    # l_discount\n",
    "        float(fields[7]),    # l_tax\n",
    "        fields[8],           # l_returnflag\n",
    "        fields[9],           # l_linestatus\n",
    "        fields[10],          # l_shipdate (YYYY-MM-DD)\n",
    "        fields[11],          # l_commitdate\n",
    "        fields[12],          # l_receiptdate\n",
    "        fields[13],          # l_shipinstruct\n",
    "        fields[14],          # l_shipmode\n",
    "        fields[15]           # l_comment\n",
    "    )\n",
    "\n",
    "def parse_orders(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse orders.tbl\n",
    "    \n",
    "    Schema (9 fields):\n",
    "    0: o_orderkey (int)\n",
    "    1: o_custkey (int)\n",
    "    2: o_orderstatus (str)\n",
    "    3: o_totalprice (float)\n",
    "    4: o_orderdate (str YYYY-MM-DD)\n",
    "    5: o_orderpriority (str)\n",
    "    6: o_clerk (str)\n",
    "    7: o_shippriority (int)\n",
    "    8: o_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # o_orderkey\n",
    "        int(fields[1]),      # o_custkey\n",
    "        fields[2],           # o_orderstatus\n",
    "        float(fields[3]),    # o_totalprice\n",
    "        fields[4],           # o_orderdate\n",
    "        fields[5],           # o_orderpriority\n",
    "        fields[6],           # o_clerk\n",
    "        int(fields[7]),      # o_shippriority\n",
    "        fields[8]            # o_comment\n",
    "    )\n",
    "\n",
    "def parse_part(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse part.tbl\n",
    "    \n",
    "    Schema (9 fields):\n",
    "    0: p_partkey (int)\n",
    "    1: p_name (str)\n",
    "    2: p_mfgr (str)\n",
    "    3: p_brand (str)\n",
    "    4: p_type (str)\n",
    "    5: p_size (int)\n",
    "    6: p_container (str)\n",
    "    7: p_retailprice (float)\n",
    "    8: p_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # p_partkey\n",
    "        fields[1],           # p_name\n",
    "        fields[2],           # p_mfgr\n",
    "        fields[3],           # p_brand\n",
    "        fields[4],           # p_type\n",
    "        int(fields[5]),      # p_size\n",
    "        fields[6],           # p_container\n",
    "        float(fields[7]),    # p_retailprice\n",
    "        fields[8]            # p_comment\n",
    "    )\n",
    "\n",
    "def parse_supplier(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse supplier.tbl\n",
    "    \n",
    "    Schema (7 fields):\n",
    "    0: s_suppkey (int)\n",
    "    1: s_name (str)\n",
    "    2: s_address (str)\n",
    "    3: s_nationkey (int)\n",
    "    4: s_phone (str)\n",
    "    5: s_acctbal (float)\n",
    "    6: s_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # s_suppkey\n",
    "        fields[1],           # s_name\n",
    "        fields[2],           # s_address\n",
    "        int(fields[3]),      # s_nationkey\n",
    "        fields[4],           # s_phone\n",
    "        float(fields[5]),    # s_acctbal\n",
    "        fields[6]            # s_comment\n",
    "    )\n",
    "\n",
    "def parse_customer(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse customer.tbl\n",
    "    \n",
    "    Schema (8 fields):\n",
    "    0: c_custkey (int)\n",
    "    1: c_name (str)\n",
    "    2: c_address (str)\n",
    "    3: c_nationkey (int)\n",
    "    4: c_phone (str)\n",
    "    5: c_acctbal (float)\n",
    "    6: c_mktsegment (str)\n",
    "    7: c_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # c_custkey\n",
    "        fields[1],           # c_name\n",
    "        fields[2],           # c_address\n",
    "        int(fields[3]),      # c_nationkey\n",
    "        fields[4],           # c_phone\n",
    "        float(fields[5]),    # c_acctbal\n",
    "        fields[6],           # c_mktsegment\n",
    "        fields[7]            # c_comment\n",
    "    )\n",
    "\n",
    "def parse_nation(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse nation.tbl\n",
    "    \n",
    "    Schema (4 fields):\n",
    "    0: n_nationkey (int)\n",
    "    1: n_name (str)\n",
    "    2: n_regionkey (int)\n",
    "    3: n_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # n_nationkey\n",
    "        fields[1],           # n_name\n",
    "        int(fields[2]),      # n_regionkey\n",
    "        fields[3]            # n_comment\n",
    "    )\n",
    "\n",
    "def parse_region(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse region.tbl\n",
    "    \n",
    "    Schema (3 fields):\n",
    "    0: r_regionkey (int)\n",
    "    1: r_name (str)\n",
    "    2: r_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # r_regionkey\n",
    "        fields[1],           # r_name\n",
    "        fields[2]            # r_comment\n",
    "    )\n",
    "\n",
    "def parse_partsupp(line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Parse partsupp.tbl\n",
    "    \n",
    "    Schema (5 fields):\n",
    "    0: ps_partkey (int)\n",
    "    1: ps_suppkey (int)\n",
    "    2: ps_availqty (int)\n",
    "    3: ps_supplycost (float)\n",
    "    4: ps_comment (str)\n",
    "    \"\"\"\n",
    "    fields = line.rstrip('|\\n').split('|')\n",
    "    \n",
    "    return (\n",
    "        int(fields[0]),      # ps_partkey\n",
    "        int(fields[1]),      # ps_suppkey\n",
    "        int(fields[2]),      # ps_availqty\n",
    "        float(fields[3]),    # ps_supplycost\n",
    "        fields[4]            # ps_comment\n",
    "    )\n",
    "\n",
    "print(\"  ‚úÖ Text parsers defined for 8 TPC-H tables\")\n",
    "\n",
    "# ========== PARQUET LOADERS ==========\n",
    "\n",
    "print(\"\\nüì¶ Defining Parquet Loaders...\")\n",
    "\n",
    "def load_parquet_as_rdd(spark, table_name: str, base_path: str):\n",
    "    \"\"\"\n",
    "    Load Parquet table and convert to RDD\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        table_name: Name of table (e.g., \"lineitem\", \"orders\")\n",
    "        base_path: Base path to PARQUET directory (e.g., data/tpch/TPC-H-0.1-PARQUET)\n",
    "    \n",
    "    Returns:\n",
    "        RDD of Rows\n",
    "    \n",
    "    Note: Assignment allows loading Parquet with DataFrame reader, then .rdd\n",
    "    \"\"\"\n",
    "    parquet_path = os.path.join(base_path, table_name)\n",
    "    \n",
    "    # Load Parquet as DataFrame\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "    \n",
    "    # Convert to RDD (allowed by assignment)\n",
    "    rdd = df.rdd\n",
    "    \n",
    "    print(f\"  ‚úÖ Loaded {table_name} from Parquet: {rdd.count():,} rows, {rdd.getNumPartitions()} partitions\")\n",
    "    \n",
    "    return rdd\n",
    "\n",
    "print(\"  ‚úÖ Parquet loader function defined\")\n",
    "\n",
    "# ========== BROADCAST HELPER ==========\n",
    "\n",
    "print(\"\\nüì° Defining Broadcast Helper...\")\n",
    "\n",
    "def broadcast_dimension(spark, rdd, key_extractor):\n",
    "    \"\"\"\n",
    "    Broadcast small dimension table for hash joins\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        rdd: RDD of tuples\n",
    "        key_extractor: Function to extract key from tuple\n",
    "    \n",
    "    Returns:\n",
    "        Broadcast variable containing dict {key -> tuple}\n",
    "    \n",
    "    Example:\n",
    "        # Broadcast part table keyed by p_partkey\n",
    "        part_bc = broadcast_dimension(spark, part_rdd, lambda t: t[0])\n",
    "        \n",
    "        # Use in map: part_bc.value.get(partkey)\n",
    "    \"\"\"\n",
    "    # Collect dimension into dict {key -> tuple}\n",
    "    dim_dict = rdd.map(lambda t: (key_extractor(t), t)).collectAsMap()\n",
    "    \n",
    "    # Broadcast\n",
    "    bc_var = spark.sparkContext.broadcast(dim_dict)\n",
    "    \n",
    "    print(f\"  ‚úÖ Broadcasted dimension: {len(dim_dict):,} entries\")\n",
    "    \n",
    "    return bc_var\n",
    "\n",
    "print(\"  ‚úÖ Broadcast helper defined\")\n",
    "\n",
    "# ========== UTILITIES ==========\n",
    "\n",
    "print(\"\\nüîß Defining Utility Functions...\")\n",
    "\n",
    "def save_tuples(output_path: str, tuples_iter: Iterator, header: str = None):\n",
    "    \"\"\"\n",
    "    Save iterator of tuples to file (CSV-like format)\n",
    "    \n",
    "    Args:\n",
    "        output_path: Path to output file\n",
    "        tuples_iter: Iterator of tuples\n",
    "        header: Optional CSV header line\n",
    "    \n",
    "    Example:\n",
    "        save_tuples(\"outputs/a1_result.txt\", \n",
    "                    [(1, 'Alice'), (2, 'Bob')],\n",
    "                    header=\"id,name\")\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        if header:\n",
    "            f.write(header + '\\n')\n",
    "        \n",
    "        for tup in tuples_iter:\n",
    "            # Join tuple elements with comma\n",
    "            line = ','.join(str(x) for x in tup)\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    print(f\"  ‚úÖ Saved tuples to {output_path}\")\n",
    "\n",
    "def month_trunc(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Truncate date to YYYY-MM format\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date in YYYY-MM-DD format\n",
    "    \n",
    "    Returns:\n",
    "        YYYY-MM string\n",
    "    \n",
    "    Example:\n",
    "        >>> month_trunc('1996-01-15')\n",
    "        '1996-01'\n",
    "    \"\"\"\n",
    "    return date_str[:7]  # Extract first 7 chars (YYYY-MM)\n",
    "\n",
    "def format_answer(count: int) -> str:\n",
    "    \"\"\"\n",
    "    Format count as ANSWER=<count> for A1 output\n",
    "    \n",
    "    Args:\n",
    "        count: Integer count\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string\n",
    "    \n",
    "    Example:\n",
    "        >>> format_answer(1234)\n",
    "        'ANSWER=1234'\n",
    "    \"\"\"\n",
    "    return f\"ANSWER={count}\"\n",
    "\n",
    "print(\"  ‚úÖ Utility functions defined:\")\n",
    "print(\"     - save_tuples(path, iter, header)\")\n",
    "print(\"     - month_trunc(date_str) -> YYYY-MM\")\n",
    "print(\"     - format_answer(count) -> ANSWER=<count>\")\n",
    "\n",
    "# ========== VERIFICATION TESTS ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 2 ‚Äî Parser Verification Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test text parsers with sample data\n",
    "print(\"\\nüß™ Testing Text Parsers:\\n\")\n",
    "\n",
    "# Sample lineitem line\n",
    "sample_lineitem = \"1|15519|785|1|17.00|24710.35|0.04|0.02|N|O|1996-03-13|1996-02-12|1996-03-22|DELIVER IN PERSON|TRUCK|egular courts above the|\"\n",
    "parsed_lineitem = parse_lineitem(sample_lineitem)\n",
    "\n",
    "print(\"  Lineitem Parser:\")\n",
    "print(f\"    Input  : {sample_lineitem[:80]}...\")\n",
    "print(f\"    Output : l_orderkey={parsed_lineitem[0]}, l_partkey={parsed_lineitem[1]}, \"\n",
    "      f\"l_shipdate={parsed_lineitem[10]}\")\n",
    "print(f\"    Fields : {len(parsed_lineitem)} (expected 16)\")\n",
    "\n",
    "# Sample orders line\n",
    "sample_orders = \"1|370|O|172799.49|1996-01-02|5-LOW|Clerk#000000951|0|nstructions sleep furiously among |\"\n",
    "parsed_orders = parse_orders(sample_orders)\n",
    "\n",
    "print(\"\\n  Orders Parser:\")\n",
    "print(f\"    Input  : {sample_orders[:80]}...\")\n",
    "print(f\"    Output : o_orderkey={parsed_orders[0]}, o_clerk={parsed_orders[6]}, \"\n",
    "      f\"o_orderdate={parsed_orders[4]}\")\n",
    "print(f\"    Fields : {len(parsed_orders)} (expected 9)\")\n",
    "\n",
    "# Sample nation line\n",
    "sample_nation = \"0|ALGERIA|0| haggle. carefully final deposits detect slyly agai|\"\n",
    "parsed_nation = parse_nation(sample_nation)\n",
    "\n",
    "print(\"\\n  Nation Parser:\")\n",
    "print(f\"    Input  : {sample_nation}\")\n",
    "print(f\"    Output : n_nationkey={parsed_nation[0]}, n_name={parsed_nation[1]}\")\n",
    "print(f\"    Fields : {len(parsed_nation)} (expected 4)\")\n",
    "\n",
    "# Test utility functions\n",
    "print(\"\\nüß™ Testing Utility Functions:\\n\")\n",
    "\n",
    "print(\"  month_trunc():\")\n",
    "print(f\"    '1996-01-15' ‚Üí '{month_trunc('1996-01-15')}'\")\n",
    "print(f\"    '1995-12-31' ‚Üí '{month_trunc('1995-12-31')}'\")\n",
    "\n",
    "print(\"\\n  format_answer():\")\n",
    "print(f\"    1234 ‚Üí '{format_answer(1234)}'\")\n",
    "print(f\"    0    ‚Üí '{format_answer(0)}'\")\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 2 Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ TPC-H Helpers & Parsers Ready:\")\n",
    "print(\"\\n  üìã Text Parsers (8 tables):\")\n",
    "print(\"     - parse_lineitem(line) ‚Üí 16-field tuple\")\n",
    "print(\"     - parse_orders(line) ‚Üí 9-field tuple\")\n",
    "print(\"     - parse_part(line) ‚Üí 9-field tuple\")\n",
    "print(\"     - parse_supplier(line) ‚Üí 7-field tuple\")\n",
    "print(\"     - parse_customer(line) ‚Üí 8-field tuple\")\n",
    "print(\"     - parse_nation(line) ‚Üí 4-field tuple\")\n",
    "print(\"     - parse_region(line) ‚Üí 3-field tuple\")\n",
    "print(\"     - parse_partsupp(line) ‚Üí 5-field tuple\")\n",
    "\n",
    "print(\"\\n  üì¶ Parquet Loaders:\")\n",
    "print(\"     - load_parquet_as_rdd(spark, table, path)\")\n",
    "\n",
    "print(\"\\n  üì° Broadcast Helper:\")\n",
    "print(\"     - broadcast_dimension(spark, rdd, key_extractor)\")\n",
    "\n",
    "print(\"\\n  üîß Utilities:\")\n",
    "print(\"     - save_tuples(path, iterator, header)\")\n",
    "print(\"     - month_trunc(date_str) ‚Üí YYYY-MM\")\n",
    "print(\"     - format_answer(count) ‚Üí ANSWER=<count>\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  - Load TPC-H tables (TXT or Parquet)\")\n",
    "print(\"  - Implement A1-A7 queries using RDD operations only\")\n",
    "print(\"  - Compare TXT vs Parquet performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c21bf",
   "metadata": {},
   "source": [
    "## Part A ‚Äî Relational (RDD‚Äëonly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a47e9",
   "metadata": {},
   "source": [
    "### A1 ‚Äî Q1: shipped items on DATE (print ANSWER=\\d+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de86dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 3 ‚Äî Part A: Task A1 - Shipped Items Count\n",
      "============================================================\n",
      "\n",
      "üéØ Running A1 in Notebook Mode (both TXT and Parquet)\n",
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "Running A1 with TXT format\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A1 ‚Äî Shipped Items Count on 1996-01-01\n",
      "Format: TXT\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading TXT from: data/tpch/TPC-H-0.1-TXT/lineitem.tbl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded TXT: 600,572 lines\n",
      "  ‚úÖ Parsed to tuples: 3 partitions\n",
      "\n",
      "üîç Filtering by l_shipdate = '1996-01-01'...\n",
      "üìä Counting filtered records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A1 Result:\n",
      "============================================================\n",
      "\n",
      "  ANSWER=266\n",
      "\n",
      "  Format      : TXT\n",
      "  Date        : 1996-01-01\n",
      "  Duration    : 13.07 seconds\n",
      "============================================================\n",
      "\n",
      "üíæ Saved output: outputs/a1_result_txt_19960101.txt\n",
      "\n",
      "\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "Running A1 with Parquet format\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A1 ‚Äî Shipped Items Count on 1996-01-01\n",
      "Format: Parquet\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading Parquet from: data/tpch/TPC-H-0.1-PARQUET/lineitem\n",
      "  ‚úÖ Loaded Parquet: 600,572 rows\n",
      "  üìÑ Saved plan: proof/plan_a1_parquet_load_19960101.txt\n",
      "  ‚úÖ Converted to RDD: 7 partitions\n",
      "\n",
      "üîç Filtering by l_shipdate = '1996-01-01'...\n",
      "üìä Counting filtered records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=========================================>                (5 + 2) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A1 Result:\n",
      "============================================================\n",
      "\n",
      "  ANSWER=266\n",
      "\n",
      "  Format      : Parquet\n",
      "  Date        : 1996-01-01\n",
      "  Duration    : 12.52 seconds\n",
      "============================================================\n",
      "\n",
      "üíæ Saved output: outputs/a1_result_parquet_19960101.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A1 ‚Äî TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "\n",
      "| Metric              | TXT              | Parquet          | Speedup     |\n",
      "|---------------------|------------------|------------------|-------------|\n",
      "| Count               | 266           | 266           | N/A         |\n",
      "| Duration            | 13.07s            | 12.52s            | 1.04x        |\n",
      "| Partitions          | 3               | 7               | N/A         |\n",
      "\n",
      "‚úÖ Validation: Both formats return same count (266)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Updating lab_metrics_log.csv...\n",
      "  ‚úÖ Updated proof/lab_metrics_log.csv\n",
      "\n",
      "\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "IMPORTANT: Capture Spark UI Screenshots\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "\n",
      "üéØ Required Screenshots for A1:\n",
      "\n",
      "  1. Jobs Tab:\n",
      "     - A1-TXT job: Input size, duration\n",
      "     - A1-Parquet job: Input size, duration\n",
      "     ‚Üí Save as: proof/screenshots/a1_jobs_comparison.png\n",
      "\n",
      "  2. SQL Tab (Parquet only):\n",
      "     - Physical plan showing FileScan parquet\n",
      "     - Predicate pushdown (if visible)\n",
      "     ‚Üí Save as: proof/screenshots/a1_parquet_sql_plan.png\n",
      "\n",
      "  3. Stages Tab:\n",
      "     - Task metrics (input records, shuffle)\n",
      "     ‚Üí Save as: proof/screenshots/a1_stages_metrics.png\n",
      "\n",
      "üìù Update lab_metrics_log.csv with:\n",
      "   - input_size_mb (from Spark UI ‚Üí Jobs ‚Üí Input)\n",
      "   - Any shuffle metrics (should be 0 for A1)\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "Section 3 Summary ‚Äî Task A1 Complete\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "üìã Deliverables:\n",
      "  ‚úÖ Output TXT    : outputs/a1_result_txt_19960101.txt\n",
      "  ‚úÖ Output Parquet: outputs/a1_result_parquet_19960101.txt\n",
      "  ‚úÖ Plan Parquet  : proof/plan_a1_parquet_load_19960101.txt\n",
      "  ‚úÖ Metrics Log   : proof/lab_metrics_log.csv\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. Capture Spark UI screenshots (Jobs, SQL, Stages)\n",
      "  2. Update lab_metrics_log.csv with Input Size from UI\n",
      "  3. Proceed to A2 (cogroup join orders+lineitem)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# args: --input, --date, --text/--parquet\n",
    "# pipeline (text): read lineitem -> filter by l_shipdate -> count -> print('ANSWER=', n)\n",
    "# parquet path variant: spark.read.parquet(...).rdd\n",
    "# ============================================================\n",
    "# Section 3 - Part A: Task A1 - Shipped Items Count on Date\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 3 ‚Äî Part A: Task A1 - Shipped Items Count\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ========== A1 IMPLEMENTATION ==========\n",
    "\n",
    "def run_a1_shipped_items_count(input_path, target_date, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A1 (Q1) ‚Äî Count(*) on lineitem for a given l_shipdate = YYYY-MM-DD\n",
    "    \n",
    "    Args:\n",
    "        input_path: Base path to TPC-H data (TXT or Parquet)\n",
    "        target_date: Ship date to filter (YYYY-MM-DD format)\n",
    "        use_parquet: True to use Parquet, False for TXT\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (count, duration_seconds, metrics_dict)\n",
    "    \n",
    "    Output: Print ANSWER=<count>\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A1 ‚Äî Shipped Items Count on {target_date}\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========== LOAD DATA ==========\n",
    "    \n",
    "    if use_parquet:\n",
    "        # ‚úÖ PERMIS : Load Parquet with DataFrame, then .rdd\n",
    "        lineitem_path = os.path.join(input_path, \"lineitem\")\n",
    "        \n",
    "        print(f\"üìÇ Loading Parquet from: {lineitem_path}\")\n",
    "        \n",
    "        # Load Parquet as DataFrame\n",
    "        lineitem_df = spark.read.parquet(lineitem_path)\n",
    "        \n",
    "        print(f\"  ‚úÖ Loaded Parquet: {lineitem_df.count():,} rows\")\n",
    "        \n",
    "        # Save explain plan (before conversion to RDD)\n",
    "        plan_path = f\"proof/plan_a1_parquet_load_{target_date.replace('-', '')}.txt\"\n",
    "        os.makedirs(\"proof\", exist_ok=True)\n",
    "        \n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A1 Parquet Load Plan ({target_date}) ===\\n\\n\")\n",
    "            f.write(lineitem_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        \n",
    "        print(f\"  üìÑ Saved plan: {plan_path}\")\n",
    "        \n",
    "        # Convert to RDD (PERMIS par assignment)\n",
    "        lineitem_rdd = lineitem_df.rdd\n",
    "        \n",
    "        print(f\"  ‚úÖ Converted to RDD: {lineitem_rdd.getNumPartitions()} partitions\\n\")\n",
    "        \n",
    "    else:\n",
    "        # ‚úÖ TEXT: Load TXT with RDD, parse manually\n",
    "        lineitem_path = os.path.join(input_path, \"lineitem.tbl\")\n",
    "        \n",
    "        print(f\"üìÇ Loading TXT from: {lineitem_path}\")\n",
    "        \n",
    "        # Load text file as RDD\n",
    "        lineitem_rdd = sc.textFile(lineitem_path)\n",
    "        \n",
    "        print(f\"  ‚úÖ Loaded TXT: {lineitem_rdd.count():,} lines\")\n",
    "        \n",
    "        # Parse pipe-delimited lines\n",
    "        lineitem_rdd = lineitem_rdd.map(parse_lineitem)\n",
    "        \n",
    "        print(f\"  ‚úÖ Parsed to tuples: {lineitem_rdd.getNumPartitions()} partitions\\n\")\n",
    "    \n",
    "    # ========== FILTER BY SHIPDATE ==========\n",
    "    \n",
    "    print(f\"üîç Filtering by l_shipdate = '{target_date}'...\")\n",
    "    \n",
    "    if use_parquet:\n",
    "        # For Parquet RDD: rows are Row objects with named fields\n",
    "        filtered_rdd = lineitem_rdd.filter(lambda row: row.l_shipdate == target_date)\n",
    "    else:\n",
    "        # For TXT RDD: tuples with positional access\n",
    "        # l_shipdate is at index 10 (see parse_lineitem)\n",
    "        filtered_rdd = lineitem_rdd.filter(lambda t: t[10] == target_date)\n",
    "    \n",
    "    # ========== COUNT ==========\n",
    "    \n",
    "    print(f\"üìä Counting filtered records...\")\n",
    "    \n",
    "    count = filtered_rdd.count()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # ========== OUTPUT ==========\n",
    "    \n",
    "    # Required format: ANSWER=<count>\n",
    "    answer = format_answer(count)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ A1 Result:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\n  {answer}\\n\")\n",
    "    print(f\"  Format      : {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"  Date        : {target_date}\")\n",
    "    print(f\"  Duration    : {duration:.2f} seconds\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ========== SAVE OUTPUT ==========\n",
    "    \n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a1_result_{format_suffix}_{target_date.replace('-', '')}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A1 - Shipped Items Count\\n\")\n",
    "        f.write(f\"Date: {target_date}\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\")\n",
    "        f.write(f\"\\n{answer}\\n\")\n",
    "    \n",
    "    print(f\"üíæ Saved output: {output_path}\\n\")\n",
    "    \n",
    "    # ========== COLLECT METRICS ==========\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A1',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'date': target_date,\n",
    "        'count': count,\n",
    "        'duration_s': round(duration, 2),\n",
    "        'num_partitions': lineitem_rdd.getNumPartitions()\n",
    "    }\n",
    "    \n",
    "    return count, duration, metrics\n",
    "\n",
    "# ========== CLI ARGUMENT PARSER (for spark-submit) ==========\n",
    "\n",
    "def parse_args_a1():\n",
    "    \"\"\"\n",
    "    Parse command-line arguments for A1\n",
    "    \n",
    "    Example:\n",
    "        spark-submit script.py --input data/tpch/TPC-H-0.1-TXT --date 1996-01-01 --text\n",
    "        spark-submit script.py --input data/tpch/TPC-H-0.1-PARQUET --date 1996-01-01 --parquet\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='A1 - Shipped Items Count')\n",
    "    \n",
    "    parser.add_argument('--input', required=True, \n",
    "                        help='Base path to TPC-H data (TXT or Parquet)')\n",
    "    \n",
    "    parser.add_argument('--date', required=True, \n",
    "                        help='Ship date to filter (YYYY-MM-DD)')\n",
    "    \n",
    "    # Mutually exclusive group for format\n",
    "    format_group = parser.add_mutually_exclusive_group(required=True)\n",
    "    format_group.add_argument('--text', action='store_true', \n",
    "                              help='Use TXT format')\n",
    "    format_group.add_argument('--parquet', action='store_true', \n",
    "                              help='Use Parquet format')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "# ========== NOTEBOOK EXECUTION (interactive mode) ==========\n",
    "\n",
    "print(\"\\nüéØ Running A1 in Notebook Mode (both TXT and Parquet)\\n\")\n",
    "\n",
    "# Configuration\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "TARGET_DATE = \"1996-01-01\"  # Default test date\n",
    "\n",
    "# ========== RUN A1 WITH TXT ==========\n",
    "\n",
    "print(\"\\n\" + \"üî∑\" * 30)\n",
    "print(\"Running A1 with TXT format\")\n",
    "print(\"üî∑\" * 30 + \"\\n\")\n",
    "\n",
    "count_txt, duration_txt, metrics_txt = run_a1_shipped_items_count(\n",
    "    input_path=INPUT_TXT_PATH,\n",
    "    target_date=TARGET_DATE,\n",
    "    use_parquet=False\n",
    ")\n",
    "\n",
    "# ========== RUN A1 WITH PARQUET ==========\n",
    "\n",
    "print(\"\\n\" + \"üî∂\" * 30)\n",
    "print(\"Running A1 with Parquet format\")\n",
    "print(\"üî∂\" * 30 + \"\\n\")\n",
    "\n",
    "count_parquet, duration_parquet, metrics_parquet = run_a1_shipped_items_count(\n",
    "    input_path=INPUT_PARQUET_PATH,\n",
    "    target_date=TARGET_DATE,\n",
    "    use_parquet=True\n",
    ")\n",
    "\n",
    "# ========== COMPARISON ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"A1 ‚Äî TXT vs Parquet Comparison\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "comparison_table = f\"\"\"\n",
    "| Metric              | TXT              | Parquet          | Speedup     |\n",
    "|---------------------|------------------|------------------|-------------|\n",
    "| Count               | {count_txt:,}           | {count_parquet:,}           | N/A         |\n",
    "| Duration            | {duration_txt:.2f}s            | {duration_parquet:.2f}s            | {duration_txt/duration_parquet:.2f}x        |\n",
    "| Partitions          | {metrics_txt['num_partitions']}               | {metrics_parquet['num_partitions']}               | N/A         |\n",
    "\"\"\"\n",
    "\n",
    "print(comparison_table)\n",
    "\n",
    "# Validation\n",
    "if count_txt != count_parquet:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Counts differ! TXT={count_txt}, Parquet={count_parquet}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Validation: Both formats return same count ({count_txt:,})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# ========== UPDATE METRICS LOG ==========\n",
    "\n",
    "print(\"üìä Updating lab_metrics_log.csv...\")\n",
    "\n",
    "# Read existing metrics or create new\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "os.makedirs(\"proof\", exist_ok=True)\n",
    "\n",
    "import csv\n",
    "\n",
    "# Check if file exists\n",
    "file_exists = os.path.exists(metrics_log_path)\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write header if new file\n",
    "    if not file_exists:\n",
    "        writer.writerow([\n",
    "            'run_id', 'task', 'format', 'date', 'num_files', 'input_size_mb',\n",
    "            'duration_s', 'shuffle_read_mb', 'shuffle_write_mb', 'notes', 'timestamp'\n",
    "        ])\n",
    "    \n",
    "    # Get next run_id\n",
    "    if file_exists:\n",
    "        with open(metrics_log_path, 'r') as rf:\n",
    "            lines = list(csv.reader(rf))\n",
    "            run_id = len(lines)  # Header + existing rows\n",
    "    else:\n",
    "        run_id = 1\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    # Add TXT entry\n",
    "    writer.writerow([\n",
    "        run_id,\n",
    "        'A1',\n",
    "        'TXT',\n",
    "        TARGET_DATE,\n",
    "        1,  # num_files (lineitem.tbl)\n",
    "        'N/A',  # To be filled from Spark UI\n",
    "        duration_txt,\n",
    "        0,  # No shuffle for simple count\n",
    "        0,\n",
    "        f'count(*) on lineitem where l_shipdate={TARGET_DATE}',\n",
    "        timestamp\n",
    "    ])\n",
    "    \n",
    "    # Add Parquet entry\n",
    "    writer.writerow([\n",
    "        run_id + 1,\n",
    "        'A1',\n",
    "        'Parquet',\n",
    "        TARGET_DATE,\n",
    "        'N/A',  # Multiple parquet files\n",
    "        'N/A',  # To be filled from Spark UI\n",
    "        duration_parquet,\n",
    "        0,\n",
    "        0,\n",
    "        f'same query with Parquet',\n",
    "        timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"  ‚úÖ Updated {metrics_log_path}\\n\")\n",
    "\n",
    "# ========== SPARK UI REMINDER ==========\n",
    "\n",
    "print(\"\\n\" + \"üì∏\" * 30)\n",
    "print(\"IMPORTANT: Capture Spark UI Screenshots\")\n",
    "print(\"üì∏\" * 30 + \"\\n\")\n",
    "\n",
    "print(\"üéØ Required Screenshots for A1:\\n\")\n",
    "print(\"  1. Jobs Tab:\")\n",
    "print(\"     - A1-TXT job: Input size, duration\")\n",
    "print(\"     - A1-Parquet job: Input size, duration\")\n",
    "print(\"     ‚Üí Save as: proof/screenshots/a1_jobs_comparison.png\\n\")\n",
    "\n",
    "print(\"  2. SQL Tab (Parquet only):\")\n",
    "print(\"     - Physical plan showing FileScan parquet\")\n",
    "print(\"     - Predicate pushdown (if visible)\")\n",
    "print(\"     ‚Üí Save as: proof/screenshots/a1_parquet_sql_plan.png\\n\")\n",
    "\n",
    "print(\"  3. Stages Tab:\")\n",
    "print(\"     - Task metrics (input records, shuffle)\")\n",
    "print(\"     ‚Üí Save as: proof/screenshots/a1_stages_metrics.png\\n\")\n",
    "\n",
    "print(\"üìù Update lab_metrics_log.csv with:\")\n",
    "print(\"   - input_size_mb (from Spark UI ‚Üí Jobs ‚Üí Input)\")\n",
    "print(\"   - Any shuffle metrics (should be 0 for A1)\\n\")\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "\n",
    "print(\"\\n\" + \"‚úÖ\" * 30)\n",
    "print(\"Section 3 Summary ‚Äî Task A1 Complete\")\n",
    "print(\"‚úÖ\" * 30 + \"\\n\")\n",
    "\n",
    "print(\"üìã Deliverables:\")\n",
    "print(f\"  ‚úÖ Output TXT    : outputs/a1_result_txt_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Output Parquet: outputs/a1_result_parquet_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Plan Parquet  : proof/plan_a1_parquet_load_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Metrics Log   : proof/lab_metrics_log.csv\\n\")\n",
    "\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28e5ce",
   "metadata": {},
   "source": [
    "### A2 ‚Äî Q2: clerks by order key (reduce‚Äëside join via cogroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004259bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 4 ‚Äî Part A: Task A2 - Clerks by Order Key\n",
      "============================================================\n",
      "\n",
      "üéØ Running A2 in Notebook Mode (both TXT and Parquet)\n",
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "Running A2 with TXT format\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A2 ‚Äî Clerks by Order Key on 1996-01-01\n",
      "Format: TXT\n",
      "Strategy: Reduce-side join via cogroup\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading TXT tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded lineitem.tbl: 600,572 rows, 3 partitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded orders.tbl: 150,000 rows, 2 partitions\n",
      "\n",
      "üîß Step 1: Building key-value pairs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Lineitem filtered by shipdate '1996-01-01': 266 records\n",
      "     Format: (l_orderkey, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Orders keyed by orderkey: 150,000 records\n",
      "     Format: (o_orderkey, o_clerk)\n",
      "\n",
      "üîÄ Step 2: Performing reduce-side join via cogroup...\n",
      "\n",
      "  ‚úÖ Cogroup completed\n",
      "     Result format: (orderkey, (lineitem_values, orders_values))\n",
      "\n",
      "üì§ Step 3: Expanding cogroup results...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Join expanded: 262 (clerk, orderkey) pairs\n",
      "     Format: (o_clerk, o_orderkey)\n",
      "\n",
      "üî¢ Step 4: Sorting and taking top 20...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A2 Results (First 20):\n",
      "============================================================\n",
      "\n",
      "  Format      : TXT\n",
      "  Date        : 1996-01-01\n",
      "  Duration    : 18.07 seconds\n",
      "  Total Pairs : 262\n",
      "\n",
      "  Top 20 (o_clerk, o_orderkey):\n",
      "\n",
      "     1. Clerk#000000005      | Order: 473858\n",
      "     2. Clerk#000000007      | Order: 591104\n",
      "     3. Clerk#000000007      | Order: 312610\n",
      "     4. Clerk#000000021      | Order: 58247\n",
      "     5. Clerk#000000027      | Order: 573478\n",
      "     6. Clerk#000000030      | Order: 343043\n",
      "     7. Clerk#000000033      | Order: 480613\n",
      "     8. Clerk#000000033      | Order: 307783\n",
      "     9. Clerk#000000043      | Order: 235043\n",
      "    10. Clerk#000000050      | Order: 18372\n",
      "    11. Clerk#000000050      | Order: 438982\n",
      "    12. Clerk#000000050      | Order: 237415\n",
      "    13. Clerk#000000059      | Order: 138789\n",
      "    14. Clerk#000000059      | Order: 161221\n",
      "    15. Clerk#000000063      | Order: 473351\n",
      "    16. Clerk#000000081      | Order: 501218\n",
      "    17. Clerk#000000082      | Order: 221441\n",
      "    18. Clerk#000000087      | Order: 151330\n",
      "    19. Clerk#000000089      | Order: 56484\n",
      "    20. Clerk#000000092      | Order: 213511\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved output: outputs/a2_result_txt_19960101.txt\n",
      "\n",
      "\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "Running A2 with Parquet format\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A2 ‚Äî Clerks by Order Key on 1996-01-01\n",
      "Format: Parquet\n",
      "Strategy: Reduce-side join via cogroup\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading Parquet tables...\n",
      "\n",
      "  ‚úÖ Lineitem plan saved: proof/plan_a2_lineitem_parquet_19960101.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded lineitem: 600,572 rows, 7 partitions\n",
      "  ‚úÖ Orders plan saved: proof/plan_a2_orders_parquet_19960101.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded orders: 150,000 rows, 4 partitions\n",
      "\n",
      "üîß Step 1: Building key-value pairs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Lineitem filtered by shipdate '1996-01-01': 266 records\n",
      "     Format: (l_orderkey, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Orders keyed by orderkey: 150,000 records\n",
      "     Format: (o_orderkey, o_clerk)\n",
      "\n",
      "üîÄ Step 2: Performing reduce-side join via cogroup...\n",
      "\n",
      "  ‚úÖ Cogroup completed\n",
      "     Result format: (orderkey, (lineitem_values, orders_values))\n",
      "\n",
      "üì§ Step 3: Expanding cogroup results...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Join expanded: 262 (clerk, orderkey) pairs\n",
      "     Format: (o_clerk, o_orderkey)\n",
      "\n",
      "üî¢ Step 4: Sorting and taking top 20...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A2 Results (First 20):\n",
      "============================================================\n",
      "\n",
      "  Format      : Parquet\n",
      "  Date        : 1996-01-01\n",
      "  Duration    : 32.22 seconds\n",
      "  Total Pairs : 262\n",
      "\n",
      "  Top 20 (o_clerk, o_orderkey):\n",
      "\n",
      "     1. Clerk#000000005      | Order: 473858\n",
      "     2. Clerk#000000007      | Order: 591104\n",
      "     3. Clerk#000000007      | Order: 312610\n",
      "     4. Clerk#000000021      | Order: 58247\n",
      "     5. Clerk#000000027      | Order: 573478\n",
      "     6. Clerk#000000030      | Order: 343043\n",
      "     7. Clerk#000000033      | Order: 480613\n",
      "     8. Clerk#000000033      | Order: 307783\n",
      "     9. Clerk#000000043      | Order: 235043\n",
      "    10. Clerk#000000050      | Order: 18372\n",
      "    11. Clerk#000000050      | Order: 438982\n",
      "    12. Clerk#000000050      | Order: 237415\n",
      "    13. Clerk#000000059      | Order: 138789\n",
      "    14. Clerk#000000059      | Order: 161221\n",
      "    15. Clerk#000000063      | Order: 473351\n",
      "    16. Clerk#000000081      | Order: 501218\n",
      "    17. Clerk#000000082      | Order: 221441\n",
      "    18. Clerk#000000087      | Order: 151330\n",
      "    19. Clerk#000000089      | Order: 56484\n",
      "    20. Clerk#000000092      | Order: 213511\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved output: outputs/a2_result_parquet_19960101.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A2 ‚Äî TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "\n",
      "| Metric              | TXT                  | Parquet              | Speedup     |\n",
      "|---------------------|----------------------|----------------------|-------------|\n",
      "| Total Pairs         | 262              | 262              | N/A         |\n",
      "| Duration            | 18.07s                | 32.22s                | 0.56x        |\n",
      "| Partitions Lineitem | 3                    | 7                    | N/A         |\n",
      "| Partitions Orders   | 2                    | 4                    | N/A         |\n",
      "\n",
      "‚úÖ Validation: Both formats return identical top 20 results\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Updating lab_metrics_log.csv...\n",
      "  ‚úÖ Updated proof/lab_metrics_log.csv\n",
      "\n",
      "\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "IMPORTANT: Capture Spark UI Screenshots for A2\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "\n",
      "üéØ Required Screenshots:\n",
      "\n",
      "  1. Jobs Tab:\n",
      "     - A2-TXT cogroup job\n",
      "     - A2-Parquet cogroup job\n",
      "     ‚Üí Save as: proof/screenshots/a2_jobs_cogroup.png\n",
      "\n",
      "  2. Stages Tab (CRITICAL for cogroup):\n",
      "     - Look for 'cogroup' stage\n",
      "     - Capture Shuffle Read/Write metrics\n",
      "     - Note: cogroup causes shuffle on BOTH sides\n",
      "     ‚Üí Save as: proof/screenshots/a2_cogroup_shuffle.png\n",
      "\n",
      "  3. SQL Tab (Parquet only):\n",
      "     - Physical plans for lineitem and orders loads\n",
      "     ‚Üí Save as: proof/screenshots/a2_parquet_plans.png\n",
      "\n",
      "üìù Update lab_metrics_log.csv with:\n",
      "   - Shuffle Read (MB) from Stages tab\n",
      "   - Shuffle Write (MB) from Stages tab\n",
      "   - Input Size for both lineitem and orders\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "Section 4 Summary ‚Äî Task A2 Complete\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "üìã Deliverables:\n",
      "  ‚úÖ Output TXT    : outputs/a2_result_txt_19960101.txt\n",
      "  ‚úÖ Output Parquet: outputs/a2_result_parquet_19960101.txt\n",
      "  ‚úÖ Plans Parquet : proof/plan_a2_lineitem_parquet_*.txt\n",
      "                     proof/plan_a2_orders_parquet_*.txt\n",
      "  ‚úÖ Metrics Log   : proof/lab_metrics_log.csv\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. Capture Spark UI screenshots (Jobs, Stages with shuffle)\n",
      "  2. Update lab_metrics_log.csv with shuffle metrics\n",
      "  3. Proceed to A3 (broadcast hash join)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# build (orderkey, clerk) from orders and (orderkey, 1) from lineitem(date)\n",
    "# cogroup -> expand -> sortByKey -> take(20)\n",
    "# ============================================================\n",
    "# Section 4 - Part A: Task A2 - Clerks by Order Key\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 4 ‚Äî Part A: Task A2 - Clerks by Order Key\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "\n",
    "# ========== A2 IMPLEMENTATION ==========\n",
    "\n",
    "def run_a2_clerks_by_orderkey(input_path, target_date, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A2 (Q2) ‚Äî First 20 (o_clerk, o_orderkey) where l_orderkey=o_orderkey \n",
    "              and l_shipdate=DATE\n",
    "    \n",
    "    Strategy: Reduce-side join via cogroup\n",
    "    \n",
    "    Args:\n",
    "        input_path: Base path to TPC-H data (TXT or Parquet)\n",
    "        target_date: Ship date to filter (YYYY-MM-DD format)\n",
    "        use_parquet: True to use Parquet, False for TXT\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results_list, duration_seconds, metrics_dict)\n",
    "    \n",
    "    Output Format: First 20 (o_clerk, o_orderkey) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A2 ‚Äî Clerks by Order Key on {target_date}\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"Strategy: Reduce-side join via cogroup\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========== LOAD DATA ==========\n",
    "    \n",
    "    if use_parquet:\n",
    "        print(f\"üìÇ Loading Parquet tables...\\n\")\n",
    "        \n",
    "        # Load lineitem\n",
    "        lineitem_path = os.path.join(input_path, \"lineitem\")\n",
    "        lineitem_df = spark.read.parquet(lineitem_path)\n",
    "        \n",
    "        # Save explain plan for lineitem\n",
    "        plan_path = f\"proof/plan_a2_lineitem_parquet_{target_date.replace('-', '')}.txt\"\n",
    "        os.makedirs(\"proof\", exist_ok=True)\n",
    "        \n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A2 Lineitem Parquet Load Plan ({target_date}) ===\\n\\n\")\n",
    "            f.write(lineitem_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        \n",
    "        print(f\"  ‚úÖ Lineitem plan saved: {plan_path}\")\n",
    "        \n",
    "        # Convert to RDD\n",
    "        lineitem_rdd = lineitem_df.rdd\n",
    "        print(f\"  ‚úÖ Loaded lineitem: {lineitem_rdd.count():,} rows, {lineitem_rdd.getNumPartitions()} partitions\")\n",
    "        \n",
    "        # Load orders\n",
    "        orders_path = os.path.join(input_path, \"orders\")\n",
    "        orders_df = spark.read.parquet(orders_path)\n",
    "        \n",
    "        # Save explain plan for orders\n",
    "        plan_path = f\"proof/plan_a2_orders_parquet_{target_date.replace('-', '')}.txt\"\n",
    "        \n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A2 Orders Parquet Load Plan ({target_date}) ===\\n\\n\")\n",
    "            f.write(orders_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        \n",
    "        print(f\"  ‚úÖ Orders plan saved: {plan_path}\\n\")\n",
    "        \n",
    "        # Convert to RDD\n",
    "        orders_rdd = orders_df.rdd\n",
    "        print(f\"  ‚úÖ Loaded orders: {orders_rdd.count():,} rows, {orders_rdd.getNumPartitions()} partitions\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"üìÇ Loading TXT tables...\\n\")\n",
    "        \n",
    "        # Load lineitem.tbl\n",
    "        lineitem_path = os.path.join(input_path, \"lineitem.tbl\")\n",
    "        lineitem_rdd = sc.textFile(lineitem_path).map(parse_lineitem)\n",
    "        print(f\"  ‚úÖ Loaded lineitem.tbl: {lineitem_rdd.count():,} rows, {lineitem_rdd.getNumPartitions()} partitions\")\n",
    "        \n",
    "        # Load orders.tbl\n",
    "        orders_path = os.path.join(input_path, \"orders.tbl\")\n",
    "        orders_rdd = sc.textFile(orders_path).map(parse_orders)\n",
    "        print(f\"  ‚úÖ Loaded orders.tbl: {orders_rdd.count():,} rows, {orders_rdd.getNumPartitions()} partitions\\n\")\n",
    "    \n",
    "    # ========== STEP 1: BUILD KEY-VALUE PAIRS ==========\n",
    "    \n",
    "    print(f\"üîß Step 1: Building key-value pairs...\\n\")\n",
    "    \n",
    "    # From lineitem: filter by shipdate, then emit (l_orderkey, 1)\n",
    "    \n",
    "    if use_parquet:\n",
    "        # Parquet RDD: Row objects with named fields\n",
    "        lineitem_filtered = lineitem_rdd \\\n",
    "            .filter(lambda row: row.l_shipdate == target_date) \\\n",
    "            .map(lambda row: (row.l_orderkey, 1))\n",
    "    else:\n",
    "        # TXT RDD: tuples with positional access\n",
    "        # l_orderkey at index 0, l_shipdate at index 10\n",
    "        lineitem_filtered = lineitem_rdd \\\n",
    "            .filter(lambda t: t[10] == target_date) \\\n",
    "            .map(lambda t: (t[0], 1))\n",
    "    \n",
    "    lineitem_count = lineitem_filtered.count()\n",
    "    print(f\"  ‚úÖ Lineitem filtered by shipdate '{target_date}': {lineitem_count:,} records\")\n",
    "    print(f\"     Format: (l_orderkey, 1)\\n\")\n",
    "    \n",
    "    # From orders: emit (o_orderkey, o_clerk)\n",
    "    \n",
    "    if use_parquet:\n",
    "        # Parquet: Row objects\n",
    "        orders_keyed = orders_rdd.map(lambda row: (row.o_orderkey, row.o_clerk))\n",
    "    else:\n",
    "        # TXT: tuples\n",
    "        # o_orderkey at index 0, o_clerk at index 6\n",
    "        orders_keyed = orders_rdd.map(lambda t: (t[0], t[6]))\n",
    "    \n",
    "    orders_count = orders_keyed.count()\n",
    "    print(f\"  ‚úÖ Orders keyed by orderkey: {orders_count:,} records\")\n",
    "    print(f\"     Format: (o_orderkey, o_clerk)\\n\")\n",
    "    \n",
    "    # ========== STEP 2: COGROUP (REDUCE-SIDE JOIN) ==========\n",
    "    \n",
    "    print(f\"üîÄ Step 2: Performing reduce-side join via cogroup...\\n\")\n",
    "    \n",
    "    cogrouped = lineitem_filtered.cogroup(orders_keyed)\n",
    "    \n",
    "    print(f\"  ‚úÖ Cogroup completed\")\n",
    "    print(f\"     Result format: (orderkey, (lineitem_values, orders_values))\\n\")\n",
    "    \n",
    "    # ========== STEP 3: EXPAND COGROUP RESULTS ==========\n",
    "    \n",
    "    print(f\"üì§ Step 3: Expanding cogroup results...\\n\")\n",
    "    \n",
    "    def expand_cogroup(item):\n",
    "        \"\"\"\n",
    "        Expand cogroup result to list of (clerk, orderkey) tuples\n",
    "        \n",
    "        Args:\n",
    "            item: (orderkey, (lineitem_iter, orders_iter))\n",
    "        \n",
    "        Returns:\n",
    "            List of (o_clerk, o_orderkey) tuples\n",
    "        \"\"\"\n",
    "        orderkey, (lineitem_vals, orders_vals) = item\n",
    "        \n",
    "        # Convert iterables to lists\n",
    "        lineitem_list = list(lineitem_vals)\n",
    "        orders_list = list(orders_vals)\n",
    "        \n",
    "        # Inner join: only if both sides have values\n",
    "        if len(lineitem_list) > 0 and len(orders_list) > 0:\n",
    "            # Emit (clerk, orderkey) for each clerk\n",
    "            return [(clerk, orderkey) for clerk in orders_list]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    # Apply expansion and flatten\n",
    "    joined_rdd = cogrouped.flatMap(expand_cogroup)\n",
    "    \n",
    "    joined_count = joined_rdd.count()\n",
    "    print(f\"  ‚úÖ Join expanded: {joined_count:,} (clerk, orderkey) pairs\")\n",
    "    print(f\"     Format: (o_clerk, o_orderkey)\\n\")\n",
    "    \n",
    "    # ========== STEP 4: SORT AND TAKE TOP 20 ==========\n",
    "    \n",
    "    print(f\"üî¢ Step 4: Sorting and taking top 20...\\n\")\n",
    "    \n",
    "    # sortByKey sorts by first element of tuple (o_clerk)\n",
    "    sorted_rdd = joined_rdd.sortByKey(ascending=True)\n",
    "    \n",
    "    # Take first 20\n",
    "    top_20 = sorted_rdd.take(20)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # ========== OUTPUT ==========\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ A2 Results (First 20):\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"  Format      : {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"  Date        : {target_date}\")\n",
    "    print(f\"  Duration    : {duration:.2f} seconds\")\n",
    "    print(f\"  Total Pairs : {joined_count:,}\")\n",
    "    print(f\"\\n  Top 20 (o_clerk, o_orderkey):\\n\")\n",
    "    \n",
    "    for i, (clerk, orderkey) in enumerate(top_20, 1):\n",
    "        print(f\"    {i:2d}. {clerk:<20} | Order: {orderkey}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    # ========== SAVE OUTPUT ==========\n",
    "    \n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a2_result_{format_suffix}_{target_date.replace('-', '')}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A2 - Clerks by Order Key (Reduce-side Join)\\n\")\n",
    "        f.write(f\"Date: {target_date}\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\")\n",
    "        f.write(f\"Total Pairs: {joined_count:,}\\n\")\n",
    "        f.write(f\"\\nFirst 20 (o_clerk, o_orderkey):\\n\\n\")\n",
    "        \n",
    "        for clerk, orderkey in top_20:\n",
    "            f.write(f\"{clerk},{orderkey}\\n\")\n",
    "    \n",
    "    print(f\"üíæ Saved output: {output_path}\\n\")\n",
    "    \n",
    "    # ========== COLLECT METRICS ==========\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A2',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'date': target_date,\n",
    "        'total_pairs': joined_count,\n",
    "        'duration_s': round(duration, 2),\n",
    "        'num_partitions_lineitem': lineitem_filtered.getNumPartitions(),\n",
    "        'num_partitions_orders': orders_keyed.getNumPartitions()\n",
    "    }\n",
    "    \n",
    "    return top_20, duration, metrics\n",
    "\n",
    "# ========== NOTEBOOK EXECUTION ==========\n",
    "\n",
    "print(\"\\nüéØ Running A2 in Notebook Mode (both TXT and Parquet)\\n\")\n",
    "\n",
    "# ========== DEFINE PATHS (FIX FOR NameError) ==========\n",
    "\n",
    "# Configuration from Section 1\n",
    "TARGET_DATE = \"1996-01-01\"  # Same date as A1\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "\n",
    "# ========== RUN A2 WITH TXT ==========\n",
    "\n",
    "print(\"\\n\" + \"üî∑\" * 30)\n",
    "print(\"Running A2 with TXT format\")\n",
    "print(\"üî∑\" * 30 + \"\\n\")\n",
    "\n",
    "results_txt, duration_txt, metrics_txt = run_a2_clerks_by_orderkey(\n",
    "    input_path=INPUT_TXT_PATH,\n",
    "    target_date=TARGET_DATE,\n",
    "    use_parquet=False\n",
    ")\n",
    "\n",
    "# ========== RUN A2 WITH PARQUET ==========\n",
    "\n",
    "print(\"\\n\" + \"üî∂\" * 30)\n",
    "print(\"Running A2 with Parquet format\")\n",
    "print(\"üî∂\" * 30 + \"\\n\")\n",
    "\n",
    "results_parquet, duration_parquet, metrics_parquet = run_a2_clerks_by_orderkey(\n",
    "    input_path=INPUT_PARQUET_PATH,\n",
    "    target_date=TARGET_DATE,\n",
    "    use_parquet=True\n",
    ")\n",
    "\n",
    "# ========== COMPARISON ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"A2 ‚Äî TXT vs Parquet Comparison\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "comparison_table = f\"\"\"\n",
    "| Metric              | TXT                  | Parquet              | Speedup     |\n",
    "|---------------------|----------------------|----------------------|-------------|\n",
    "| Total Pairs         | {metrics_txt['total_pairs']:,}              | {metrics_parquet['total_pairs']:,}              | N/A         |\n",
    "| Duration            | {duration_txt:.2f}s                | {duration_parquet:.2f}s                | {duration_txt/duration_parquet:.2f}x        |\n",
    "| Partitions Lineitem | {metrics_txt['num_partitions_lineitem']}                    | {metrics_parquet['num_partitions_lineitem']}                    | N/A         |\n",
    "| Partitions Orders   | {metrics_txt['num_partitions_orders']}                    | {metrics_parquet['num_partitions_orders']}                    | N/A         |\n",
    "\"\"\"\n",
    "\n",
    "print(comparison_table)\n",
    "\n",
    "# Validation: Check if top 20 results match\n",
    "results_match = results_txt == results_parquet\n",
    "\n",
    "if results_match:\n",
    "    print(f\"‚úÖ Validation: Both formats return identical top 20 results\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Results differ between TXT and Parquet!\")\n",
    "    print(f\"\\n  TXT first 5:\")\n",
    "    for i, (clerk, orderkey) in enumerate(results_txt[:5], 1):\n",
    "        print(f\"    {i}. {clerk}, {orderkey}\")\n",
    "    \n",
    "    print(f\"\\n  Parquet first 5:\")\n",
    "    for i, (clerk, orderkey) in enumerate(results_parquet[:5], 1):\n",
    "        print(f\"    {i}. {clerk}, {orderkey}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# ========== UPDATE METRICS LOG ==========\n",
    "\n",
    "print(\"üìä Updating lab_metrics_log.csv...\")\n",
    "\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Get next run_id\n",
    "    with open(metrics_log_path, 'r') as rf:\n",
    "        lines = list(csv.reader(rf))\n",
    "        run_id = len(lines)\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    # Add TXT entry\n",
    "    writer.writerow([\n",
    "        run_id,\n",
    "        'A2',\n",
    "        'TXT',\n",
    "        TARGET_DATE,\n",
    "        2,  # lineitem.tbl + orders.tbl\n",
    "        'N/A',  # To be filled from Spark UI\n",
    "        duration_txt,\n",
    "        'N/A',  # Shuffle Read (cogroup causes shuffle)\n",
    "        'N/A',  # Shuffle Write\n",
    "        f'cogroup join lineitem+orders, shipdate={TARGET_DATE}',\n",
    "        timestamp\n",
    "    ])\n",
    "    \n",
    "    # Add Parquet entry\n",
    "    writer.writerow([\n",
    "        run_id + 1,\n",
    "        'A2',\n",
    "        'Parquet',\n",
    "        TARGET_DATE,\n",
    "        'N/A',  # Multiple parquet files\n",
    "        'N/A',\n",
    "        duration_parquet,\n",
    "        'N/A',\n",
    "        'N/A',\n",
    "        f'same query with Parquet',\n",
    "        timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"  ‚úÖ Updated {metrics_log_path}\\n\")\n",
    "\n",
    "# ========== SPARK UI REMINDER ==========\n",
    "\n",
    "print(\"\\n\" + \"üì∏\" * 30)\n",
    "print(\"IMPORTANT: Capture Spark UI Screenshots for A2\")\n",
    "print(\"üì∏\" * 30 + \"\\n\")\n",
    "\n",
    "print(\"üéØ Required Screenshots:\\n\")\n",
    "\n",
    "print(\"  1. Jobs Tab:\")\n",
    "print(\"     - A2-TXT cogroup job\")\n",
    "print(\"     - A2-Parquet cogroup job\")\n",
    "print(\"     ‚Üí Save as: proof/screenshots/a2_jobs_cogroup.png\\n\")\n",
    "\n",
    "print(\"  2. Stages Tab (CRITICAL for cogroup):\")\n",
    "print(\"     - Look for 'cogroup' stage\")\n",
    "print(\"     - Capture Shuffle Read/Write metrics\")\n",
    "print(\"     - Note: cogroup causes shuffle on BOTH sides\")\n",
    "print(\"     ‚Üí Save as: proof/screenshots/a2_cogroup_shuffle.png\\n\")\n",
    "\n",
    "print(\"  3. SQL Tab (Parquet only):\")\n",
    "print(\"     - Physical plans for lineitem and orders loads\")\n",
    "print(\"     ‚Üí Save as: proof/screenshots/a2_parquet_plans.png\\n\")\n",
    "\n",
    "print(\"üìù Update lab_metrics_log.csv with:\")\n",
    "print(\"   - Shuffle Read (MB) from Stages tab\")\n",
    "print(\"   - Shuffle Write (MB) from Stages tab\")\n",
    "print(\"   - Input Size for both lineitem and orders\\n\")\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "\n",
    "print(\"\\n\" + \"‚úÖ\" * 30)\n",
    "print(\"Section 4 Summary ‚Äî Task A2 Complete\")\n",
    "print(\"‚úÖ\" * 30 + \"\\n\")\n",
    "\n",
    "print(\"üìã Deliverables:\")\n",
    "print(f\"  ‚úÖ Output TXT    : outputs/a2_result_txt_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Output Parquet: outputs/a2_result_parquet_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Plans Parquet : proof/plan_a2_lineitem_parquet_*.txt\")\n",
    "print(f\"                     proof/plan_a2_orders_parquet_*.txt\")\n",
    "print(f\"  ‚úÖ Metrics Log   : proof/lab_metrics_log.csv\\n\")\n",
    "\n",
    "print(\"üéØ Next Steps:\")\n",
    "print(\"  1. Capture Spark UI screenshots (Jobs, Stages with shuffle)\")\n",
    "print(\"  2. Update lab_metrics_log.csv with shuffle metrics\")\n",
    "print(\"  3. Proceed to A3 (broadcast hash join)\\n\")\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d69eda",
   "metadata": {},
   "source": [
    "### A3 ‚Äî Q3: part & supplier names (broadcast hash join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fc6e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 5 ‚Äî Part A: Task A3 - Part & Supplier Names\n",
      "============================================================\n",
      "\n",
      "üéØ Running A3 in Notebook Mode (both TXT and Parquet)\n",
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "Running A3 with TXT format\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A3 ‚Äî Part & Supplier Names on 1996-01-01\n",
      "Format: TXT\n",
      "Strategy: Broadcast hash join (map-side)\n",
      "============================================================\n",
      "\n",
      "üì° Step 1: Loading and broadcasting dimension tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 20,000 entries\n",
      "  ‚úÖ Broadcasted part: 20,000 entries\n",
      "  ‚úÖ Broadcasted dimension: 1,000 entries\n",
      "  ‚úÖ Broadcasted supplier: 1,000 entries\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded lineitem: 600,572 rows\n",
      "\n",
      "üîç Step 2: Filtering lineitem by shipdate = '1996-01-01'...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Filtered lineitem: 266 records\n",
      "\n",
      "üîó Step 3: Performing map-side join with broadcast tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Map-side join completed: 266 results\n",
      "     Format: (l_orderkey, p_name, s_name)\n",
      "\n",
      "üî¢ Step 4: Sorting by orderkey and taking top 20...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A3 Results (First 20):\n",
      "============================================================\n",
      "\n",
      "  Format         : TXT\n",
      "  Date           : 1996-01-01\n",
      "  Duration       : 14.79 seconds\n",
      "  Total Results  : 266\n",
      "\n",
      "  Top 20 (l_orderkey, p_name, s_name):\n",
      "\n",
      "     1. Order:   2309 | Part: burnished orchid rose rosy tomato | Supplier: Supplier#000000519\n",
      "     2. Order:   2595 | Part: purple floral green slate smoke | Supplier: Supplier#000000675\n",
      "     3. Order:   4773 | Part: turquoise yellow wheat salmon dim | Supplier: Supplier#000000315\n",
      "     4. Order:   9381 | Part: turquoise blush indian moccasin burlywood | Supplier: Supplier#000000020\n",
      "     5. Order:  17189 | Part: lavender green chocolate pink peach | Supplier: Supplier#000000561\n",
      "     6. Order:  17600 | Part: chartreuse snow saddle ghost medium | Supplier: Supplier#000000968\n",
      "     7. Order:  18372 | Part: navy black coral rose papaya   | Supplier: Supplier#000000641\n",
      "     8. Order:  19462 | Part: royal steel lime purple light  | Supplier: Supplier#000000679\n",
      "     9. Order:  22149 | Part: sienna snow frosted cornflower blush | Supplier: Supplier#000000655\n",
      "    10. Order:  22210 | Part: bisque tan coral blue salmon   | Supplier: Supplier#000000740\n",
      "    11. Order:  22982 | Part: blanched ghost burlywood rose white | Supplier: Supplier#000000377\n",
      "    12. Order:  26176 | Part: medium royal puff sky saddle   | Supplier: Supplier#000000182\n",
      "    13. Order:  26308 | Part: midnight ivory gainsboro slate khaki | Supplier: Supplier#000000905\n",
      "    14. Order:  26311 | Part: frosted orange seashell lemon chartreuse | Supplier: Supplier#000000437\n",
      "    15. Order:  27271 | Part: bisque steel azure blush linen | Supplier: Supplier#000000861\n",
      "    16. Order:  29383 | Part: goldenrod saddle plum chiffon lawn | Supplier: Supplier#000000409\n",
      "    17. Order:  32069 | Part: cornsilk medium midnight coral mint | Supplier: Supplier#000000867\n",
      "    18. Order:  32069 | Part: coral rose puff chiffon green  | Supplier: Supplier#000000991\n",
      "    19. Order:  33734 | Part: powder smoke aquamarine midnight grey | Supplier: Supplier#000000166\n",
      "    20. Order:  37703 | Part: chartreuse plum orchid violet lavender | Supplier: Supplier#000000187\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved output: outputs/a3_result_txt_19960101.txt\n",
      "\n",
      "\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "Running A3 with Parquet format\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A3 ‚Äî Part & Supplier Names on 1996-01-01\n",
      "Format: Parquet\n",
      "Strategy: Broadcast hash join (map-side)\n",
      "============================================================\n",
      "\n",
      "üì° Step 1: Loading and broadcasting dimension tables...\n",
      "\n",
      "  ‚úÖ Part plan saved: proof/plan_a3_part_parquet_19960101.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 20,000 entries\n",
      "  ‚úÖ Broadcasted part: 20,000 entries\n",
      "  ‚úÖ Supplier plan saved: proof/plan_a3_supplier_parquet_19960101.txt\n",
      "  ‚úÖ Broadcasted dimension: 1,000 entries\n",
      "  ‚úÖ Broadcasted supplier: 1,000 entries\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded lineitem: 600,572 rows\n",
      "\n",
      "üîç Step 2: Filtering lineitem by shipdate = '1996-01-01'...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Filtered lineitem: 266 records\n",
      "\n",
      "üîó Step 3: Performing map-side join with broadcast tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Map-side join completed: 266 results\n",
      "     Format: (l_orderkey, p_name, s_name)\n",
      "\n",
      "üî¢ Step 4: Sorting by orderkey and taking top 20...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=================================>                        (4 + 3) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A3 Results (First 20):\n",
      "============================================================\n",
      "\n",
      "  Format         : Parquet\n",
      "  Date           : 1996-01-01\n",
      "  Duration       : 29.46 seconds\n",
      "  Total Results  : 266\n",
      "\n",
      "  Top 20 (l_orderkey, p_name, s_name):\n",
      "\n",
      "     1. Order:   2309 | Part: burnished orchid rose rosy tomato | Supplier: Supplier#000000519\n",
      "     2. Order:   2595 | Part: purple floral green slate smoke | Supplier: Supplier#000000675\n",
      "     3. Order:   4773 | Part: turquoise yellow wheat salmon dim | Supplier: Supplier#000000315\n",
      "     4. Order:   9381 | Part: turquoise blush indian moccasin burlywood | Supplier: Supplier#000000020\n",
      "     5. Order:  17189 | Part: lavender green chocolate pink peach | Supplier: Supplier#000000561\n",
      "     6. Order:  17600 | Part: chartreuse snow saddle ghost medium | Supplier: Supplier#000000968\n",
      "     7. Order:  18372 | Part: navy black coral rose papaya   | Supplier: Supplier#000000641\n",
      "     8. Order:  19462 | Part: royal steel lime purple light  | Supplier: Supplier#000000679\n",
      "     9. Order:  22149 | Part: sienna snow frosted cornflower blush | Supplier: Supplier#000000655\n",
      "    10. Order:  22210 | Part: bisque tan coral blue salmon   | Supplier: Supplier#000000740\n",
      "    11. Order:  22982 | Part: blanched ghost burlywood rose white | Supplier: Supplier#000000377\n",
      "    12. Order:  26176 | Part: medium royal puff sky saddle   | Supplier: Supplier#000000182\n",
      "    13. Order:  26308 | Part: midnight ivory gainsboro slate khaki | Supplier: Supplier#000000905\n",
      "    14. Order:  26311 | Part: frosted orange seashell lemon chartreuse | Supplier: Supplier#000000437\n",
      "    15. Order:  27271 | Part: bisque steel azure blush linen | Supplier: Supplier#000000861\n",
      "    16. Order:  29383 | Part: goldenrod saddle plum chiffon lawn | Supplier: Supplier#000000409\n",
      "    17. Order:  32069 | Part: cornsilk medium midnight coral mint | Supplier: Supplier#000000867\n",
      "    18. Order:  32069 | Part: coral rose puff chiffon green  | Supplier: Supplier#000000991\n",
      "    19. Order:  33734 | Part: powder smoke aquamarine midnight grey | Supplier: Supplier#000000166\n",
      "    20. Order:  37703 | Part: chartreuse plum orchid violet lavender | Supplier: Supplier#000000187\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved output: outputs/a3_result_parquet_19960101.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A3 ‚Äî TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "\n",
      "| Metric              | TXT                  | Parquet              | Speedup     |\n",
      "|---------------------|----------------------|----------------------|-------------|\n",
      "| Total Results       | 266              | 266              | N/A         |\n",
      "| Duration            | 14.79s                | 29.46s                | 0.50x        |\n",
      "| Broadcast Part      | 20,000              | 20,000              | N/A         |\n",
      "| Broadcast Supplier  | 1,000              | 1,000              | N/A         |\n",
      "| Lineitem Filtered   | 266              | 266              | N/A         |\n",
      "\n",
      "‚úÖ Validation: Both formats return identical top 20 results\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Updating lab_metrics_log.csv...\n",
      "  ‚úÖ Updated proof/lab_metrics_log.csv\n",
      "\n",
      "\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "IMPORTANT: Capture Spark UI Screenshots for A3\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "\n",
      "üéØ Required Screenshots:\n",
      "\n",
      "  1. Jobs Tab:\n",
      "     - A3-TXT broadcast job\n",
      "     - A3-Parquet broadcast job\n",
      "     ‚Üí Save as: proof/screenshots/a3_jobs_broadcast.png\n",
      "\n",
      "  2. Stages Tab:\n",
      "     - Look for stages with 'broadcast' in name\n",
      "     - Note: NO shuffle (map-side join)\n",
      "     - Capture task metrics\n",
      "     ‚Üí Save as: proof/screenshots/a3_stages_no_shuffle.png\n",
      "\n",
      "  3. Storage Tab (CRITICAL for broadcast):\n",
      "     - Show broadcast variables in memory\n",
      "     - Capture size of part_bc and supplier_bc\n",
      "     ‚Üí Save as: proof/screenshots/a3_storage_broadcast.png\n",
      "\n",
      "  4. SQL Tab (Parquet only):\n",
      "     - Physical plans for part and supplier loads\n",
      "     ‚Üí Save as: proof/screenshots/a3_parquet_plans.png\n",
      "\n",
      "üìù Update lab_metrics_log.csv with:\n",
      "   - Input Size (lineitem + part + supplier)\n",
      "   - Broadcast size from Storage tab\n",
      "   - Note: shuffle_read_mb = 0 (no shuffle!)\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "Section 5 Summary ‚Äî Task A3 Complete\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "üìã Deliverables:\n",
      "  ‚úÖ Output TXT    : outputs/a3_result_txt_19960101.txt\n",
      "  ‚úÖ Output Parquet: outputs/a3_result_parquet_19960101.txt\n",
      "  ‚úÖ Plans Parquet : proof/plan_a3_part_parquet_*.txt\n",
      "                     proof/plan_a3_supplier_parquet_*.txt\n",
      "  ‚úÖ Metrics Log   : proof/lab_metrics_log.csv\n",
      "\n",
      "üéØ Key Observations for A3:\n",
      "  ‚úÖ Broadcast join = NO SHUFFLE (faster than A2 cogroup)\n",
      "  ‚úÖ Dimension tables loaded once and broadcasted to all workers\n",
      "  ‚úÖ Map-side join = each partition processes independently\n",
      "  ‚úÖ Ideal for small dimension tables (part, supplier)\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. Capture Spark UI screenshots (Jobs, Stages, Storage)\n",
      "  2. Verify NO shuffle in Stages tab\n",
      "  3. Check broadcast size in Storage tab\n",
      "  4. Update lab_metrics_log.csv\n",
      "  5. Proceed to A4 (mixed joins)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# broadcast maps for part and supplier\n",
    "# map over lineitem(date) -> join in-map -> take(20)\n",
    "\n",
    "# ============================================================\n",
    "# Section 5 - Part A: Task A3 - Part & Supplier Names\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 5 ‚Äî Part A: Task A3 - Part & Supplier Names\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# ========== DEFINE PATHS (FIX: Declare variables early) ==========\n",
    "\n",
    "# Configuration from Section 1\n",
    "DATA_DIR = \"data\"\n",
    "TPCH_DIR = os.path.join(DATA_DIR, \"tpch\")\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "TARGET_DATE = \"1996-01-01\"\n",
    "\n",
    "# ========== A3 IMPLEMENTATION ==========\n",
    "\n",
    "def run_a3_part_supplier_broadcast(input_path, target_date, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A3 (Q3) ‚Äî First 20 (l_orderkey, p_name, s_name) for shipped items on DATE\n",
    "    \n",
    "    Strategy: Broadcast hash join (map-side join)\n",
    "    - Broadcast small dimension tables (part, supplier)\n",
    "    - Map over filtered lineitem to join in-memory\n",
    "    \n",
    "    Args:\n",
    "        input_path: Base path to TPC-H data (TXT or Parquet)\n",
    "        target_date: Ship date to filter (YYYY-MM-DD format)\n",
    "        use_parquet: True to use Parquet, False for TXT\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results_list, duration_seconds, metrics_dict)\n",
    "    \n",
    "    Output Format: First 20 (l_orderkey, p_name, s_name) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A3 ‚Äî Part & Supplier Names on {target_date}\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"Strategy: Broadcast hash join (map-side)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========== STEP 1: LOAD & BROADCAST DIMENSION TABLES ==========\n",
    "    \n",
    "    print(f\"üì° Step 1: Loading and broadcasting dimension tables...\\n\")\n",
    "    \n",
    "    if use_parquet:\n",
    "        # Load part table\n",
    "        part_path = os.path.join(input_path, \"part\")\n",
    "        part_df = spark.read.parquet(part_path)\n",
    "        \n",
    "        # Save explain plan\n",
    "        plan_path = f\"proof/plan_a3_part_parquet_{target_date.replace('-', '')}.txt\"\n",
    "        os.makedirs(\"proof\", exist_ok=True)\n",
    "        \n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A3 Part Parquet Load Plan ({target_date}) ===\\n\\n\")\n",
    "            f.write(part_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        \n",
    "        print(f\"  ‚úÖ Part plan saved: {plan_path}\")\n",
    "        \n",
    "        # Convert to RDD and broadcast\n",
    "        part_rdd = part_df.rdd\n",
    "        part_count = part_rdd.count()\n",
    "        \n",
    "        # Broadcast: {p_partkey -> (p_partkey, p_name, ...)}\n",
    "        part_bc = broadcast_dimension(\n",
    "            spark, \n",
    "            part_rdd, \n",
    "            lambda row: row.p_partkey\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Broadcasted part: {part_count:,} entries\")\n",
    "        \n",
    "        # Load supplier table\n",
    "        supplier_path = os.path.join(input_path, \"supplier\")\n",
    "        supplier_df = spark.read.parquet(supplier_path)\n",
    "        \n",
    "        # Save explain plan\n",
    "        plan_path = f\"proof/plan_a3_supplier_parquet_{target_date.replace('-', '')}.txt\"\n",
    "        \n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A3 Supplier Parquet Load Plan ({target_date}) ===\\n\\n\")\n",
    "            f.write(supplier_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        \n",
    "        print(f\"  ‚úÖ Supplier plan saved: {plan_path}\")\n",
    "        \n",
    "        # Convert to RDD and broadcast\n",
    "        supplier_rdd = supplier_df.rdd\n",
    "        supplier_count = supplier_rdd.count()\n",
    "        \n",
    "        # Broadcast: {s_suppkey -> (s_suppkey, s_name, ...)}\n",
    "        supplier_bc = broadcast_dimension(\n",
    "            spark,\n",
    "            supplier_rdd,\n",
    "            lambda row: row.s_suppkey\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Broadcasted supplier: {supplier_count:,} entries\\n\")\n",
    "        \n",
    "        # Load lineitem\n",
    "        lineitem_path = os.path.join(input_path, \"lineitem\")\n",
    "        lineitem_df = spark.read.parquet(lineitem_path)\n",
    "        lineitem_rdd = lineitem_df.rdd\n",
    "        \n",
    "    else:\n",
    "        # Load part.tbl\n",
    "        part_path = os.path.join(input_path, \"part.tbl\")\n",
    "        part_rdd = sc.textFile(part_path).map(parse_part)\n",
    "        part_count = part_rdd.count()\n",
    "        \n",
    "        # Broadcast part: key = p_partkey (index 0)\n",
    "        part_bc = broadcast_dimension(\n",
    "            spark,\n",
    "            part_rdd,\n",
    "            lambda t: t[0]  # p_partkey at index 0\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Broadcasted part: {part_count:,} entries\")\n",
    "        \n",
    "        # Load supplier.tbl\n",
    "        supplier_path = os.path.join(input_path, \"supplier.tbl\")\n",
    "        supplier_rdd = sc.textFile(supplier_path).map(parse_supplier)\n",
    "        supplier_count = supplier_rdd.count()\n",
    "        \n",
    "        # Broadcast supplier: key = s_suppkey (index 0)\n",
    "        supplier_bc = broadcast_dimension(\n",
    "            spark,\n",
    "            supplier_rdd,\n",
    "            lambda t: t[0]  # s_suppkey at index 0\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Broadcasted supplier: {supplier_count:,} entries\\n\")\n",
    "        \n",
    "        # Load lineitem.tbl\n",
    "        lineitem_path = os.path.join(input_path, \"lineitem.tbl\")\n",
    "        lineitem_rdd = sc.textFile(lineitem_path).map(parse_lineitem)\n",
    "    \n",
    "    lineitem_count = lineitem_rdd.count()\n",
    "    print(f\"  ‚úÖ Loaded lineitem: {lineitem_count:,} rows\\n\")\n",
    "    \n",
    "    # ========== STEP 2: FILTER LINEITEM BY SHIPDATE ==========\n",
    "    \n",
    "    print(f\"üîç Step 2: Filtering lineitem by shipdate = '{target_date}'...\\n\")\n",
    "    \n",
    "    if use_parquet:\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda row: row.l_shipdate == target_date)\n",
    "    else:\n",
    "        # l_shipdate at index 10\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda t: t[10] == target_date)\n",
    "    \n",
    "    filtered_count = lineitem_filtered.count()\n",
    "    print(f\"  ‚úÖ Filtered lineitem: {filtered_count:,} records\\n\")\n",
    "    \n",
    "    # ========== STEP 3: MAP-SIDE JOIN (BROADCAST JOIN) ==========\n",
    "    \n",
    "    print(f\"üîó Step 3: Performing map-side join with broadcast tables...\\n\")\n",
    "    \n",
    "    def map_join_part_supplier(lineitem_row):\n",
    "        \"\"\"\n",
    "        Join lineitem with part and supplier using broadcast variables\n",
    "        \n",
    "        Args:\n",
    "            lineitem_row: Row (Parquet) or tuple (TXT)\n",
    "        \n",
    "        Returns:\n",
    "            (l_orderkey, p_name, s_name) or None if no match\n",
    "        \"\"\"\n",
    "        if use_parquet:\n",
    "            # Parquet: Row objects\n",
    "            l_orderkey = lineitem_row.l_orderkey\n",
    "            l_partkey = lineitem_row.l_partkey\n",
    "            l_suppkey = lineitem_row.l_suppkey\n",
    "        else:\n",
    "            # TXT: tuples\n",
    "            # l_orderkey at 0, l_partkey at 1, l_suppkey at 2\n",
    "            l_orderkey = lineitem_row[0]\n",
    "            l_partkey = lineitem_row[1]\n",
    "            l_suppkey = lineitem_row[2]\n",
    "        \n",
    "        # Lookup in broadcast maps\n",
    "        part_tuple = part_bc.value.get(l_partkey)\n",
    "        supplier_tuple = supplier_bc.value.get(l_suppkey)\n",
    "        \n",
    "        # Inner join: only return if both dimensions found\n",
    "        if part_tuple is not None and supplier_tuple is not None:\n",
    "            if use_parquet:\n",
    "                # Parquet Row objects\n",
    "                p_name = part_tuple.p_name\n",
    "                s_name = supplier_tuple.s_name\n",
    "            else:\n",
    "                # TXT tuples\n",
    "                # p_name at index 1, s_name at index 1\n",
    "                p_name = part_tuple[1]\n",
    "                s_name = supplier_tuple[1]\n",
    "            \n",
    "            return (l_orderkey, p_name, s_name)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Apply map-side join\n",
    "    joined_rdd = lineitem_filtered \\\n",
    "        .map(map_join_part_supplier) \\\n",
    "        .filter(lambda x: x is not None)  # Remove None (non-matching rows)\n",
    "    \n",
    "    joined_count = joined_rdd.count()\n",
    "    print(f\"  ‚úÖ Map-side join completed: {joined_count:,} results\")\n",
    "    print(f\"     Format: (l_orderkey, p_name, s_name)\\n\")\n",
    "    \n",
    "    # ========== STEP 4: SORT AND TAKE TOP 20 ==========\n",
    "    \n",
    "    print(f\"üî¢ Step 4: Sorting by orderkey and taking top 20...\\n\")\n",
    "    \n",
    "    # Sort by l_orderkey (first element)\n",
    "    sorted_rdd = joined_rdd.sortBy(lambda t: t[0], ascending=True)\n",
    "    \n",
    "    # Take first 20\n",
    "    top_20 = sorted_rdd.take(20)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # ========== OUTPUT ==========\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ A3 Results (First 20):\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"  Format         : {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"  Date           : {target_date}\")\n",
    "    print(f\"  Duration       : {duration:.2f} seconds\")\n",
    "    print(f\"  Total Results  : {joined_count:,}\")\n",
    "    print(f\"\\n  Top 20 (l_orderkey, p_name, s_name):\\n\")\n",
    "    \n",
    "    for i, (orderkey, p_name, s_name) in enumerate(top_20, 1):\n",
    "        print(f\"    {i:2d}. Order: {orderkey:>6} | Part: {p_name:<30} | Supplier: {s_name}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    # ========== SAVE OUTPUT ==========\n",
    "    \n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a3_result_{format_suffix}_{target_date.replace('-', '')}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A3 - Part & Supplier Names (Broadcast Hash Join)\\n\")\n",
    "        f.write(f\"Date: {target_date}\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\")\n",
    "        f.write(f\"Total Results: {joined_count:,}\\n\")\n",
    "        f.write(f\"\\nFirst 20 (l_orderkey, p_name, s_name):\\n\\n\")\n",
    "        \n",
    "        for orderkey, p_name, s_name in top_20:\n",
    "            f.write(f\"{orderkey},{p_name},{s_name}\\n\")\n",
    "    \n",
    "    print(f\"üíæ Saved output: {output_path}\\n\")\n",
    "    \n",
    "    # ========== COLLECT METRICS ==========\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A3',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'date': target_date,\n",
    "        'total_results': joined_count,\n",
    "        'duration_s': round(duration, 2),\n",
    "        'broadcast_size_part': part_count,\n",
    "        'broadcast_size_supplier': supplier_count,\n",
    "        'lineitem_filtered': filtered_count\n",
    "    }\n",
    "    \n",
    "    return top_20, duration, metrics\n",
    "\n",
    "# ========== NOTEBOOK EXECUTION ==========\n",
    "\n",
    "print(\"\\nüéØ Running A3 in Notebook Mode (both TXT and Parquet)\\n\")\n",
    "\n",
    "# ========== RUN A3 WITH TXT ==========\n",
    "\n",
    "print(\"\\n\" + \"üî∑\" * 30)\n",
    "print(\"Running A3 with TXT format\")\n",
    "print(\"üî∑\" * 30 + \"\\n\")\n",
    "\n",
    "results_txt, duration_txt, metrics_txt = run_a3_part_supplier_broadcast(\n",
    "    input_path=INPUT_TXT_PATH,\n",
    "    target_date=TARGET_DATE,\n",
    "    use_parquet=False\n",
    ")\n",
    "\n",
    "# ========== RUN A3 WITH PARQUET ==========\n",
    "\n",
    "print(\"\\n\" + \"üî∂\" * 30)\n",
    "print(\"Running A3 with Parquet format\")\n",
    "print(\"üî∂\" * 30 + \"\\n\")\n",
    "\n",
    "results_parquet, duration_parquet, metrics_parquet = run_a3_part_supplier_broadcast(\n",
    "    input_path=INPUT_PARQUET_PATH,\n",
    "    target_date=TARGET_DATE,\n",
    "    use_parquet=True\n",
    ")\n",
    "\n",
    "# ========== COMPARISON ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"A3 ‚Äî TXT vs Parquet Comparison\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "comparison_table = f\"\"\"\n",
    "| Metric              | TXT                  | Parquet              | Speedup     |\n",
    "|---------------------|----------------------|----------------------|-------------|\n",
    "| Total Results       | {metrics_txt['total_results']:,}              | {metrics_parquet['total_results']:,}              | N/A         |\n",
    "| Duration            | {duration_txt:.2f}s                | {duration_parquet:.2f}s                | {duration_txt/duration_parquet:.2f}x        |\n",
    "| Broadcast Part      | {metrics_txt['broadcast_size_part']:,}              | {metrics_parquet['broadcast_size_part']:,}              | N/A         |\n",
    "| Broadcast Supplier  | {metrics_txt['broadcast_size_supplier']:,}              | {metrics_parquet['broadcast_size_supplier']:,}              | N/A         |\n",
    "| Lineitem Filtered   | {metrics_txt['lineitem_filtered']:,}              | {metrics_parquet['lineitem_filtered']:,}              | N/A         |\n",
    "\"\"\"\n",
    "\n",
    "print(comparison_table)\n",
    "\n",
    "# Validation\n",
    "results_match = results_txt == results_parquet\n",
    "\n",
    "if results_match:\n",
    "    print(f\"‚úÖ Validation: Both formats return identical top 20 results\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Results differ between TXT and Parquet!\")\n",
    "    print(f\"\\n  TXT first 5:\")\n",
    "    for i, (orderkey, p_name, s_name) in enumerate(results_txt[:5], 1):\n",
    "        print(f\"    {i}. {orderkey}, {p_name}, {s_name}\")\n",
    "    \n",
    "    print(f\"\\n  Parquet first 5:\")\n",
    "    for i, (orderkey, p_name, s_name) in enumerate(results_parquet[:5], 1):\n",
    "        print(f\"    {i}. {orderkey}, {p_name}, {s_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# ========== UPDATE METRICS LOG ==========\n",
    "\n",
    "print(\"üìä Updating lab_metrics_log.csv...\")\n",
    "\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Get next run_id\n",
    "    with open(metrics_log_path, 'r') as rf:\n",
    "        lines = list(csv.reader(rf))\n",
    "        run_id = len(lines)\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    # Add TXT entry\n",
    "    writer.writerow([\n",
    "        run_id,\n",
    "        'A3',\n",
    "        'TXT',\n",
    "        TARGET_DATE,\n",
    "        3,  # lineitem.tbl + part.tbl + supplier.tbl\n",
    "        'N/A',  # To be filled from Spark UI\n",
    "        duration_txt,\n",
    "        0,  # No shuffle for broadcast join\n",
    "        0,\n",
    "        f'broadcast join lineitem+part+supplier, shipdate={TARGET_DATE}, part_bc={metrics_txt[\"broadcast_size_part\"]}, supp_bc={metrics_txt[\"broadcast_size_supplier\"]}',\n",
    "        timestamp\n",
    "    ])\n",
    "    \n",
    "    # Add Parquet entry\n",
    "    writer.writerow([\n",
    "        run_id + 1,\n",
    "        'A3',\n",
    "        'Parquet',\n",
    "        TARGET_DATE,\n",
    "        'N/A',  # Multiple parquet files\n",
    "        'N/A',\n",
    "        duration_parquet,\n",
    "        0,\n",
    "        0,\n",
    "        f'same query with Parquet',\n",
    "        timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"  ‚úÖ Updated {metrics_log_path}\\n\")\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "\n",
    "print(\"\\n\" + \"‚úÖ\" * 30)\n",
    "print(\"Section 5 Summary ‚Äî Task A3 Complete\")\n",
    "print(\"‚úÖ\" * 30 + \"\\n\")\n",
    "\n",
    "print(\"üìã Deliverables:\")\n",
    "print(f\"  ‚úÖ Output TXT    : outputs/a3_result_txt_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Output Parquet: outputs/a3_result_parquet_{TARGET_DATE.replace('-', '')}.txt\")\n",
    "print(f\"  ‚úÖ Plans Parquet : proof/plan_a3_part_parquet_*.txt\")\n",
    "print(f\"                     proof/plan_a3_supplier_parquet_*.txt\")\n",
    "print(f\"  ‚úÖ Metrics Log   : proof/lab_metrics_log.csv\\n\")\n",
    "\n",
    "print(\"üéØ Key Observations for A3:\")\n",
    "print(\"  ‚úÖ Broadcast join = NO SHUFFLE (faster than A2 cogroup)\")\n",
    "print(\"  ‚úÖ Dimension tables loaded once and broadcasted to all workers\")\n",
    "print(\"  ‚úÖ Map-side join = each partition processes independently\")\n",
    "print(\"  ‚úÖ Ideal for small dimension tables (part, supplier)\\n\")\n",
    "\n",
    "print(\"üéØ Next Steps:\")\n",
    "print(\"  1. Capture Spark UI screenshots (Jobs, Stages, Storage)\")\n",
    "print(\"  2. Verify NO shuffle in Stages tab\")\n",
    "print(\"  3. Check broadcast size in Storage tab\")\n",
    "print(\"  4. Update lab_metrics_log.csv\")\n",
    "print(\"  5. Proceed to A4 (mixed joins)\\n\")\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b24354",
   "metadata": {},
   "source": [
    "### A4 ‚Äî Q4: shipped items by nation (mixed joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf254d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 6 ‚Äî Part A: Task A4 - Nation Shipment Counts\n",
      "============================================================\n",
      "\n",
      "üéØ Running A4 (both TXT and Parquet)\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "============================================================\n",
      "Task A4 ‚Äî Nation Shipment Counts on 1996-01-01\n",
      "Format: TXT\n",
      "Strategy: Reduce-side (lineitem+orders) + Broadcast (customer+nation)\n",
      "============================================================\n",
      "\n",
      "üì° Step 1: Broadcasting dimension tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 15,000 entries\n",
      "  ‚úÖ Broadcasted customer: 15,000 entries\n",
      "  ‚úÖ Broadcasted dimension: 25 entries\n",
      "  ‚úÖ Broadcasted nation: 25 entries\n",
      "\n",
      "üîç Step 2: Filtering lineitem by shipdate...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Filtered: 266 records\n",
      "\n",
      "üîÄ Step 3: Reduce-side join lineitem ‚®ù orders...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Join completed: 266 (orderkey, custkey) pairs\n",
      "\n",
      "üîó Step 4: Broadcast join with customer+nation...\n",
      "\n",
      "üìä Step 5: Aggregating by nation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A4 Results (Nation Shipment Counts):\n",
      "============================================================\n",
      "\n",
      "  Format   : TXT\n",
      "  Date     : 1996-01-01\n",
      "  Duration : 13.94s\n",
      "  Nations  : 25\n",
      "\n",
      "   0 | ALGERIA              |     5\n",
      "   1 | ARGENTINA            |     8\n",
      "   2 | BRAZIL               |    15\n",
      "   3 | CANADA               |    12\n",
      "   4 | EGYPT                |     8\n",
      "   5 | ETHIOPIA             |     6\n",
      "   6 | FRANCE               |     9\n",
      "   7 | GERMANY              |    17\n",
      "   8 | INDIA                |     8\n",
      "   9 | INDONESIA            |    13\n",
      "  10 | IRAN                 |    14\n",
      "  11 | IRAQ                 |    13\n",
      "  12 | JAPAN                |    19\n",
      "  13 | JORDAN               |    14\n",
      "  14 | KENYA                |     7\n",
      "  15 | MOROCCO              |    10\n",
      "  16 | MOZAMBIQUE           |     9\n",
      "  17 | PERU                 |     8\n",
      "  18 | CHINA                |     4\n",
      "  19 | ROMANIA              |    21\n",
      "  20 | SAUDI ARABIA         |     9\n",
      "  21 | VIETNAM              |     7\n",
      "  22 | RUSSIA               |    13\n",
      "  23 | UNITED KINGDOM       |     8\n",
      "  24 | UNITED STATES        |     9\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved: outputs/a4_result_txt_19960101.txt\n",
      "\n",
      "\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "üî∂\n",
      "\n",
      "============================================================\n",
      "Task A4 ‚Äî Nation Shipment Counts on 1996-01-01\n",
      "Format: Parquet\n",
      "Strategy: Reduce-side (lineitem+orders) + Broadcast (customer+nation)\n",
      "============================================================\n",
      "\n",
      "üì° Step 1: Broadcasting dimension tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 15,000 entries\n",
      "  ‚úÖ Broadcasted customer: 15,000 entries\n",
      "  ‚úÖ Broadcasted dimension: 25 entries\n",
      "  ‚úÖ Broadcasted nation: 25 entries\n",
      "\n",
      "üîç Step 2: Filtering lineitem by shipdate...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Filtered: 266 records\n",
      "\n",
      "üîÄ Step 3: Reduce-side join lineitem ‚®ù orders...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Join completed: 266 (orderkey, custkey) pairs\n",
      "\n",
      "üîó Step 4: Broadcast join with customer+nation...\n",
      "\n",
      "üìä Step 5: Aggregating by nation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:===========================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ A4 Results (Nation Shipment Counts):\n",
      "============================================================\n",
      "\n",
      "  Format   : Parquet\n",
      "  Date     : 1996-01-01\n",
      "  Duration : 18.39s\n",
      "  Nations  : 25\n",
      "\n",
      "   0 | ALGERIA              |     5\n",
      "   1 | ARGENTINA            |     8\n",
      "   2 | BRAZIL               |    15\n",
      "   3 | CANADA               |    12\n",
      "   4 | EGYPT                |     8\n",
      "   5 | ETHIOPIA             |     6\n",
      "   6 | FRANCE               |     9\n",
      "   7 | GERMANY              |    17\n",
      "   8 | INDIA                |     8\n",
      "   9 | INDONESIA            |    13\n",
      "  10 | IRAN                 |    14\n",
      "  11 | IRAQ                 |    13\n",
      "  12 | JAPAN                |    19\n",
      "  13 | JORDAN               |    14\n",
      "  14 | KENYA                |     7\n",
      "  15 | MOROCCO              |    10\n",
      "  16 | MOZAMBIQUE           |     9\n",
      "  17 | PERU                 |     8\n",
      "  18 | CHINA                |     4\n",
      "  19 | ROMANIA              |    21\n",
      "  20 | SAUDI ARABIA         |     9\n",
      "  21 | VIETNAM              |     7\n",
      "  22 | RUSSIA               |    13\n",
      "  23 | UNITED KINGDOM       |     8\n",
      "  24 | UNITED STATES        |     9\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved: outputs/a4_result_parquet_19960101.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A4 ‚Äî TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "| Metric    | TXT       | Parquet   | Speedup |\n",
      "|-----------|-----------|-----------|---------|\n",
      "| Duration  | 13.94s     | 18.39s     | 0.76x    |\n",
      "| Nations   | 25        | 25        | -       |\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Updated proof/lab_metrics_log.csv\n",
      "\n",
      "üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏üì∏\n",
      "\n",
      "üéØ Spark UI Screenshots for A4:\n",
      "\n",
      "  1. Jobs Tab ‚Üí A4-TXT and A4-Parquet (cogroup + broadcast)\n",
      "  2. Stages Tab ‚Üí Cogroup stage (shuffle metrics)\n",
      "  3. Storage Tab ‚Üí Broadcast variables (customer, nation)\n",
      "\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "üì∏\n",
      "\n",
      "‚úÖ Section 6 ‚Äî Task A4 Complete\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# reduce-side for (lineitem ‚®ù orders); broadcast for (customer, nation)\n",
    "# aggregate to (n_nationkey, n_name, count)\n",
    "# ============================================================\n",
    "# Section 6 - Part A: Task A4 - Nation Shipment Counts\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 6 ‚Äî Part A: Task A4 - Nation Shipment Counts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# ========== DEFINE PATHS ==========\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "TPCH_DIR = os.path.join(DATA_DIR, \"tpch\")\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "TARGET_DATE = \"1996-01-01\"\n",
    "\n",
    "# ========== A4 IMPLEMENTATION ==========\n",
    "\n",
    "def run_a4_nation_counts(input_path, target_date, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A4 (Q4) ‚Äî (n_nationkey, n_name, count) for items shipped to each nation on DATE\n",
    "    \n",
    "    Strategy: Mixed joins\n",
    "    - Reduce-side join: lineitem ‚®ù orders (both large)\n",
    "    - Broadcast join: customer, nation (small dimensions)\n",
    "    \n",
    "    Output Format: (n_nationkey, n_name, count) sorted by nationkey\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A4 ‚Äî Nation Shipment Counts on {target_date}\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"Strategy: Reduce-side (lineitem+orders) + Broadcast (customer+nation)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========== STEP 1: BROADCAST DIMENSIONS ==========\n",
    "    \n",
    "    print(f\"üì° Step 1: Broadcasting dimension tables...\\n\")\n",
    "    \n",
    "    if use_parquet:\n",
    "        # Customer\n",
    "        customer_df = spark.read.parquet(os.path.join(input_path, \"customer\"))\n",
    "        customer_rdd = customer_df.rdd\n",
    "        customer_bc = broadcast_dimension(spark, customer_rdd, lambda row: row.c_custkey)\n",
    "        print(f\"  ‚úÖ Broadcasted customer: {customer_rdd.count():,} entries\")\n",
    "        \n",
    "        # Nation\n",
    "        nation_df = spark.read.parquet(os.path.join(input_path, \"nation\"))\n",
    "        nation_rdd = nation_df.rdd\n",
    "        nation_bc = broadcast_dimension(spark, nation_rdd, lambda row: row.n_nationkey)\n",
    "        print(f\"  ‚úÖ Broadcasted nation: {nation_rdd.count():,} entries\\n\")\n",
    "        \n",
    "        # Load lineitem & orders\n",
    "        lineitem_rdd = spark.read.parquet(os.path.join(input_path, \"lineitem\")).rdd\n",
    "        orders_rdd = spark.read.parquet(os.path.join(input_path, \"orders\")).rdd\n",
    "        \n",
    "    else:\n",
    "        # Customer\n",
    "        customer_rdd = sc.textFile(os.path.join(input_path, \"customer.tbl\")).map(parse_customer)\n",
    "        customer_bc = broadcast_dimension(spark, customer_rdd, lambda t: t[0])  # c_custkey\n",
    "        print(f\"  ‚úÖ Broadcasted customer: {customer_rdd.count():,} entries\")\n",
    "        \n",
    "        # Nation\n",
    "        nation_rdd = sc.textFile(os.path.join(input_path, \"nation.tbl\")).map(parse_nation)\n",
    "        nation_bc = broadcast_dimension(spark, nation_rdd, lambda t: t[0])  # n_nationkey\n",
    "        print(f\"  ‚úÖ Broadcasted nation: {nation_rdd.count():,} entries\\n\")\n",
    "        \n",
    "        # Load lineitem & orders\n",
    "        lineitem_rdd = sc.textFile(os.path.join(input_path, \"lineitem.tbl\")).map(parse_lineitem)\n",
    "        orders_rdd = sc.textFile(os.path.join(input_path, \"orders.tbl\")).map(parse_orders)\n",
    "    \n",
    "    # ========== STEP 2: FILTER LINEITEM ==========\n",
    "    \n",
    "    print(f\"üîç Step 2: Filtering lineitem by shipdate...\\n\")\n",
    "    \n",
    "    if use_parquet:\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda row: row.l_shipdate == target_date)\n",
    "    else:\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda t: t[10] == target_date)  # l_shipdate\n",
    "    \n",
    "    print(f\"  ‚úÖ Filtered: {lineitem_filtered.count():,} records\\n\")\n",
    "    \n",
    "    # ========== STEP 3: REDUCE-SIDE JOIN (lineitem ‚®ù orders) ==========\n",
    "    \n",
    "    print(f\"üîÄ Step 3: Reduce-side join lineitem ‚®ù orders...\\n\")\n",
    "    \n",
    "    if use_parquet:\n",
    "        lineitem_keyed = lineitem_filtered.map(lambda row: (row.l_orderkey, row))\n",
    "        orders_keyed = orders_rdd.map(lambda row: (row.o_orderkey, row.o_custkey))\n",
    "    else:\n",
    "        lineitem_keyed = lineitem_filtered.map(lambda t: (t[0], t))  # (l_orderkey, row)\n",
    "        orders_keyed = orders_rdd.map(lambda t: (t[0], t[1]))  # (o_orderkey, o_custkey)\n",
    "    \n",
    "    # Cogroup join\n",
    "    joined = lineitem_keyed.cogroup(orders_keyed) \\\n",
    "        .flatMap(lambda kv: [(kv[0], custkey) for lineitem_list in [list(kv[1][0])] \n",
    "                              for orders_list in [list(kv[1][1])] \n",
    "                              for _ in lineitem_list \n",
    "                              for custkey in orders_list])\n",
    "    \n",
    "    print(f\"  ‚úÖ Join completed: {joined.count():,} (orderkey, custkey) pairs\\n\")\n",
    "    \n",
    "    # ========== STEP 4: BROADCAST JOIN (customer ‚Üí nation) ==========\n",
    "    \n",
    "    print(f\"üîó Step 4: Broadcast join with customer+nation...\\n\")\n",
    "    \n",
    "    def map_to_nation(item):\n",
    "        orderkey, custkey = item\n",
    "        customer = customer_bc.value.get(custkey)\n",
    "        if customer:\n",
    "            if use_parquet:\n",
    "                nationkey = customer.c_nationkey\n",
    "            else:\n",
    "                nationkey = customer[3]  # c_nationkey at index 3\n",
    "            \n",
    "            nation = nation_bc.value.get(nationkey)\n",
    "            if nation:\n",
    "                if use_parquet:\n",
    "                    return (nation.n_nationkey, nation.n_name)\n",
    "                else:\n",
    "                    return (nation[0], nation[1])  # (n_nationkey, n_name)\n",
    "        return None\n",
    "    \n",
    "    nations = joined.map(map_to_nation).filter(lambda x: x is not None)\n",
    "    \n",
    "    # ========== STEP 5: AGGREGATE BY NATION ==========\n",
    "    \n",
    "    print(f\"üìä Step 5: Aggregating by nation...\\n\")\n",
    "    \n",
    "    counts = nations.map(lambda kv: (kv, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .map(lambda kv: (kv[0][0], kv[0][1], kv[1])) \\\n",
    "        .sortBy(lambda t: t[0])  # Sort by nationkey\n",
    "    \n",
    "    results = counts.collect()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # ========== OUTPUT ==========\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ A4 Results (Nation Shipment Counts):\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"  Format   : {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"  Date     : {target_date}\")\n",
    "    print(f\"  Duration : {duration:.2f}s\")\n",
    "    print(f\"  Nations  : {len(results)}\\n\")\n",
    "    \n",
    "    for nationkey, name, count in results:\n",
    "        print(f\"  {nationkey:2d} | {name:<20} | {count:>5}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    # ========== SAVE OUTPUT ==========\n",
    "    \n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a4_result_{format_suffix}_{target_date.replace('-', '')}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A4 - Nation Shipment Counts (Mixed Join)\\n\")\n",
    "        f.write(f\"Date: {target_date}\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\\n\")\n",
    "        \n",
    "        for nationkey, name, count in results:\n",
    "            f.write(f\"{nationkey},{name},{count}\\n\")\n",
    "    \n",
    "    print(f\"üíæ Saved: {output_path}\\n\")\n",
    "    \n",
    "    # ========== METRICS ==========\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A4',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'date': target_date,\n",
    "        'duration_s': round(duration, 2),\n",
    "        'num_nations': len(results)\n",
    "    }\n",
    "    \n",
    "    return results, duration, metrics\n",
    "\n",
    "# ========== RUN A4 ==========\n",
    "\n",
    "print(\"\\nüéØ Running A4 (both TXT and Parquet)\\n\")\n",
    "\n",
    "# TXT\n",
    "\n",
    "results_txt, duration_txt, metrics_txt = run_a4_nation_counts(\n",
    "    INPUT_TXT_PATH, TARGET_DATE, use_parquet=False\n",
    ")\n",
    "\n",
    "# Parquet\n",
    "\n",
    "results_parquet, duration_parquet, metrics_parquet = run_a4_nation_counts(\n",
    "    INPUT_PARQUET_PATH, TARGET_DATE, use_parquet=True\n",
    ")\n",
    "\n",
    "# ========== COMPARISON ==========\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"A4 ‚Äî TXT vs Parquet Comparison\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"| Metric    | TXT       | Parquet   | Speedup |\")\n",
    "print(f\"|-----------|-----------|-----------|---------|\")\n",
    "print(f\"| Duration  | {duration_txt:.2f}s     | {duration_parquet:.2f}s     | {duration_txt/duration_parquet:.2f}x    |\")\n",
    "print(f\"| Nations   | {len(results_txt)}        | {len(results_parquet)}        | -       |\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# ========== UPDATE METRICS LOG ==========\n",
    "\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    with open(metrics_log_path, 'r') as rf:\n",
    "        run_id = len(list(csv.reader(rf)))\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id, 'A4', 'TXT', TARGET_DATE, 4, 'N/A', duration_txt,\n",
    "        'N/A', 'N/A', 'mixed join: reduce-side (lineitem+orders) + broadcast (customer+nation)', timestamp\n",
    "    ])\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id + 1, 'A4', 'Parquet', TARGET_DATE, 'N/A', 'N/A', duration_parquet,\n",
    "        'N/A', 'N/A', 'same query with Parquet', timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"‚úÖ Updated {metrics_log_path}\\n\")\n",
    "\n",
    "# ========== SPARK UI REMINDER ==========\n",
    "\n",
    "\n",
    "print(\"\\nüéØ Spark UI Screenshots for A4:\\n\")\n",
    "print(\"  1. Jobs Tab ‚Üí A4-TXT and A4-Parquet (cogroup + broadcast)\")\n",
    "print(\"  2. Stages Tab ‚Üí Cogroup stage (shuffle metrics)\")\n",
    "print(\"  3. Storage Tab ‚Üí Broadcast variables (customer, nation)\")\n",
    "print(\"\\nüì∏\" * 30 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Section 6 ‚Äî Task A4 Complete\\n\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a6688",
   "metadata": {},
   "source": [
    "### A5 ‚Äî Q5: monthly US vs CANADA volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c2e744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running A5 (TXT and Parquet)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task A5 - Monthly Volumes (US vs CANADA)\n",
      "Format: TXT\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 25 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 15,000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results: 166 monthly volumes\n",
      "\n",
      "  3 | CANADA          | 1992-01 |     29\n",
      "  3 | CANADA          | 1992-02 |     99\n",
      "  3 | CANADA          | 1992-03 |    198\n",
      "  3 | CANADA          | 1992-04 |    285\n",
      "  3 | CANADA          | 1992-05 |    349\n",
      "  3 | CANADA          | 1992-06 |    317\n",
      "  3 | CANADA          | 1992-07 |    362\n",
      "  3 | CANADA          | 1992-08 |    335\n",
      "  3 | CANADA          | 1992-09 |    292\n",
      "  3 | CANADA          | 1992-10 |    286\n",
      "  3 | CANADA          | 1992-11 |    321\n",
      "  3 | CANADA          | 1992-12 |    314\n",
      "  3 | CANADA          | 1993-01 |    344\n",
      "  3 | CANADA          | 1993-02 |    314\n",
      "  3 | CANADA          | 1993-03 |    350\n",
      "  3 | CANADA          | 1993-04 |    305\n",
      "  3 | CANADA          | 1993-05 |    286\n",
      "  3 | CANADA          | 1993-06 |    322\n",
      "  3 | CANADA          | 1993-07 |    324\n",
      "  3 | CANADA          | 1993-08 |    336\n",
      "  ... (146 more rows)\n",
      "\n",
      "Duration: 14.39s\n",
      "\n",
      "Saved: outputs/a5_result_txt.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A5 - Monthly Volumes (US vs CANADA)\n",
      "Format: Parquet\n",
      "============================================================\n",
      "\n",
      "  ‚úÖ Broadcasted dimension: 25 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 15,000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results: 166 monthly volumes\n",
      "\n",
      "  3 | CANADA          | 1992-01 |     29\n",
      "  3 | CANADA          | 1992-02 |     99\n",
      "  3 | CANADA          | 1992-03 |    198\n",
      "  3 | CANADA          | 1992-04 |    285\n",
      "  3 | CANADA          | 1992-05 |    349\n",
      "  3 | CANADA          | 1992-06 |    317\n",
      "  3 | CANADA          | 1992-07 |    362\n",
      "  3 | CANADA          | 1992-08 |    335\n",
      "  3 | CANADA          | 1992-09 |    292\n",
      "  3 | CANADA          | 1992-10 |    286\n",
      "  3 | CANADA          | 1992-11 |    321\n",
      "  3 | CANADA          | 1992-12 |    314\n",
      "  3 | CANADA          | 1993-01 |    344\n",
      "  3 | CANADA          | 1993-02 |    314\n",
      "  3 | CANADA          | 1993-03 |    350\n",
      "  3 | CANADA          | 1993-04 |    305\n",
      "  3 | CANADA          | 1993-05 |    286\n",
      "  3 | CANADA          | 1993-06 |    322\n",
      "  3 | CANADA          | 1993-07 |    324\n",
      "  3 | CANADA          | 1993-08 |    336\n",
      "  ... (146 more rows)\n",
      "\n",
      "Duration: 20.98s\n",
      "\n",
      "Saved: outputs/a5_result_parquet.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A5 - TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "| Metric    | TXT       | Parquet   | Speedup |\n",
      "|-----------|-----------|-----------|---------|\n",
      "| Duration  | 14.39s     | 20.98s     | 0.69x    |\n",
      "| Results   | 166        | 166        | -       |\n",
      "\n",
      "============================================================\n",
      "\n",
      "Updated proof/lab_metrics_log.csv\n",
      "\n",
      "============================================================\n",
      "Section 7 - Task A5 Complete\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# compute (nationkey, n_name, yyyy-mm, count) across full data\n",
    "# write CSV for plotting; keep timings for TXT vs PARQUET\n",
    "# ============================================================\n",
    "# Section 7 - Part A: Task A5 - Monthly Volumes US vs CANADA\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data\"\n",
    "TPCH_DIR = os.path.join(DATA_DIR, \"tpch\")\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "\n",
    "def run_a5_monthly_volumes(input_path, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A5 (Q5) - Monthly volumes for US vs CANADA across full warehouse\n",
    "    \n",
    "    Strategy:\n",
    "    - Broadcast nation, customer\n",
    "    - Reduce-side join lineitem+orders\n",
    "    - Extract YYYY-MM from l_shipdate\n",
    "    - Aggregate by (nationkey, n_name, YYYY-MM)\n",
    "    \n",
    "    Output: (nationkey, n_name, YYYY-MM, count)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A5 - Monthly Volumes (US vs CANADA)\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Broadcast dimensions\n",
    "    if use_parquet:\n",
    "        nation_df = spark.read.parquet(os.path.join(input_path, \"nation\"))\n",
    "        nation_rdd = nation_df.rdd\n",
    "        nation_bc = broadcast_dimension(spark, nation_rdd, lambda row: row.n_nationkey)\n",
    "        \n",
    "        customer_df = spark.read.parquet(os.path.join(input_path, \"customer\"))\n",
    "        customer_rdd = customer_df.rdd\n",
    "        customer_bc = broadcast_dimension(spark, customer_rdd, lambda row: row.c_custkey)\n",
    "        \n",
    "        lineitem_rdd = spark.read.parquet(os.path.join(input_path, \"lineitem\")).rdd\n",
    "        orders_rdd = spark.read.parquet(os.path.join(input_path, \"orders\")).rdd\n",
    "        \n",
    "    else:\n",
    "        nation_rdd = sc.textFile(os.path.join(input_path, \"nation.tbl\")).map(parse_nation)\n",
    "        nation_bc = broadcast_dimension(spark, nation_rdd, lambda t: t[0])\n",
    "        \n",
    "        customer_rdd = sc.textFile(os.path.join(input_path, \"customer.tbl\")).map(parse_customer)\n",
    "        customer_bc = broadcast_dimension(spark, customer_rdd, lambda t: t[0])\n",
    "        \n",
    "        lineitem_rdd = sc.textFile(os.path.join(input_path, \"lineitem.tbl\")).map(parse_lineitem)\n",
    "        orders_rdd = sc.textFile(os.path.join(input_path, \"orders.tbl\")).map(parse_orders)\n",
    "    \n",
    "    # Filter US and CANADA\n",
    "    us_canada_nations = nation_bc.value\n",
    "    us_canada_keys = [k for k, v in us_canada_nations.items() \n",
    "                      if (v.n_name if use_parquet else v[1]) in ['UNITED STATES', 'CANADA']]\n",
    "    \n",
    "    # Join lineitem+orders (reduce-side)\n",
    "    if use_parquet:\n",
    "        lineitem_keyed = lineitem_rdd.map(lambda row: (row.l_orderkey, (row.l_shipdate,)))\n",
    "        orders_keyed = orders_rdd.map(lambda row: (row.o_orderkey, row.o_custkey))\n",
    "    else:\n",
    "        lineitem_keyed = lineitem_rdd.map(lambda t: (t[0], (t[10],)))  # (orderkey, shipdate)\n",
    "        orders_keyed = orders_rdd.map(lambda t: (t[0], t[1]))  # (orderkey, custkey)\n",
    "    \n",
    "    joined = lineitem_keyed.cogroup(orders_keyed) \\\n",
    "        .flatMap(lambda kv: [(shipdate, custkey) \n",
    "                             for lineitem_list in [list(kv[1][0])] \n",
    "                             for orders_list in [list(kv[1][1])] \n",
    "                             for shipdate, in lineitem_list \n",
    "                             for custkey in orders_list])\n",
    "    \n",
    "    # Map to nation via customer broadcast\n",
    "    def map_to_nation_month(item):\n",
    "        shipdate, custkey = item\n",
    "        customer = customer_bc.value.get(custkey)\n",
    "        if customer:\n",
    "            nationkey = customer.c_nationkey if use_parquet else customer[3]\n",
    "            if nationkey in us_canada_keys:\n",
    "                nation = nation_bc.value.get(nationkey)\n",
    "                if nation:\n",
    "                    n_name = nation.n_name if use_parquet else nation[1]\n",
    "                    year_month = shipdate[:7]  # YYYY-MM\n",
    "                    return (nationkey, n_name, year_month)\n",
    "        return None\n",
    "    \n",
    "    nations_months = joined.map(map_to_nation_month).filter(lambda x: x is not None)\n",
    "    \n",
    "    # Aggregate\n",
    "    counts = nations_months.map(lambda kv: (kv, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .map(lambda kv: (kv[0][0], kv[0][1], kv[0][2], kv[1])) \\\n",
    "        .sortBy(lambda t: (t[0], t[2]))  # Sort by nationkey, year-month\n",
    "    \n",
    "    results = counts.collect()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Output\n",
    "    print(f\"\\nResults: {len(results)} monthly volumes\\n\")\n",
    "    \n",
    "    for nationkey, name, year_month, count in results[:20]:\n",
    "        print(f\"  {nationkey} | {name:<15} | {year_month} | {count:>6}\")\n",
    "    \n",
    "    if len(results) > 20:\n",
    "        print(f\"  ... ({len(results) - 20} more rows)\")\n",
    "    \n",
    "    print(f\"\\nDuration: {duration:.2f}s\\n\")\n",
    "    \n",
    "    # Save output\n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a5_result_{format_suffix}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A5 - Monthly Volumes (US vs CANADA)\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\\n\")\n",
    "        \n",
    "        for nationkey, name, year_month, count in results:\n",
    "            f.write(f\"{nationkey},{name},{year_month},{count}\\n\")\n",
    "    \n",
    "    print(f\"Saved: {output_path}\\n\")\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A5',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'duration_s': round(duration, 2),\n",
    "        'num_results': len(results)\n",
    "    }\n",
    "    \n",
    "    return results, duration, metrics\n",
    "\n",
    "# Run A5\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running A5 (TXT and Parquet)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_txt, duration_txt, metrics_txt = run_a5_monthly_volumes(INPUT_TXT_PATH, use_parquet=False)\n",
    "results_parquet, duration_parquet, metrics_parquet = run_a5_monthly_volumes(INPUT_PARQUET_PATH, use_parquet=True)\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"A5 - TXT vs Parquet Comparison\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"| Metric    | TXT       | Parquet   | Speedup |\")\n",
    "print(f\"|-----------|-----------|-----------|---------|\")\n",
    "print(f\"| Duration  | {duration_txt:.2f}s     | {duration_parquet:.2f}s     | {duration_txt/duration_parquet:.2f}x    |\")\n",
    "print(f\"| Results   | {len(results_txt)}        | {len(results_parquet)}        | -       |\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Update metrics log\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    with open(metrics_log_path, 'r') as rf:\n",
    "        run_id = len(list(csv.reader(rf)))\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id, 'A5', 'TXT', 'full-warehouse', 4, 'N/A', duration_txt,\n",
    "        'N/A', 'N/A', 'monthly volumes US+CANADA, cogroup lineitem+orders, broadcast customer+nation', timestamp\n",
    "    ])\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id + 1, 'A5', 'Parquet', 'full-warehouse', 'N/A', 'N/A', duration_parquet,\n",
    "        'N/A', 'N/A', 'same query with Parquet', timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"Updated {metrics_log_path}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Section 7 - Task A5 Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99324dd",
   "metadata": {},
   "source": [
    "### A6 ‚Äî Q6: Pricing Summary (filtered by DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af9d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running A6 (TXT and Parquet)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task A6 - Pricing Summary Report\n",
      "Date: 1996-01-01\n",
      "Format: TXT\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered lineitem: 266 records\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "A6 Results - Pricing Summary\n",
      "============================================================\n",
      "\n",
      "Date            : 1996-01-01\n",
      "Format          : TXT\n",
      "Duration        : 7.45s\n",
      "Count Orders    : 266\n",
      "\n",
      "Sum Qty         : 7,050.00\n",
      "Sum Base Price  : $9,670,322.45\n",
      "Sum Disc Price  : $9,158,295.64\n",
      "Sum Charge      : $9,535,800.64\n",
      "\n",
      "Avg Qty         : 26.50\n",
      "Avg Price       : $36354.60\n",
      "Avg Discount    : 0.0527\n",
      "\n",
      "============================================================\n",
      "\n",
      "Saved: outputs/a6_result_txt_19960101.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A6 - Pricing Summary Report\n",
      "Date: 1996-01-01\n",
      "Format: Parquet\n",
      "============================================================\n",
      "\n",
      "Saved plan: proof/plan_a6_lineitem_parquet_19960101.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered lineitem: 266 records\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:================================>                        (4 + 3) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "A6 Results - Pricing Summary\n",
      "============================================================\n",
      "\n",
      "Date            : 1996-01-01\n",
      "Format          : Parquet\n",
      "Duration        : 11.16s\n",
      "Count Orders    : 266\n",
      "\n",
      "Sum Qty         : 7,050.00\n",
      "Sum Base Price  : $9,670,322.45\n",
      "Sum Disc Price  : $9,158,295.64\n",
      "Sum Charge      : $9,535,800.64\n",
      "\n",
      "Avg Qty         : 26.50\n",
      "Avg Price       : $36354.60\n",
      "Avg Discount    : 0.0527\n",
      "\n",
      "============================================================\n",
      "\n",
      "Saved: outputs/a6_result_parquet_19960101.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A6 - TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "| Metric       | TXT       | Parquet   | Speedup |\n",
      "|--------------|-----------|-----------|---------|\n",
      "| Duration     | 7.45s     | 11.16s     | 0.67x    |\n",
      "| Count Orders | 266       | 266       | -       |\n",
      "| Sum Charge   | $9,535,800.64 | $9,535,800.64 | -       |\n",
      "\n",
      "Validation: Results match (difference < $0.01)\n",
      "\n",
      "============================================================\n",
      "\n",
      "Updated proof/lab_metrics_log.csv\n",
      "\n",
      "============================================================\n",
      "Section 8 - Task A6 Complete\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# implement sums/averages over lineitem for given date\n",
    "# emit tuples per (l_returnflag, l_linestatus, ...)\n",
    "# ============================================================\n",
    "# Section 8 - Part A: Task A6 - Pricing Summary Report\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data\"\n",
    "TPCH_DIR = os.path.join(DATA_DIR, \"tpch\")\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "TARGET_DATE = \"1996-01-01\"\n",
    "\n",
    "def run_a6_pricing_summary(input_path, target_date, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A6 (Q6) - Modified TPC-H Q1 Pricing Summary Report\n",
    "    \n",
    "    Filter: l_shipdate = target_date\n",
    "    Compute:\n",
    "    - sum_qty = SUM(l_quantity)\n",
    "    - sum_base_price = SUM(l_extendedprice)\n",
    "    - sum_disc_price = SUM(l_extendedprice * (1 - l_discount))\n",
    "    - sum_charge = SUM(l_extendedprice * (1 - l_discount) * (1 + l_tax))\n",
    "    - avg_qty = AVG(l_quantity)\n",
    "    - avg_price = AVG(l_extendedprice)\n",
    "    - avg_disc = AVG(l_discount)\n",
    "    - count_order = COUNT(*)\n",
    "    \n",
    "    Strategy: Single pass with combiners (in-mapper combining pattern)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A6 - Pricing Summary Report\")\n",
    "    print(f\"Date: {target_date}\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load lineitem\n",
    "    if use_parquet:\n",
    "        lineitem_df = spark.read.parquet(os.path.join(input_path, \"lineitem\"))\n",
    "        \n",
    "        # Save formatted plan\n",
    "        plan_path = f\"proof/plan_a6_lineitem_parquet_{target_date.replace('-', '')}.txt\"\n",
    "        os.makedirs(\"proof\", exist_ok=True)\n",
    "        \n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A6 Lineitem Parquet Load Plan ({target_date}) ===\\n\\n\")\n",
    "            f.write(lineitem_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        \n",
    "        print(f\"Saved plan: {plan_path}\")\n",
    "        \n",
    "        lineitem_rdd = lineitem_df.rdd\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda row: row.l_shipdate == target_date)\n",
    "        \n",
    "    else:\n",
    "        lineitem_rdd = sc.textFile(os.path.join(input_path, \"lineitem.tbl\")).map(parse_lineitem)\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda t: t[10] == target_date)\n",
    "    \n",
    "    filtered_count = lineitem_filtered.count()\n",
    "    print(f\"Filtered lineitem: {filtered_count} records\\n\")\n",
    "    \n",
    "    # Aggregation with combiners\n",
    "    def map_to_aggregates(row):\n",
    "        if use_parquet:\n",
    "            qty = float(row.l_quantity)\n",
    "            price = float(row.l_extendedprice)\n",
    "            disc = float(row.l_discount)\n",
    "            tax = float(row.l_tax)\n",
    "        else:\n",
    "            qty = float(row[4])\n",
    "            price = float(row[5])\n",
    "            disc = float(row[6])\n",
    "            tax = float(row[7])\n",
    "        \n",
    "        disc_price = price * (1 - disc)\n",
    "        charge = disc_price * (1 + tax)\n",
    "        \n",
    "        return (\n",
    "            qty,\n",
    "            price,\n",
    "            disc_price,\n",
    "            charge,\n",
    "            qty,\n",
    "            price,\n",
    "            disc,\n",
    "            1\n",
    "        )\n",
    "    \n",
    "    def combine_aggregates(agg1, agg2):\n",
    "        return tuple(a + b for a, b in zip(agg1, agg2))\n",
    "    \n",
    "    aggregates = lineitem_filtered \\\n",
    "        .map(map_to_aggregates) \\\n",
    "        .reduce(combine_aggregates)\n",
    "    \n",
    "    sum_qty, sum_base_price, sum_disc_price, sum_charge, \\\n",
    "        total_qty, total_price, total_disc, count_order = aggregates\n",
    "    \n",
    "    avg_qty = total_qty / count_order if count_order > 0 else 0\n",
    "    avg_price = total_price / count_order if count_order > 0 else 0\n",
    "    avg_disc = total_disc / count_order if count_order > 0 else 0\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"A6 Results - Pricing Summary\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"Date            : {target_date}\")\n",
    "    print(f\"Format          : {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"Duration        : {duration:.2f}s\")\n",
    "    print(f\"Count Orders    : {count_order}\\n\")\n",
    "    \n",
    "    print(f\"Sum Qty         : {sum_qty:,.2f}\")\n",
    "    print(f\"Sum Base Price  : ${sum_base_price:,.2f}\")\n",
    "    print(f\"Sum Disc Price  : ${sum_disc_price:,.2f}\")\n",
    "    print(f\"Sum Charge      : ${sum_charge:,.2f}\\n\")\n",
    "    \n",
    "    print(f\"Avg Qty         : {avg_qty:.2f}\")\n",
    "    print(f\"Avg Price       : ${avg_price:.2f}\")\n",
    "    print(f\"Avg Discount    : {avg_disc:.4f}\\n\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Save output\n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a6_result_{format_suffix}_{target_date.replace('-', '')}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A6 - Pricing Summary Report\\n\")\n",
    "        f.write(f\"Date: {target_date}\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\\n\")\n",
    "        f.write(f\"count_order,sum_qty,sum_base_price,sum_disc_price,sum_charge,avg_qty,avg_price,avg_disc\\n\")\n",
    "        f.write(f\"{count_order},{sum_qty:.2f},{sum_base_price:.2f},{sum_disc_price:.2f},{sum_charge:.2f},{avg_qty:.2f},{avg_price:.2f},{avg_disc:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Saved: {output_path}\\n\")\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A6',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'date': target_date,\n",
    "        'duration_s': round(duration, 2),\n",
    "        'count_order': count_order\n",
    "    }\n",
    "    \n",
    "    return (count_order, sum_qty, sum_base_price, sum_disc_price, sum_charge, \n",
    "            avg_qty, avg_price, avg_disc), duration, metrics\n",
    "\n",
    "# Run A6\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running A6 (TXT and Parquet)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_txt, duration_txt, metrics_txt = run_a6_pricing_summary(INPUT_TXT_PATH, TARGET_DATE, use_parquet=False)\n",
    "results_parquet, duration_parquet, metrics_parquet = run_a6_pricing_summary(INPUT_PARQUET_PATH, TARGET_DATE, use_parquet=True)\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"A6 - TXT vs Parquet Comparison\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"| Metric       | TXT       | Parquet   | Speedup |\")\n",
    "print(f\"|--------------|-----------|-----------|---------|\")\n",
    "print(f\"| Duration     | {duration_txt:.2f}s     | {duration_parquet:.2f}s     | {duration_txt/duration_parquet:.2f}x    |\")\n",
    "print(f\"| Count Orders | {results_txt[0]}       | {results_parquet[0]}       | -       |\")\n",
    "print(f\"| Sum Charge   | ${results_txt[4]:,.2f} | ${results_parquet[4]:,.2f} | -       |\")\n",
    "\n",
    "if abs(results_txt[4] - results_parquet[4]) < 0.01:\n",
    "    print(f\"\\nValidation: Results match (difference < $0.01)\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Results differ by ${abs(results_txt[4] - results_parquet[4]):,.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Update metrics log\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    with open(metrics_log_path, 'r') as rf:\n",
    "        run_id = len(list(csv.reader(rf)))\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id, 'A6', 'TXT', TARGET_DATE, 1, 'N/A', duration_txt,\n",
    "        0, 0, f'pricing summary single-pass combiners, {results_txt[0]} orders', timestamp\n",
    "    ])\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id + 1, 'A6', 'Parquet', TARGET_DATE, 'N/A', 'N/A', duration_parquet,\n",
    "        0, 0, f'same query Parquet', timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"Updated {metrics_log_path}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Section 8 - Task A6 Complete\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf169bb",
   "metadata": {},
   "source": [
    "### A7 ‚Äî Q7: Shipping Priority Top‚Äë10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf20d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running A7 (TXT and Parquet)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Task A7 - Shipping Priority (Modified Q3)\n",
      "Shipdate > 1995-03-15, Orderdate < 1995-03-01\n",
      "Format: TXT\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 15,000 entries\n",
      "Broadcasted customer: 15000 entries\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered lineitem: 324322 records (shipdate > 1995-03-15)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered orders: 71815 records (orderdate < 1995-03-01)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined: 11887 (orderkey, price, disc, custkey, orderdate, priority)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "A7 Results - Top 10 Shipping Priority\n",
      "============================================================\n",
      "\n",
      "Format   : TXT\n",
      "Duration : 15.62s\n",
      "\n",
      " 1. Order: 584291 | Revenue: $  354,494.73 | Date: 1995-02-21 | Priority: 0\n",
      " 2. Order: 132774 | Revenue: $  350,015.70 | Date: 1995-02-27 | Priority: 0\n",
      " 3. Order: 568514 | Revenue: $  348,837.83 | Date: 1995-02-18 | Priority: 0\n",
      " 4. Order: 181793 | Revenue: $  344,198.09 | Date: 1995-02-14 | Priority: 0\n",
      " 5. Order: 306247 | Revenue: $  340,789.15 | Date: 1995-02-17 | Priority: 0\n",
      " 6. Order: 519556 | Revenue: $  323,264.53 | Date: 1995-02-15 | Priority: 0\n",
      " 7. Order: 492164 | Revenue: $  321,305.19 | Date: 1995-02-19 | Priority: 0\n",
      " 8. Order: 311266 | Revenue: $  319,173.77 | Date: 1995-02-17 | Priority: 0\n",
      " 9. Order: 155972 | Revenue: $  315,457.78 | Date: 1995-01-08 | Priority: 0\n",
      "10. Order: 108514 | Revenue: $  314,967.08 | Date: 1995-02-20 | Priority: 0\n",
      "\n",
      "============================================================\n",
      "\n",
      "Saved: outputs/a7_result_txt.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "Task A7 - Shipping Priority (Modified Q3)\n",
      "Shipdate > 1995-03-15, Orderdate < 1995-03-01\n",
      "Format: Parquet\n",
      "============================================================\n",
      "\n",
      "Saved plan: proof/plan_a7_lineitem_parquet.txt\n",
      "Saved plan: proof/plan_a7_orders_parquet.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Broadcasted dimension: 15,000 entries\n",
      "Broadcasted customer: 15000 entries\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered lineitem: 324322 records (shipdate > 1995-03-15)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered orders: 71815 records (orderdate < 1995-03-01)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined: 11887 (orderkey, price, disc, custkey, orderdate, priority)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:>                                                        (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "A7 Results - Top 10 Shipping Priority\n",
      "============================================================\n",
      "\n",
      "Format   : Parquet\n",
      "Duration : 22.98s\n",
      "\n",
      " 1. Order: 584291 | Revenue: $  354,494.73 | Date: 1995-02-21 | Priority: 0\n",
      " 2. Order: 132774 | Revenue: $  350,015.70 | Date: 1995-02-27 | Priority: 0\n",
      " 3. Order: 568514 | Revenue: $  348,837.83 | Date: 1995-02-18 | Priority: 0\n",
      " 4. Order: 181793 | Revenue: $  344,198.09 | Date: 1995-02-14 | Priority: 0\n",
      " 5. Order: 306247 | Revenue: $  340,789.15 | Date: 1995-02-17 | Priority: 0\n",
      " 6. Order: 519556 | Revenue: $  323,264.53 | Date: 1995-02-15 | Priority: 0\n",
      " 7. Order: 492164 | Revenue: $  321,305.19 | Date: 1995-02-19 | Priority: 0\n",
      " 8. Order: 311266 | Revenue: $  319,173.77 | Date: 1995-02-17 | Priority: 0\n",
      " 9. Order: 155972 | Revenue: $  315,457.78 | Date: 1995-01-08 | Priority: 0\n",
      "10. Order: 108514 | Revenue: $  314,967.08 | Date: 1995-02-20 | Priority: 0\n",
      "\n",
      "============================================================\n",
      "\n",
      "Saved: outputs/a7_result_parquet.txt\n",
      "\n",
      "\n",
      "============================================================\n",
      "A7 - TXT vs Parquet Comparison\n",
      "============================================================\n",
      "\n",
      "| Metric       | TXT         | Parquet     | Speedup |\n",
      "|--------------|-------------|-------------|---------|\n",
      "| Duration     | 15.62s       | 22.98s       | 0.68x    |\n",
      "| Top Revenue  | $354,494.73 | $354,494.73 | -       |\n",
      "\n",
      "Validation: Results match (difference < $0.01)\n",
      "\n",
      "============================================================\n",
      "\n",
      "Updated proof/lab_metrics_log.csv\n",
      "\n",
      "============================================================\n",
      "Section 9 - Task A7 Complete\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Part A (A1-A7) - All Tasks Complete\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# join customer, orders, lineitem with appropriate filters and groupBy\n",
    "# compute revenue and order by desc; limit 10\n",
    "# ============================================================\n",
    "# Section 9 - Part A: Task A7 - Shipping Priority (Modified Q3)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data\"\n",
    "TPCH_DIR = os.path.join(DATA_DIR, \"tpch\")\n",
    "INPUT_TXT_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-TXT\")\n",
    "INPUT_PARQUET_PATH = os.path.join(TPCH_DIR, \"TPC-H-0.1-PARQUET\")\n",
    "SHIPDATE_BEFORE = \"1995-03-15\"\n",
    "ORDERDATE_AFTER = \"1995-03-01\"\n",
    "\n",
    "def run_a7_shipping_priority(input_path, shipdate_before, orderdate_after, use_parquet=False):\n",
    "    \"\"\"\n",
    "    A7 (Q7) - Modified TPC-H Q3 Shipping Priority Query\n",
    "    \n",
    "    Find top 10 unshipped orders by revenue where:\n",
    "    - l_shipdate > shipdate_before\n",
    "    - o_orderdate < orderdate_after\n",
    "    \n",
    "    Output: (l_orderkey, revenue, o_orderdate, o_shippriority)\n",
    "    Revenue = SUM(l_extendedprice * (1 - l_discount))\n",
    "    \n",
    "    Strategy:\n",
    "    - Broadcast customer (small)\n",
    "    - Reduce-side join lineitem + orders (both large)\n",
    "    - Aggregate revenue by orderkey\n",
    "    - Sort by revenue DESC, take top 10\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task A7 - Shipping Priority (Modified Q3)\")\n",
    "    print(f\"Shipdate > {shipdate_before}, Orderdate < {orderdate_after}\")\n",
    "    print(f\"Format: {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load tables\n",
    "    if use_parquet:\n",
    "        # Save formatted plans\n",
    "        os.makedirs(\"proof\", exist_ok=True)\n",
    "        \n",
    "        lineitem_df = spark.read.parquet(os.path.join(input_path, \"lineitem\"))\n",
    "        plan_path = f\"proof/plan_a7_lineitem_parquet.txt\"\n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A7 Lineitem Parquet Load Plan ===\\n\\n\")\n",
    "            f.write(lineitem_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        print(f\"Saved plan: {plan_path}\")\n",
    "        \n",
    "        orders_df = spark.read.parquet(os.path.join(input_path, \"orders\"))\n",
    "        plan_path = f\"proof/plan_a7_orders_parquet.txt\"\n",
    "        with open(plan_path, 'w') as f:\n",
    "            f.write(f\"=== A7 Orders Parquet Load Plan ===\\n\\n\")\n",
    "            f.write(orders_df._jdf.queryExecution().explainString(\n",
    "                spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "            ))\n",
    "        print(f\"Saved plan: {plan_path}\\n\")\n",
    "        \n",
    "        customer_df = spark.read.parquet(os.path.join(input_path, \"customer\"))\n",
    "        customer_rdd = customer_df.rdd\n",
    "        customer_bc = broadcast_dimension(spark, customer_rdd, lambda row: row.c_custkey)\n",
    "        \n",
    "        lineitem_rdd = lineitem_df.rdd\n",
    "        orders_rdd = orders_df.rdd\n",
    "        \n",
    "    else:\n",
    "        customer_rdd = sc.textFile(os.path.join(input_path, \"customer.tbl\")).map(parse_customer)\n",
    "        customer_bc = broadcast_dimension(spark, customer_rdd, lambda t: t[0])\n",
    "        \n",
    "        lineitem_rdd = sc.textFile(os.path.join(input_path, \"lineitem.tbl\")).map(parse_lineitem)\n",
    "        orders_rdd = sc.textFile(os.path.join(input_path, \"orders.tbl\")).map(parse_orders)\n",
    "    \n",
    "    print(f\"Broadcasted customer: {customer_rdd.count()} entries\\n\")\n",
    "    \n",
    "    # Filter lineitem: l_shipdate > shipdate_before\n",
    "    if use_parquet:\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda row: row.l_shipdate > shipdate_before)\n",
    "    else:\n",
    "        lineitem_filtered = lineitem_rdd.filter(lambda t: t[10] > shipdate_before)\n",
    "    \n",
    "    print(f\"Filtered lineitem: {lineitem_filtered.count()} records (shipdate > {shipdate_before})\\n\")\n",
    "    \n",
    "    # Filter orders: o_orderdate < orderdate_after\n",
    "    if use_parquet:\n",
    "        orders_filtered = orders_rdd.filter(lambda row: row.o_orderdate < orderdate_after)\n",
    "    else:\n",
    "        orders_filtered = orders_rdd.filter(lambda t: t[4] < orderdate_after)\n",
    "    \n",
    "    print(f\"Filtered orders: {orders_filtered.count()} records (orderdate < {orderdate_after})\\n\")\n",
    "    \n",
    "    # Reduce-side join lineitem + orders\n",
    "    if use_parquet:\n",
    "        lineitem_keyed = lineitem_filtered.map(lambda row: (\n",
    "            row.l_orderkey,\n",
    "            (float(row.l_extendedprice), float(row.l_discount))\n",
    "        ))\n",
    "        orders_keyed = orders_filtered.map(lambda row: (\n",
    "            row.o_orderkey,\n",
    "            (row.o_custkey, row.o_orderdate, row.o_shippriority)\n",
    "        ))\n",
    "    else:\n",
    "        lineitem_keyed = lineitem_filtered.map(lambda t: (\n",
    "            t[0],\n",
    "            (float(t[5]), float(t[6]))\n",
    "        ))\n",
    "        orders_keyed = orders_filtered.map(lambda t: (\n",
    "            t[0],\n",
    "            (t[1], t[4], t[7])\n",
    "        ))\n",
    "    \n",
    "    # Cogroup join\n",
    "    joined = lineitem_keyed.cogroup(orders_keyed) \\\n",
    "        .flatMap(lambda kv: [\n",
    "            (kv[0], price, disc, custkey, orderdate, shippriority)\n",
    "            for lineitem_list in [list(kv[1][0])]\n",
    "            for orders_list in [list(kv[1][1])]\n",
    "            for price, disc in lineitem_list\n",
    "            for custkey, orderdate, shippriority in orders_list\n",
    "        ])\n",
    "    \n",
    "    print(f\"Joined: {joined.count()} (orderkey, price, disc, custkey, orderdate, priority)\\n\")\n",
    "    \n",
    "    # Aggregate revenue by orderkey\n",
    "    def compute_revenue(item):\n",
    "        orderkey, price, disc, custkey, orderdate, shippriority = item\n",
    "        revenue = price * (1 - disc)\n",
    "        return ((orderkey, orderdate, shippriority), revenue)\n",
    "    \n",
    "    revenue_by_order = joined \\\n",
    "        .map(compute_revenue) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .map(lambda kv: (kv[0][0], kv[1], kv[0][1], kv[0][2]))\n",
    "    \n",
    "    # Sort by revenue DESC, take top 10\n",
    "    top_10 = revenue_by_order \\\n",
    "        .sortBy(lambda t: t[1], ascending=False) \\\n",
    "        .take(10)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Output\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"A7 Results - Top 10 Shipping Priority\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"Format   : {'Parquet' if use_parquet else 'TXT'}\")\n",
    "    print(f\"Duration : {duration:.2f}s\\n\")\n",
    "    \n",
    "    for i, (orderkey, revenue, orderdate, shippriority) in enumerate(top_10, 1):\n",
    "        print(f\"{i:2d}. Order: {orderkey:>6} | Revenue: ${revenue:>12,.2f} | Date: {orderdate} | Priority: {shippriority}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    # Save output\n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    format_suffix = \"parquet\" if use_parquet else \"txt\"\n",
    "    output_path = os.path.join(output_dir, f\"a7_result_{format_suffix}.txt\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(f\"Task: A7 - Shipping Priority (Modified Q3)\\n\")\n",
    "        f.write(f\"Shipdate > {shipdate_before}, Orderdate < {orderdate_after}\\n\")\n",
    "        f.write(f\"Format: {'Parquet' if use_parquet else 'TXT'}\\n\")\n",
    "        f.write(f\"Duration: {duration:.2f}s\\n\\n\")\n",
    "        f.write(f\"l_orderkey,revenue,o_orderdate,o_shippriority\\n\")\n",
    "        \n",
    "        for orderkey, revenue, orderdate, shippriority in top_10:\n",
    "            f.write(f\"{orderkey},{revenue:.2f},{orderdate},{shippriority}\\n\")\n",
    "    \n",
    "    print(f\"Saved: {output_path}\\n\")\n",
    "    \n",
    "    metrics = {\n",
    "        'task': 'A7',\n",
    "        'format': 'Parquet' if use_parquet else 'TXT',\n",
    "        'duration_s': round(duration, 2),\n",
    "        'top_revenue': round(top_10[0][1], 2) if top_10 else 0\n",
    "    }\n",
    "    \n",
    "    return top_10, duration, metrics\n",
    "\n",
    "# Run A7\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running A7 (TXT and Parquet)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_txt, duration_txt, metrics_txt = run_a7_shipping_priority(\n",
    "    INPUT_TXT_PATH, SHIPDATE_BEFORE, ORDERDATE_AFTER, use_parquet=False\n",
    ")\n",
    "\n",
    "results_parquet, duration_parquet, metrics_parquet = run_a7_shipping_priority(\n",
    "    INPUT_PARQUET_PATH, SHIPDATE_BEFORE, ORDERDATE_AFTER, use_parquet=True\n",
    ")\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"A7 - TXT vs Parquet Comparison\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"| Metric       | TXT         | Parquet     | Speedup |\")\n",
    "print(f\"|--------------|-------------|-------------|---------|\")\n",
    "print(f\"| Duration     | {duration_txt:.2f}s       | {duration_parquet:.2f}s       | {duration_txt/duration_parquet:.2f}x    |\")\n",
    "if results_txt and results_parquet:\n",
    "    print(f\"| Top Revenue  | ${results_txt[0][1]:,.2f} | ${results_parquet[0][1]:,.2f} | -       |\")\n",
    "    \n",
    "    if abs(results_txt[0][1] - results_parquet[0][1]) < 0.01:\n",
    "        print(f\"\\nValidation: Results match (difference < $0.01)\")\n",
    "    else:\n",
    "        print(f\"\\nWARNING: Results differ by ${abs(results_txt[0][1] - results_parquet[0][1]):,.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Update metrics log\n",
    "metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "\n",
    "with open(metrics_log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    with open(metrics_log_path, 'r') as rf:\n",
    "        run_id = len(list(csv.reader(rf)))\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    top_rev_txt = round(results_txt[0][1], 2) if results_txt else 0\n",
    "    top_rev_pq = round(results_parquet[0][1], 2) if results_parquet else 0\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id, 'A7', 'TXT', f'shipdate>{SHIPDATE_BEFORE}', 3, 'N/A', duration_txt,\n",
    "        'N/A', 'N/A', f'cogroup lineitem+orders, broadcast customer, top 10 revenue ${top_rev_txt:,.2f}', timestamp\n",
    "    ])\n",
    "    \n",
    "    writer.writerow([\n",
    "        run_id + 1, 'A7', 'Parquet', f'shipdate>{SHIPDATE_BEFORE}', 'N/A', 'N/A', duration_parquet,\n",
    "        'N/A', 'N/A', f'same query Parquet, top revenue ${top_rev_pq:,.2f}', timestamp\n",
    "    ])\n",
    "\n",
    "print(f\"Updated {metrics_log_path}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Section 9 - Task A7 Complete\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part A (A1-A7) - All Tasks Complete\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f374e",
   "metadata": {},
   "source": [
    "## Evidence for Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a791ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# capture DF explain('formatted') when using parquet readers\n",
    "# collect timings and notes TXT vs PARQUET; broadcast vs reduce-side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26caa",
   "metadata": {},
   "source": [
    "## Part B ‚Äî Streaming (Structured Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d780fc9",
   "metadata": {},
   "source": [
    "### B1 ‚Äî HourlyTripCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a15bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part B - Streaming Analytics (NYC Taxi)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Section 10 - Part B: Task B1 - Hourly Trip Count\n",
      "============================================================\n",
      "\n",
      "Step 1: Verifying NYC Taxi data...\n",
      "\n",
      "‚úÖ Taxi data already extracted (2880 CSV files)\n",
      "‚úÖ Found 1440 CSV files\n",
      "Sample: ['part-2015-12-01-0000.csv', 'part-2015-12-01-0001.csv', 'part-2015-12-01-0002.csv']\n",
      "\n",
      "Step 2: Inspecting CSV schema...\n",
      "\n",
      "Inspecting: data/taxi-data/part-2015-12-01-0000.csv\n",
      "Columns detected: 20\n",
      "First 10 values: ['yellow', '2', '2015-12-01 00:00:00', '2015-12-01 00:00:00', '2', '2.69', '-73.972335815429687', '40.762378692626953', '1', 'N']\n",
      "Sample row: yellow,2,2015-12-01 00:00:00,2015-12-01 00:00:00,2,2.69,-73.972335815429687,40.762378692626953,1,N,-73.993629455566406,40.745998382568359,1,21.5,0,0.5,3.34,0,0.3,25.64...\n",
      "\n",
      "Step 3: Defining NYC Taxi Yellow Cab 2015 schema (20 columns)...\n",
      "\n",
      "Schema: 20 columns (vendor_id ‚Üí congestion_surcharge)\n",
      "pickup_datetime at position 1 (correct for Yellow Cab 2015 format)\n",
      "\n",
      "Step 4: Configuring Structured Streaming...\n",
      "\n",
      "Input: data/taxi-data\n",
      "Checkpoint: checkpoint/b1_hourly\n",
      "Output: output/b1_hourly\n",
      "Mode: PERMISSIVE\n",
      "\n",
      "‚úÖ Stream configured\n",
      "\n",
      "Step 5: Building aggregation pipeline...\n",
      "\n",
      "Pipeline:\n",
      "  1. Read CSV (20 columns, PERMISSIVE)\n",
      "  2. Parse pickup_datetime (position 1) ‚Üí pickup_ts\n",
      "  3. Filter NULL timestamps\n",
      "  4. Watermark: 10 minutes\n",
      "  5. Window: 1 hour\n",
      "  6. Count(*)\n",
      "\n",
      "Step 6: Saving plan...\n",
      "\n",
      "‚úÖ Saved: proof/plan_b1_hourly_stream.txt\n",
      "\n",
      "Step 7: Starting streaming query...\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 11:30:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 2be3e1cc-fcdf-458a-b713-e255fa852b07\n",
      "Status: Initializing sources\n",
      "\n",
      "Step 8: Processing batches (60s)...\n",
      "\n",
      "Batch    InputRows    ProcessedRows/s    InputRate       Duration (ms)  \n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 11:30:27 WARN FileStreamSource: Listed 1440 file(s) in 26551 ms\n",
      "25/12/07 11:30:31 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 29430 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        1            0.44               0.03            2259           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2        11           3.39               1.23            3243           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3        19           7.00               1.90            2716           \n",
      "\n",
      "============================================================\n",
      "Batches: 3 | Input rows: 31 | Duration: 59.22s\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 11:31:00 WARN DAGScheduler: Failed to cancel job group 209bb312-fa77-4034-bebe-ee308109630f. Cannot find active jobs for it.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query stopped\n",
      "\n",
      "Step 9: Reading results...\n",
      "\n",
      "============================================================\n",
      "B1 Results - Hourly Trip Counts\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 11:31:03 WARN DAGScheduler: Failed to cancel job group 209bb312-fa77-4034-bebe-ee308109630f. Cannot find active jobs for it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Summary:\n",
      "  Windows: 0 | Trips: 0 | Duration: 59.22s\n",
      "\n",
      "+------------+----------+----------+\n",
      "|window_start|window_end|trip_count|\n",
      "+------------+----------+----------+\n",
      "+------------+----------+----------+\n",
      "\n",
      "\n",
      "‚úÖ Saved: outputs/b1_hourly_trip_count.txt\n",
      "\n",
      "‚úÖ Updated: proof/lab_metrics_log.csv\n",
      "\n",
      "============================================================\n",
      "Section 10 - Task B1 Complete\n",
      "============================================================\n",
      "\n",
      "üìã Evidence Generated:\n",
      "  ‚úÖ proof/plan_b1_hourly_stream.txt\n",
      "  ‚úÖ outputs/b1_hourly_trip_count.txt\n",
      "  ‚úÖ proof/lab_metrics_log.csv\n",
      "  ‚úÖ output/b1_hourly/*.parquet (5 files)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# readStream from data/taxi-data (file source with schema)\n",
    "# withWatermark if needed; window='1 hour'; count\n",
    "# writeStream with checkpoint dir and output dir\n",
    "# ============================================================\n",
    "# Part B - Streaming Analytics (NYC Taxi)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part B - Streaming Analytics (NYC Taxi)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Section 10 - Part B: Task B1 - Hourly Trip Count\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import window, count, col, to_timestamp, expr\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Section 10 - Part B: Task B1 - Hourly Trip Count\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data\"\n",
    "TAXI_ARCHIVE = os.path.join(DATA_DIR, \"taxi-data.tar.gz\")\n",
    "TAXI_DIR = os.path.join(DATA_DIR, \"taxi-data\")\n",
    "CHECKPOINT_DIR = \"checkpoint/b1_hourly\"\n",
    "OUTPUT_DIR = \"output/b1_hourly\"\n",
    "\n",
    "# Clean previous runs\n",
    "for path in [CHECKPOINT_DIR, OUTPUT_DIR]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Step 1: Extract and verify\n",
    "print(\"Step 1: Verifying NYC Taxi data...\\n\")\n",
    "\n",
    "if not os.path.exists(TAXI_DIR):\n",
    "    os.makedirs(TAXI_DIR, exist_ok=True)\n",
    "\n",
    "taxi_files_before = []\n",
    "for root, dirs, files in os.walk(TAXI_DIR):\n",
    "    taxi_files_before.extend([os.path.join(root, f) for f in files if f.endswith('.csv')])\n",
    "\n",
    "if len(taxi_files_before) > 0:\n",
    "    print(f\"‚úÖ Taxi data already extracted ({len(taxi_files_before)} CSV files)\")\n",
    "    for src in taxi_files_before:\n",
    "        if os.path.dirname(src) != TAXI_DIR:\n",
    "            dst = os.path.join(TAXI_DIR, os.path.basename(src))\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "elif os.path.exists(TAXI_ARCHIVE):\n",
    "    print(f\"Extracting {TAXI_ARCHIVE}...\")\n",
    "    with tarfile.open(TAXI_ARCHIVE, 'r:gz') as tar:\n",
    "        tar.extractall(path=TAXI_DIR)\n",
    "    \n",
    "    taxi_files_after = []\n",
    "    for root, dirs, files in os.walk(TAXI_DIR):\n",
    "        for f in files:\n",
    "            if f.endswith('.csv'):\n",
    "                full_path = os.path.join(root, f)\n",
    "                taxi_files_after.append(full_path)\n",
    "                if root != TAXI_DIR:\n",
    "                    dst = os.path.join(TAXI_DIR, f)\n",
    "                    if not os.path.exists(dst):\n",
    "                        shutil.move(full_path, dst)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(taxi_files_after)} CSV files\\n\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Archive not found: {TAXI_ARCHIVE}\")\n",
    "\n",
    "taxi_files = sorted([f for f in os.listdir(TAXI_DIR) if f.endswith('.csv')])\n",
    "\n",
    "if len(taxi_files) == 0:\n",
    "    raise FileNotFoundError(f\"No CSV files in {TAXI_DIR}\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(taxi_files)} CSV files\")\n",
    "print(f\"Sample: {taxi_files[:3]}\\n\")\n",
    "\n",
    "# Step 2: Inspect actual CSV structure\n",
    "print(\"Step 2: Inspecting CSV schema...\\n\")\n",
    "\n",
    "sample_file = os.path.join(TAXI_DIR, taxi_files[0])\n",
    "print(f\"Inspecting: {sample_file}\")\n",
    "\n",
    "with open(sample_file, 'r') as f:\n",
    "    first_line = f.readline().strip()\n",
    "    columns = first_line.split(',')\n",
    "    print(f\"Columns detected: {len(columns)}\")\n",
    "    print(f\"First 10 values: {columns[:10]}\")\n",
    "    print(f\"Sample row: {first_line[:200]}...\\n\")\n",
    "\n",
    "# Step 3: Define correct schema for NYC Taxi Yellow Cab 2015 (20 columns)\n",
    "print(\"Step 3: Defining NYC Taxi Yellow Cab 2015 schema (20 columns)...\\n\")\n",
    "\n",
    "# NYC Yellow Cab 2015 schema (20 columns)\n",
    "taxi_schema = StructType([\n",
    "    StructField(\"vendor_id\", StringType(), True),           # 0\n",
    "    StructField(\"pickup_datetime\", StringType(), True),     # 1 ‚Üê CORRECT POSITION\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),    # 2\n",
    "    StructField(\"passenger_count\", IntegerType(), True),    # 3\n",
    "    StructField(\"trip_distance\", DoubleType(), True),       # 4\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),    # 5\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),     # 6\n",
    "    StructField(\"rate_code\", IntegerType(), True),          # 7\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),  # 8\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),   # 9\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),    # 10\n",
    "    StructField(\"payment_type\", IntegerType(), True),       # 11\n",
    "    StructField(\"fare_amount\", DoubleType(), True),         # 12\n",
    "    StructField(\"surcharge\", DoubleType(), True),           # 13\n",
    "    StructField(\"mta_tax\", DoubleType(), True),             # 14\n",
    "    StructField(\"tip_amount\", DoubleType(), True),          # 15\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),        # 16\n",
    "    StructField(\"total_amount\", DoubleType(), True),        # 17\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True), # 18\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)  # 19\n",
    "])\n",
    "\n",
    "print(\"Schema: 20 columns (vendor_id ‚Üí congestion_surcharge)\")\n",
    "print(\"pickup_datetime at position 1 (correct for Yellow Cab 2015 format)\\n\")\n",
    "\n",
    "# Step 4: Configure Structured Streaming\n",
    "print(\"Step 4: Configuring Structured Streaming...\\n\")\n",
    "\n",
    "print(f\"Input: {TAXI_DIR}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Mode: PERMISSIVE\\n\")\n",
    "\n",
    "taxi_stream = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(taxi_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(TAXI_DIR)\n",
    "\n",
    "print(\"‚úÖ Stream configured\\n\")\n",
    "\n",
    "# Step 5: Parse timestamps and build aggregation\n",
    "print(\"Step 5: Building aggregation pipeline...\\n\")\n",
    "\n",
    "# Parse pickup_datetime (now correctly at position 1)\n",
    "taxi_stream_clean = taxi_stream \\\n",
    "    .withColumn(\"pickup_ts\", expr(\"try_to_timestamp(pickup_datetime, 'yyyy-MM-dd HH:mm:ss')\")) \\\n",
    "    .filter(col(\"pickup_ts\").isNotNull())\n",
    "\n",
    "# B1: Hourly counts\n",
    "hourly_counts = taxi_stream_clean \\\n",
    "    .withWatermark(\"pickup_ts\", \"10 minutes\") \\\n",
    "    .groupBy(window(col(\"pickup_ts\"), \"1 hour\")) \\\n",
    "    .agg(count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "hourly_output = hourly_counts.select(\n",
    "    col(\"window.start\").alias(\"window_start\"),\n",
    "    col(\"window.end\").alias(\"window_end\"),\n",
    "    col(\"trip_count\")\n",
    ")\n",
    "\n",
    "print(\"Pipeline:\")\n",
    "print(\"  1. Read CSV (20 columns, PERMISSIVE)\")\n",
    "print(\"  2. Parse pickup_datetime (position 1) ‚Üí pickup_ts\")\n",
    "print(\"  3. Filter NULL timestamps\")\n",
    "print(\"  4. Watermark: 10 minutes\")\n",
    "print(\"  5. Window: 1 hour\")\n",
    "print(\"  6. Count(*)\\n\")\n",
    "\n",
    "# Step 6: Save plan\n",
    "print(\"Step 6: Saving plan...\\n\")\n",
    "\n",
    "plan_path = \"proof/plan_b1_hourly_stream.txt\"\n",
    "os.makedirs(\"proof\", exist_ok=True)\n",
    "\n",
    "with open(plan_path, 'w') as f:\n",
    "    f.write(\"=== B1 Hourly Trip Count Stream Plan ===\\n\\n\")\n",
    "    f.write(\"Dataset: NYC Yellow Cab 2015 (20 columns)\\n\")\n",
    "    f.write(\"Schema: vendor_id, pickup_datetime, ..., congestion_surcharge\\n\")\n",
    "    f.write(\"Timestamp column: pickup_datetime (position 1)\\n\")\n",
    "    f.write(\"Window: 1 hour tumbling\\n\")\n",
    "    f.write(\"Watermark: 10 minutes\\n\")\n",
    "    f.write(\"Output Mode: append\\n\\n\")\n",
    "    f.write(hourly_output._jdf.queryExecution().explainString(\n",
    "        spark._jvm.org.apache.spark.sql.execution.ExplainMode.fromString(\"formatted\")\n",
    "    ))\n",
    "\n",
    "print(f\"‚úÖ Saved: {plan_path}\\n\")\n",
    "\n",
    "# Step 7: Start streaming query\n",
    "print(\"Step 7: Starting streaming query...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query_b1 = hourly_output.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", OUTPUT_DIR) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIR) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(f\"Query ID: {query_b1.id}\")\n",
    "print(f\"Status: {query_b1.status['message']}\\n\")\n",
    "\n",
    "# Step 8: Monitor batches\n",
    "print(\"Step 8: Processing batches (60s)...\\n\")\n",
    "print(f\"{'Batch':<8} {'InputRows':<12} {'ProcessedRows/s':<18} {'InputRate':<15} {'Duration (ms)':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "start_time = time.time()\n",
    "batch_count = 0\n",
    "total_input_rows = 0\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(10)\n",
    "    progress = query_b1.lastProgress\n",
    "    \n",
    "    if progress:\n",
    "        batch_count += 1\n",
    "        num_input = progress.get('numInputRows', 0)\n",
    "        total_input_rows += num_input\n",
    "        processed = progress.get('processedRowsPerSecond', 0)\n",
    "        input_rate = progress.get('inputRowsPerSecond', 0)\n",
    "        duration = progress.get('durationMs', {}).get('triggerExecution', 0)\n",
    "        \n",
    "        print(f\"{batch_count:<8} {num_input:<12} {processed:<18.2f} {input_rate:<15.2f} {duration:<15}\")\n",
    "\n",
    "total_duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Batches: {batch_count} | Input rows: {total_input_rows:,} | Duration: {total_duration:.2f}s\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "query_b1.stop()\n",
    "print(\"‚úÖ Query stopped\\n\")\n",
    "\n",
    "# Step 9: Read results\n",
    "print(\"Step 9: Reading results...\\n\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"B1 Results - Hourly Trip Counts\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "output_files = []\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "        output_files.extend([f for f in files if f.endswith('.parquet')])\n",
    "\n",
    "if len(output_files) > 0:\n",
    "    results_b1 = spark.read.parquet(OUTPUT_DIR)\n",
    "    results_b1_sorted = results_b1.orderBy(\"window_start\")\n",
    "    \n",
    "    total_windows = results_b1_sorted.count()\n",
    "    total_trips = results_b1_sorted.agg({\"trip_count\": \"sum\"}).collect()[0][0] or 0\n",
    "    \n",
    "    print(f\"üìä Summary:\")\n",
    "    print(f\"  Windows: {total_windows} | Trips: {total_trips:,} | Duration: {total_duration:.2f}s\\n\")\n",
    "    \n",
    "    results_b1_sorted.show(20, truncate=False)\n",
    "    \n",
    "    # Save output\n",
    "    output_file = \"outputs/b1_hourly_trip_count.txt\"\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"Task: B1 - Hourly Trip Count\\n\")\n",
    "        f.write(f\"Dataset: NYC Yellow Cab 2015 (20 columns)\\n\")\n",
    "        f.write(f\"Windows: {total_windows} | Trips: {total_trips:,} | Duration: {total_duration:.2f}s\\n\\n\")\n",
    "        f.write(\"window_start,window_end,trip_count\\n\")\n",
    "        for row in results_b1_sorted.collect():\n",
    "            f.write(f\"{row.window_start},{row.window_end},{row.trip_count}\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved: {output_file}\\n\")\n",
    "    \n",
    "    # Update metrics\n",
    "    metrics_log_path = \"proof/lab_metrics_log.csv\"\n",
    "    if not os.path.exists(metrics_log_path):\n",
    "        with open(metrics_log_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['run_id', 'task', 'format', 'date', 'num_files', 'input_size_mb',\n",
    "                           'duration_s', 'shuffle_read_mb', 'shuffle_write_mb', 'notes', 'timestamp'])\n",
    "    \n",
    "    with open(metrics_log_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        with open(metrics_log_path, 'r') as rf:\n",
    "            run_id = len(list(csv.reader(rf)))\n",
    "        \n",
    "        writer.writerow([\n",
    "            run_id, 'B1', 'Streaming', '2015-12-01', len(taxi_files), 'N/A',\n",
    "            round(total_duration, 2), 'N/A', 'N/A',\n",
    "            f'{total_windows} windows, {total_trips:,} trips, {batch_count} batches, NYC Yellow Cab 2015 (20 cols)',\n",
    "            datetime.now(timezone.utc).isoformat()\n",
    "        ])\n",
    "    \n",
    "    print(f\"‚úÖ Updated: {metrics_log_path}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No output generated\\n\")\n",
    "    print(f\"Check:\")\n",
    "    print(f\"  - pickup_datetime at position 1 parsed correctly\")\n",
    "    print(f\"  - Timestamps not NULL after try_to_timestamp\")\n",
    "    print(f\"  - Spark logs for parsing errors\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Section 10 - Task B1 Complete\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"üìã Evidence Generated:\")\n",
    "print(f\"  {'‚úÖ' if len(output_files) > 0 else '‚ö†Ô∏è '} proof/plan_b1_hourly_stream.txt\")\n",
    "print(f\"  {'‚úÖ' if len(output_files) > 0 else '‚ö†Ô∏è '} outputs/b1_hourly_trip_count.txt\")\n",
    "print(f\"  {'‚úÖ' if len(output_files) > 0 else '‚ö†Ô∏è '} proof/lab_metrics_log.csv\")\n",
    "print(f\"  {'‚úÖ' if len(output_files) > 0 else '‚ö†Ô∏è '} output/b1_hourly/*.parquet ({len(output_files)} files)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf9117",
   "metadata": {},
   "source": [
    "### B2 ‚Äî RegionEventCount (goldman, citigroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# bounding boxes on dropoff lon/lat; label key 'goldman' or 'citigroup'\n",
    "# window='1 hour'; counts per key; writeStream append\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a50f7",
   "metadata": {},
   "source": [
    "### B3 ‚Äî TrendingArrivals (10-minute windows + state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b30249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# 10-minute windows; compare current vs previous window with state\n",
    "# trigger alert print to stdout; persist per-batch status files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87a8da",
   "metadata": {},
   "source": [
    "## Evidence for Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# collect driver logs; list output dirs; include Spark UI screenshots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c170cb",
   "metadata": {},
   "source": [
    "## Reproducibility Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce342e",
   "metadata": {},
   "source": [
    "- ENV.md present with versions and configs  \n",
    "- Exact spark-submit commands recorded  \n",
    "- Plans saved for any DF stage used  \n",
    "- UI screenshots for representative stages  \n",
    "- All outputs in deterministic locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c946f35-d196-4e38-aed2-afdd40d929dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
