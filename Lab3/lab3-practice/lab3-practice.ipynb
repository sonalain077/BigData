{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b46d0c9-7e60-4e8e-a92a-4b4065e6fdba",
   "metadata": {},
   "source": [
    "# 0.Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765ed3a6-9ebf-44ac-ab56-7c2c48e04f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/06 16:00:09 WARN Utils: Your hostname, a03-341a, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/06 16:00:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 16:00:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 16:00:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/06 16:00:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "PySpark version: 4.0.1\n",
      "Python version: 3.10.19\n",
      "Session timezone: UTC\n",
      "Shuffle partitions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 48656)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/img/miniconda3/envs/bda-env/lib/python3.10/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BDA-PracticeLab03\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Session timezone: {spark.conf.get('spark.sql.session.timeZone')}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee74e7-06ab-4fea-942e-4321cbf08f2d",
   "metadata": {},
   "source": [
    "# 1.Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0cc01c4-c46a-439e-aa5e-69e2a36548d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing graph at /home/img/BigData/Lab03/lab3-practice/data/karate_edges.txt\n",
      "Found SMS dataset at /home/img/BigData/Lab03/lab3-practice/data/sms.tsv\n",
      "Data directory ready: /home/img/BigData/Lab03/lab3-practice/data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
    "PROOF_DIR = BASE_DIR / \"proof\"\n",
    "\n",
    "for directory in (DATA_DIR, OUTPUTS_DIR, PROOF_DIR):\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "karate_path = DATA_DIR / \"karate_edges.txt\"\n",
    "if not karate_path.exists():\n",
    "    synthetic_edges = [\n",
    "        (\"1\", \"2\"), (\"1\", \"3\"), (\"1\", \"4\"), (\"2\", \"3\"), (\"2\", \"5\"), (\"2\", \"6\"),\n",
    "        (\"3\", \"4\"), (\"3\", \"6\"), (\"3\", \"7\"), (\"4\", \"5\"), (\"4\", \"7\"), (\"4\", \"8\"),\n",
    "        (\"5\", \"6\"), (\"5\", \"8\"), (\"6\", \"7\"), (\"6\", \"9\"), (\"7\", \"8\"), (\"7\", \"10\"),\n",
    "        (\"8\", \"1\"), (\"8\", \"9\"), (\"9\", \"10\"), (\"10\", \"1\"), (\"5\", \"1\"), (\"9\", \"2\"),\n",
    "    ]\n",
    "    edge_text = \"\\n\".join(f\"{u} {v}\" for u, v in synthetic_edges)\n",
    "    karate_path.write_text(edge_text)\n",
    "    print(f\"Generated synthetic graph with {len(synthetic_edges)} directed edges at {karate_path}\")\n",
    "else:\n",
    "    print(f\"Found existing graph at {karate_path}\")\n",
    "\n",
    "sms_path = DATA_DIR / \"sms.tsv\"\n",
    "if not sms_path.exists():\n",
    "    sms_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    zip_path = DATA_DIR / \"smsspamcollection.zip\"\n",
    "    if not zip_path.exists():\n",
    "        print(\"Downloading SMS Spam Collection dataset...\")\n",
    "        urllib.request.urlretrieve(sms_url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        with zf.open(\"SMSSpamCollection\") as src, sms_path.open(\"wb\") as dst:\n",
    "            dst.write(src.read())\n",
    "    print(f\"Extracted SMS dataset to {sms_path}\")\n",
    "else:\n",
    "    print(f\"Found SMS dataset at {sms_path}\")\n",
    "\n",
    "print(f\"Data directory ready: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ea176-d7dd-4a81-95f0-64ef8eef30fb",
   "metadata": {},
   "source": [
    "# 2. Helpers: tokenizers and hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970129c9-d98c-4e26-abbc-1abd3b179769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hashlib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r\"[a-z0-9]+\")\n",
    "FEATURE_HASHSIZE = 1 << 18\n",
    "\n",
    "def tokenize(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    return TOKEN_PATTERN.findall(text.lower())\n",
    "\n",
    "def make_bigrams(tokens):\n",
    "    return [f\"{tokens[i]}_{tokens[i+1]}\" for i in range(len(tokens) - 1)]\n",
    "\n",
    "def hash_token(token: str) -> int:\n",
    "    return int(hashlib.md5(token.encode(\"utf-8\")).hexdigest(), 16) % FEATURE_HASHSIZE\n",
    "\n",
    "def featurize(text: str) -> SparseVector:\n",
    "    tokens = tokenize(text)\n",
    "    grams = tokens + make_bigrams(tokens)\n",
    "    if not grams:\n",
    "        return SparseVector(FEATURE_HASHSIZE, [], [])\n",
    "    counts = {}\n",
    "    for gram in grams:\n",
    "        idx = hash_token(gram)\n",
    "        counts[idx] = counts.get(idx, 0.0) + 1.0\n",
    "    indices = sorted(counts.keys())\n",
    "    values = [float(counts[i]) for i in indices]\n",
    "    return SparseVector(FEATURE_HASHSIZE, indices, values)\n",
    "\n",
    "def featurize_counts(text: str):\n",
    "    tokens = tokenize(text)\n",
    "    grams = tokens + make_bigrams(tokens)\n",
    "    counts = {}\n",
    "    for gram in grams:\n",
    "        idx = hash_token(gram)\n",
    "        counts[idx] = counts.get(idx, 0.0) + 1.0\n",
    "    return counts\n",
    "\n",
    "tokenize_udf = F.udf(tokenize, ArrayType(StringType()))\n",
    "featurize_udf = F.udf(featurize, VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb18d97-4f67-4e71-9c04-fa8c34f5f101",
   "metadata": {},
   "source": [
    "# 3. Part A — Multi-Source Personalized PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ab4550-2358-438b-b878-1b1c62097cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PPR with alpha=0.85, iterations=10, sources=['1', '8', '10']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01 | total_mass=1.000000 | preview=[('1', 0.475), ('9', 0.14166666666666666), ('3', 0.09444444444444444)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 02 | total_mass=1.000000 | preview=[('2', 0.19479166666666664), ('4', 0.16134259259259257), ('3', 0.16134259259259257)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 03 | total_mass=1.000000 | preview=[('1', 0.1914633487654321), ('8', 0.1336226851851852), ('6', 0.11606828703703702)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04 | total_mass=1.000000 | preview=[('1', 0.2048894354423868), ('8', 0.14919694573045264), ('10', 0.12205488040123455)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 05 | total_mass=1.000000 | preview=[('1', 0.22672472774134086), ('10', 0.13597773228523663), ('8', 0.12283482454025206)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06 | total_mass=1.000000 | preview=[('1', 0.23037313626440512), ('10', 0.11790275121233068), ('8', 0.11353326898514304)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 07 | total_mass=1.000000 | preview=[('1', 0.21290990581805233), ('8', 0.11703904647091953), ('10', 0.10923831987825455)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08 | total_mass=1.000000 | preview=[('1', 0.20783631338994876), ('8', 0.12461320289258002), ('10', 0.11553234327968409)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 09 | total_mass=1.000000 | preview=[('1', 0.21627917965902768), ('8', 0.12486957199025278), ('10', 0.11888015719105788)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 | total_mass=1.000000 | preview=[('1', 0.2185327521806238), ('8', 0.12170929756934712), ('10', 0.11860620217228462)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top-10 PPR scores to /home/img/BigData/Lab03/lab3-practice/outputs/ppr_topk.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1323"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "edges_rdd = spark.sparkContext.textFile(str(karate_path)).filter(lambda line: line.strip() and not line.startswith(\"#\"))\n",
    "edges_pairs = edges_rdd.map(lambda line: line.strip().split()).filter(lambda parts: len(parts) == 2).map(lambda parts: (parts[0], parts[1]))\n",
    "\n",
    "nodes = edges_pairs.flatMap(lambda kv: kv).distinct().collect()\n",
    "adjacency_map = edges_pairs.groupByKey().mapValues(lambda nbrs: list(dict.fromkeys(nbrs))).collectAsMap()\n",
    "for node in nodes:\n",
    "    adjacency_map.setdefault(node, [])\n",
    "\n",
    "adjacency_rdd = spark.sparkContext.parallelize(list(adjacency_map.items())).cache()\n",
    "nodes_rdd = adjacency_rdd.keys().cache()\n",
    "\n",
    "alpha = 0.85\n",
    "num_iters = 10\n",
    "sources = [nodes[0], nodes[2], nodes[4]] if len(nodes) >= 5 else nodes[:1]\n",
    "source_set = set(sources)\n",
    "initial_mass = 1.0 / len(source_set)\n",
    "k = min(10, len(nodes))\n",
    "\n",
    "ranks = nodes_rdd.map(lambda node: (node, initial_mass if node in source_set else 0.0))\n",
    "print(f\"Running PPR with alpha={alpha}, iterations={num_iters}, sources={sources}\")\n",
    "\n",
    "for iteration in range(1, num_iters + 1):\n",
    "    joined = adjacency_rdd.join(ranks)\n",
    "    dangling_mass = joined.filter(lambda kv: len(kv[1][0]) == 0).map(lambda kv: kv[1][1]).sum()\n",
    "    contribs = (\n",
    "        joined\n",
    "        .flatMap(lambda kv: [] if len(kv[1][0]) == 0 else [(nbr, kv[1][1] / len(kv[1][0])) for nbr in kv[1][0]])\n",
    "        .reduceByKey(add)\n",
    "    )\n",
    "    base = (\n",
    "        nodes_rdd.map(lambda node: (node, 0.0))\n",
    "        .leftOuterJoin(contribs)\n",
    "        .mapValues(lambda pair: pair[1] if pair[1] is not None else 0.0)\n",
    "    )\n",
    "    teleport_mass = (1.0 - alpha) + alpha * dangling_mass\n",
    "    jump_mass = teleport_mass / len(source_set)\n",
    "    ranks = base.map(lambda kv: (kv[0], alpha * kv[1] + (jump_mass if kv[0] in source_set else 0.0)))\n",
    "    total_mass = ranks.values().sum()\n",
    "    ranks = ranks.mapValues(lambda value: value / total_mass)\n",
    "    preview = ranks.takeOrdered(3, key=lambda kv: -kv[1])\n",
    "    print(f\"Iteration {iteration:02d} | total_mass={total_mass:.6f} | preview={preview}\")\n",
    "\n",
    "ppr_topk = ranks.takeOrdered(k, key=lambda kv: -kv[1])\n",
    "ppr_df = spark.createDataFrame(ppr_topk, schema=[\"node\", \"score\"]).orderBy(F.desc(\"score\"))\n",
    "\n",
    "output_ppr_path = OUTPUTS_DIR / \"ppr_topk.csv\"\n",
    "ppr_df.toPandas().to_csv(output_ppr_path, index=False)\n",
    "print(f\"Saved top-{k} PPR scores to {output_ppr_path}\")\n",
    "\n",
    "plan_buffer = StringIO()\n",
    "with redirect_stdout(plan_buffer):\n",
    "    ppr_df.explain(\"formatted\")\n",
    "(PROOF_DIR / \"plan_ppr.txt\").write_text(plan_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b898b-6c97-424f-a37a-7e039b805e4a",
   "metadata": {},
   "source": [
    "# 4. Part B — Spam classification (baseline with MLlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe66bae2-ce29-4127-926c-84adb132aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances: 4503, Validation instances: 1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 16:05:44 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9960\n",
      "# SMS Spam Classification Metrics\n",
      "\n",
      "AUC: 0.9960\n",
      "Threshold: 0.5\n",
      "Precision: 1.0000\n",
      "Recall: 0.6667\n",
      "\n",
      "## Logistic Regression Summary\n",
      "Intercept: -6.7134\n",
      "Non-zero coefficients: 40793\n",
      "Feature space size: 262144\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"label_raw\", T.StringType(), False),\n",
    "    T.StructField(\"text\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "sms_df = spark.read.csv(str(sms_path), sep=\"\t\", schema=schema)\n",
    "sms_df = sms_df.filter(F.col(\"text\").isNotNull())\n",
    "sms_df = sms_df.withColumn(\"label\", F.when(F.col(\"label_raw\") == \"spam\", F.lit(1.0)).otherwise(F.lit(0.0)))\n",
    "\n",
    "features_df = sms_df.select(\"label\", \"text\", featurize_udf(\"text\").alias(\"features\")).cache()\n",
    "\n",
    "train_df, test_df = features_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training instances: {train_df.count()}, Validation instances: {test_df.count()}\")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0,\n",
    "    maxIter=80,\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_df)\n",
    "predictions = lr_model.transform(test_df).cache()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "\n",
    "extract_prob_udf = F.udf(lambda v: float(v[1]) if v is not None else 0.0, DoubleType())\n",
    "with_probs = predictions.withColumn(\"prob_spam\", extract_prob_udf(F.col(\"probability\")))\n",
    "threshold = 0.5\n",
    "scored = with_probs.withColumn(\"pred_label\", F.when(F.col(\"prob_spam\") >= threshold, F.lit(1.0)).otherwise(F.lit(0.0)))\n",
    "\n",
    "agg = scored.agg(\n",
    "    F.sum(F.when((F.col(\"label\") == 1.0) & (F.col(\"pred_label\") == 1.0), 1).otherwise(0)).alias(\"tp\"),\n",
    "    F.sum(F.when((F.col(\"label\") == 0.0) & (F.col(\"pred_label\") == 1.0), 1).otherwise(0)).alias(\"fp\"),\n",
    "    F.sum(F.when((F.col(\"label\") == 1.0) & (F.col(\"pred_label\") == 0.0), 1).otherwise(0)).alias(\"fn\"),\n",
    "    F.count(\"*\").alias(\"total\")\n",
    ").collect()[0]\n",
    "\n",
    "tp = float(agg[\"tp\"])\n",
    "fp = float(agg[\"fp\"])\n",
    "fn = float(agg[\"fn\"])\n",
    "precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "metrics_lines = [\n",
    "    \"# SMS Spam Classification Metrics\",\n",
    "    \"\",\n",
    "    f\"AUC: {auc:.4f}\",\n",
    "    f\"Threshold: {threshold}\",\n",
    "    f\"Precision: {precision:.4f}\",\n",
    "    f\"Recall: {recall:.4f}\",\n",
    "    \"\",\n",
    "    \"## Logistic Regression Summary\",\n",
    "    f\"Intercept: {lr_model.intercept:.4f}\",\n",
    "    f\"Non-zero coefficients: {len([v for v in lr_model.coefficients if v != 0.0])}\",\n",
    "    f\"Feature space size: {FEATURE_HASHSIZE}\",\n",
    "]\n",
    "\n",
    "metrics_path = OUTPUTS_DIR / \"sms_metrics.md\"\n",
    "separator = os.linesep\n",
    "metrics_path.write_text(separator.join(metrics_lines) + separator)\n",
    "print(separator.join(metrics_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38807fa3-3a52-4edc-bacc-f33b56ce2af5",
   "metadata": {},
   "source": [
    "# 5. Part B — Spam classification (manual SGD, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eeddc52-0695-4634-a7ec-a7e50817100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed; learning_rate=0.0900\n",
      "Epoch 2 completed; learning_rate=0.0810\n",
      "Epoch 3 completed; learning_rate=0.0729\n",
      "Epoch 4 completed; learning_rate=0.0656\n",
      "Epoch 5 completed; learning_rate=0.0590\n",
      "\n",
      "## Manual SGD Summary\n",
      "Epochs: 5\n",
      "Learning rate (final): 0.0590\n",
      "AUC: 0.9868\n",
      "Precision (threshold 0.5): 0.9926\n",
      "Recall (threshold 0.5): 0.9184\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "train_records = [(float(row.label), featurize_counts(row.text)) for row in train_df.select(\"label\", \"text\").collect()]\n",
    "test_records = [(float(row.label), featurize_counts(row.text)) for row in test_df.select(\"label\", \"text\").collect()]\n",
    "\n",
    "weights = {}\n",
    "bias = 0.0\n",
    "learning_rate = 0.1\n",
    "reg = 1e-5\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(train_records)\n",
    "    for label, features in train_records:\n",
    "        dot = bias\n",
    "        for idx, value in features.items():\n",
    "            dot += weights.get(idx, 0.0) * value\n",
    "        pred = sigmoid(dot)\n",
    "        error = pred - label\n",
    "        for idx, value in features.items():\n",
    "            w = weights.get(idx, 0.0)\n",
    "            grad = error * value + reg * w\n",
    "            weights[idx] = w - learning_rate * grad\n",
    "        bias -= learning_rate * (error + reg * bias)\n",
    "    learning_rate *= 0.9\n",
    "    print(f\"Epoch {epoch+1} completed; learning_rate={learning_rate:.4f}\")\n",
    "\n",
    "predictions_manual = []\n",
    "for label, features in test_records:\n",
    "    dot = bias\n",
    "    for idx, value in features.items():\n",
    "        dot += weights.get(idx, 0.0) * value\n",
    "    prob = sigmoid(dot)\n",
    "    predictions_manual.append((label, prob))\n",
    "\n",
    "threshold_manual = 0.5\n",
    "tp = fp = fn = tn = 0\n",
    "for label, prob in predictions_manual:\n",
    "    pred = 1.0 if prob >= threshold_manual else 0.0\n",
    "    if label == 1.0 and pred == 1.0:\n",
    "        tp += 1\n",
    "    elif label == 0.0 and pred == 1.0:\n",
    "        fp += 1\n",
    "    elif label == 1.0 and pred == 0.0:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "precision_manual = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "recall_manual = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "sorted_scores = sorted(predictions_manual, key=lambda pair: pair[1])\n",
    "pos = sum(1 for label, _ in sorted_scores if label == 1.0)\n",
    "neg = len(sorted_scores) - pos\n",
    "rank_sum = 0.0\n",
    "for rank, (label, _) in enumerate(sorted_scores, start=1):\n",
    "    if label == 1.0:\n",
    "        rank_sum += rank\n",
    "auc_manual = (rank_sum - pos * (pos + 1) / 2.0) / (pos * neg) if pos and neg else 0.0\n",
    "\n",
    "manual_lines = [\n",
    "    \"\",\n",
    "    \"## Manual SGD Summary\",\n",
    "    f\"Epochs: {epochs}\",\n",
    "    f\"Learning rate (final): {learning_rate:.4f}\",\n",
    "    f\"AUC: {auc_manual:.4f}\",\n",
    "    f\"Precision (threshold {threshold_manual}): {precision_manual:.4f}\",\n",
    "    f\"Recall (threshold {threshold_manual}): {recall_manual:.4f}\",\n",
    "]\n",
    "\n",
    "separator = os.linesep\n",
    "with open(OUTPUTS_DIR / \"sms_metrics.md\", \"a\", encoding=\"utf-8\") as handle:\n",
    "    handle.write(separator.join(manual_lines) + separator)\n",
    "\n",
    "print(separator.join(manual_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01994740-ac7a-480f-961c-d2f92991f3c6",
   "metadata": {},
   "source": [
    "# 7. Environment and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f650981f-4e3c-4a36-b9ee-f0c6b2ddf963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java: openjdk version \"21.0.6\" 2025-01-21\n",
      "Spark configuration (selected):\n",
      " - spark.app.id = local-1765033212540\n",
      " - spark.app.name = BDA-PracticeLab03\n",
      " - spark.app.startTime = 1765033210644\n",
      " - spark.app.submitTime = 1765033209758\n",
      " - spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      " - spark.driver.host = 10.255.255.254\n",
      " - spark.driver.port = 37753\n",
      " - spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      " - spark.executor.id = driver\n",
      " - spark.hadoop.fs.s3a.vectored.read.max.merged.size = 2M\n",
      " - spark.hadoop.fs.s3a.vectored.read.min.seek.size = 128K\n",
      " - spark.master = local[*]\n",
      " - spark.rdd.compress = True\n",
      " - spark.serializer.objectStreamReset = 100\n",
      " - spark.sql.artifact.isolation.enabled = false\n",
      " - spark.sql.session.timeZone = UTC\n",
      " - spark.sql.shuffle.partitions = 4\n",
      " - spark.sql.warehouse.dir = file:/home/img/BigData/Lab03/lab3-practice/spark-warehouse\n",
      " - spark.submit.deployMode = client\n",
      " - spark.submit.pyFiles = \n",
      " - spark.ui.showConsoleProgress = true\n",
      "Environment summary saved to /home/img/BigData/Lab03/lab3-practice/ENV.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def get_java_version():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT)\n",
    "        return output.decode(\"utf-8\").strip().splitlines()[0]\n",
    "    except Exception as exc:\n",
    "        return f\"Unavailable ({exc})\"\n",
    "\n",
    "java_version = get_java_version()\n",
    "print(f\"Java: {java_version}\")\n",
    "\n",
    "print(\"Spark configuration (selected):\")\n",
    "conf_items = sorted(spark.sparkContext.getConf().getAll())\n",
    "for key, value in conf_items:\n",
    "    print(f\" - {key} = {value}\")\n",
    "\n",
    "env_lines = [\n",
    "    \"# Environment Summary\",\n",
    "    \"\",\n",
    "    f\"- Python: {sys.version.split()[0]}\",\n",
    "    f\"- Spark: {spark.version}\",\n",
    "    f\"- PySpark: {pyspark.__version__}\",\n",
    "    f\"- Java: {java_version}\",\n",
    "    f\"- OS: {platform.platform()}\",\n",
    "    \"\",\n",
    "    \"## Spark Configuration\",\n",
    "]\n",
    "\n",
    "env_lines.extend(f\"- {k} = {v}\" for k, v in conf_items)\n",
    "\n",
    "newline = os.linesep\n",
    "ENV_PATH = BASE_DIR / \"ENV.md\"\n",
    "ENV_PATH.write_text(newline.join(env_lines) + newline)\n",
    "\n",
    "print(f\"Environment summary saved to {ENV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d834eb3-a98d-4bf3-836f-d0a3928d74c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
